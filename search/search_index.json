{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PatientFlow: Predicting demand for hospital beds using real-time data","text":""},{"location":"#summary","title":"Summary","text":"<p>patientflow, a Python package, converts patient-level predictions into output that is useful for bed managers in hospitals.</p> <p>We developed this code originally for University College London Hospitals (UCLH) NHS Trust to predict the number of emergency admissions within the next eight hours. The methods generalise to other aspects of patient flow in hospitals, including predictions of discharge numbers, within a group of patients. It can be applied to any problem where it is useful to convert patient-level predictions into outcomes for a whole cohort of patients at a point in time.</p> <p>If you have a predictive model of some outcome for a patient, like admission or discharge from hospital, you can use patientflow to create bed count distributions for a cohort of patients. We show how to prepare your data and train models for these kinds of problems. The repository includes a synthetic dataset and a series of notebooks demonstrating the use of the package.</p>"},{"location":"#what-patientflow-is-for","title":"What patientflow is for:","text":"<ul> <li>Managing patient flow in hospitals: The package can be used to predict numbers of emergency admissions, discharges or transfers between units</li> <li>Short-term operational planning: The predictions produced by this package are designed for bed managers who need to make decisions within an 4-16 hour timeframe.</li> <li>Working with real-time data: The design assumes that data from an electronic health record (EHR) is available in real-time, or near to real-time</li> <li>Point-in-time analysis: The packages works by taking \"snapshots\" of groups of patients at a particular moment, and making projections from those specific moments.</li> </ul>"},{"location":"#what-patientflow-is-not-for","title":"What patientflow is NOT for:","text":"<ul> <li>Long-term capacity planning: The package focuses on immediate operational needs (hours ahead), not strategic planning over weeks or months.</li> <li>Making decisions about individual patients: The package is not designed for clinical decision-making about specific patients. It relies on data entered into the EHR by clinical staff looking after patients, but cannot and should not be use to influence their decision-making</li> <li>General hospital analytics: It is specifically focused on short-term bed management, not broader hospital analytics like long-term demand and capacity planning.</li> <li>Finished/historical patient analysis: While historical data might train underlying models, the package itself focuses on patients currently in the hospital or soon to arrive</li> <li>Replacing human judgment: It augments the information available to bed managers, but isn't meant to automate bed management decisions completely.</li> </ul>"},{"location":"#this-package-will-help-you-if-you-want-to","title":"This package will help you if you want to:","text":"<ul> <li>Convert individual patient predictions to cohort-level insights: Its core purpose is the creation of aggregate bed count distributions, because bed numbers are the currencly used by bed managers.</li> <li>Make predictions for unfinished patient visits: It is designed for making predictions when outcome at the end of the visit are as yet unknown.</li> <li>Develop your own predictive models of emergency demand: The package includes a fully worked example of how to convert data from A&amp;E visits into the right structure, and use that data to train models that predict numbers of emergency beds.</li> </ul>"},{"location":"#this-package-will-not-help-you-if","title":"This package will not help you if:","text":"<ul> <li>You work with time series data: patientflow works with snapshots of a hospital visit summarising what is in the patient record up to that point in time</li> <li>Your focus is on predicting clinical outcomes: the approach is designed</li> </ul>"},{"location":"#mathematical-assumptions-underlying-the-conversion-from-individual-to-cohort-predictions","title":"Mathematical assumptions underlying the conversion from individual to cohort predictions:","text":"<ul> <li>Independence of patient outcomes: The package assumes that individual patient outcomes are conditionally independent given the features used in prediction.</li> <li>Symbolic probability generation: The conversion uses symbolic mathematics (via SymPy) to construct a probability generating function that represents the exact distribution of possible cohort outcomes.</li> <li>Bernoulli outcome model: Each patient outcome is modeled as a Bernoulli trial with its own probability, and the package computes the exact probability distribution for the sum of these independent trials.</li> <li>Coefficient extraction approach: The method works by expanding a symbolic expression and extracting coefficients corresponding to each possible cohort outcome count.</li> <li>Optional weighted aggregation: When converting individual probabilities to cohort-level predictions, the package allows for weighted importance of individual predictions, modifying the contribution of each patient to the overall distribution in specific contexts (eg admissions to different specialties).</li> <li>Discrete outcome space: The package assumes outcomes can be represented as discrete counts (e.g., number of admissions) rather than continuous values.</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Exploration: Start with the notebooks README to get an outline of what is included in the notebooks, and read the patientflow README for an overview of the Python package</li> <li>Installation: Follow the instructions below to set up the environment and install necessary dependencies in your own environment</li> <li>Configuration: Repurpose config.yaml to configure the package to your own data and user requirements</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p><code>patientflow</code> requires Python 3.10.</p>"},{"location":"#installation","title":"Installation","text":"<p>patientflow is not yet available on PyPI. To install the latest development version, clone it first (so that you have access to the synthetic data and the notebooks) and then install it.</p> <pre><code>git clone https://github.com/zmek/patientflow.git\ncd patientflow\npip install -e \".[test]\" #this will install the code in test mode\n\n</code></pre> <p>Navigate to the patientflow folder and run tests to confirm that the installation worked correctly. This command will only work from the root repository. (To date, this has only been tested on Linux and Mac OS machines. If you are running Windows, there may be errors we don't know about.)</p> <pre><code>pytest\n</code></pre> <p>If you get errors running the pytest command, there may be other installations needed on your local machine. (We have found copying the error messages into ChatGPT or Claude very helpful for diagnosing and troubleshooting these errors.)</p>"},{"location":"#training-models-with-data-provided","title":"Training models with data provided","text":"<p>The data provided (which is synthetic) can be used to demonstrate training the models. To run training you have two options</p> <ul> <li>step through the notebooks (for this to work you'll either need copy the two csv files from <code>data-synthetic</code>into your <code>data-public</code> folder or request access on Zenodo to real patient data</li> <li>run a Python script using following commands (by default this will run with the synthetic data in its current location; you can change the <code>data_folder_name</code> parameter if you have the real data in <code>data-public</code>)</li> </ul> <pre><code>cd src\npython -m patientflow.train.emergency_demand --data_folder_name=data-synthetic\n</code></pre> <p>The data_folder_name specifies the name of the folder containing data. The function expects this folder to be directly below the root of the repository</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Initial Research</li> <li> Minimum viable product &lt;-- You are Here</li> <li> Alpha Release</li> <li> Feature-Complete Release</li> </ul>"},{"location":"#about","title":"About","text":"<p>This project was inspired by the py-pi template developed by Tom Monks, and is based on a template developed by the Centre for Advanced Research Computing, University College London.</p>"},{"location":"#project-team","title":"Project Team","text":"<p>Dr Zella King, Clinical Operational Research Unit (CORU), University College London (zella.king@ucl.ac.uk) Jon Gillham, Institute of Health Informatics, UCL Professor Sonya Crowe, CORU Professor Martin Utley, CORU</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work was funded by a grant from the UCL Impact Funding. We are grateful to the Information Governance team and the Caldicott Guardian at UCLH for agreeing that we can release real patient data.</p>"},{"location":"LICENSE/","title":"MIT License","text":"<p>Copyright (c) 2024 Zella King</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/","title":"API reference","text":"<p>Introductory text here</p> <p>test_package</p> <p>Part of a repo containing boilerplate code for publishing on PyPi.</p>"},{"location":"api/#patientflow.aggregate","title":"<code>aggregate</code>","text":"<p>Aggregate Prediction From Patient-Level Probabilities</p> <p>This submodule provides functions to aggregate patient-level predicted probabilities into a probability distribution. The module uses symbolic mathematics to generate and manipulate expressions, enabling the computation of aggregate probabilities based on individual patient-level predictions.</p> Dependencies <ul> <li>numpy: For array operations and numerical calculations.</li> <li>pandas: To handle and manipulate tabular data (DataFrames) for analysis.</li> <li>sympy: A symbolic mathematics library for building and manipulating symbolic expressions, particularly for calculating probabilities.</li> </ul>"},{"location":"api/#patientflow.aggregate--functions","title":"Functions","text":"<p>create_symbols(n):     Generates a list of symbolic variables to represent probability terms.</p> <pre><code>Parameters\n----------\nn : int\n    Number of symbolic variables to generate.\n\nReturns\n-------\nlist of sympy.Symbol\n    A list containing n symbolic variables.\n</code></pre> <p>compute_core_expression(ri, s):     Computes a symbolic expression using symbolic variables and constants.</p> <pre><code>Parameters\n----------\nri : float\n    A constant value (often a probability).\ns : sympy.Symbol\n    A symbolic variable.\n\nReturns\n-------\nsympy.Mul\n    A symbolic expression representing the product of `ri` and `s`.\n</code></pre> <p>build_expression(syms, n):     Constructs a cumulative product of symbolic expressions using symbolic variables.</p> <pre><code>Parameters\n----------\nsyms : list of sympy.Symbol\n    A list of symbolic variables.\nn : int\n    The number of terms to include in the cumulative product.\n\nReturns\n-------\nsympy.Expr\n    A symbolic expression representing the cumulative product of `syms`.\n</code></pre> <p>expression_subs(expression, n, predictions):     Substitutes numeric values into a symbolic expression.</p> <pre><code>Parameters\n----------\nexpression : sympy.Expr\n    A symbolic expression to perform substitution on.\nn : int\n    The number of variables to substitute.\npredictions : array-like\n    Numeric values (e.g., predicted probabilities) to substitute into the expression.\n\nReturns\n-------\nsympy.Expr\n    The symbolic expression after substitution.\n</code></pre> <p>return_coeff(expression, i):     Extracts the coefficient corresponding to a specific term in an expanded symbolic expression.</p> <pre><code>Parameters\n----------\nexpression : sympy.Expr\n    A symbolic expression that has been expanded.\ni : int\n    The index of the term for which the coefficient is to be extracted.\n\nReturns\n-------\nfloat\n    The coefficient for the i-th term.\n</code></pre> <p>model_input_to_pred_proba(model_input, model):     Converts input data into predicted probabilities using the provided model.</p> <pre><code>Parameters\n----------\nmodel_input : array-like\n    The input data to feed into the model.\nmodel : object\n    A predictive model object that implements a `predict_proba` method.\n\nReturns\n-------\narray-like\n    The predicted probabilities output by the model.\n</code></pre> <p>pred_proba_to_agg_predicted(predictions_proba, weights):     Aggregates individual predicted probabilities into an overall prediction using provided weights.</p> <pre><code>Parameters\n----------\npredictions_proba : array-like\n    Predicted probabilities for individual patients.\nweights : array-like\n    Weights corresponding to each patient's probability prediction.\n\nReturns\n-------\nfloat\n    The aggregate predicted probability.\n</code></pre> <p>get_prob_dist_for_prediction_moment(X_test, model, weights, y_test, inference_time):     Computes predicted and observed probabilities for a specific prediction date.</p> <pre><code>Parameters\n----------\nX_test : DataFrame or array-like\n    Input test data to be passed to the model for prediction.\nmodel : object\n    A predictive model object that implements `predict_proba`.\nweights : array-like\n    Weights for aggregating the predicted probabilities.\ny_test : array-like\n    Observed target values corresponding to the test data (optional for inference).\ninference_time : bool\n    Indicates whether the function is used in inference mode (i.e., whether observed data is available).\n\nReturns\n-------\ndict\n    A dictionary containing the predicted and, if applicable, observed probability distributions.\n</code></pre> <p>get_prob_dist(snapshots_dict, X_test, y_test, model, weights):     Computes probability distributions for multiple snapshot dates.</p> <pre><code>Parameters\n----------\nsnapshots_dict : dict\n    A dictionary where keys are snapshot dates and values are associated metadata (e.g., test data).\nX_test : DataFrame or array-like\n    Input test data to be passed to the model.\ny_test : array-like\n    Observed target values.\nmodel : object\n    A predictive model object that implements `predict_proba`.\nweights : array-like\n    Weights for aggregating the predicted probabilities.\n\nReturns\n-------\ndict\n    A dictionary where each key is a snapshot date and the value is the corresponding probability distribution.\n\nExample Usage\n-------------\n# Assuming a predictive model and test data are available\nsnapshot_dates = ['2023-01-01', '2023-01-02']\npredicted_distribution = get_prob_dist(snapshot_dates, dataset, X_test, y_test, model)\nprint(predicted_distribution)\n</code></pre>"},{"location":"api/#patientflow.aggregate.build_expression","title":"<code>build_expression(syms, n)</code>","text":"<p>Construct a cumulative product expression by combining individual symbolic expressions.</p>"},{"location":"api/#patientflow.aggregate.build_expression--parameters","title":"Parameters","text":"<p>syms : iterable     Iterable containing symbols to use in the expressions. n : int     The number of terms to include in the cumulative product.</p>"},{"location":"api/#patientflow.aggregate.build_expression--returns","title":"Returns","text":"<p>Expr     The cumulative product of the expressions.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def build_expression(syms, n):\n    \"\"\"\n    Construct a cumulative product expression by combining individual symbolic expressions.\n\n    Parameters\n    ----------\n    syms : iterable\n        Iterable containing symbols to use in the expressions.\n    n : int\n        The number of terms to include in the cumulative product.\n\n    Returns\n    -------\n    Expr\n        The cumulative product of the expressions.\n\n    \"\"\"\n    s = sym.Symbol(\"s\")\n    expression = 1\n    for i in range(n):\n        expression *= compute_core_expression(syms[i], s)\n    return expression\n</code></pre>"},{"location":"api/#patientflow.aggregate.compute_core_expression","title":"<code>compute_core_expression(ri, s)</code>","text":"<p>Compute a symbolic expression involving a basic mathematical operation with a symbol and a constant.</p>"},{"location":"api/#patientflow.aggregate.compute_core_expression--parameters","title":"Parameters","text":"<p>ri : float     The constant value to substitute into the expression. s : Symbol     The symbolic object used in the expression.</p>"},{"location":"api/#patientflow.aggregate.compute_core_expression--returns","title":"Returns","text":"<p>Expr     The symbolic expression after substitution.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def compute_core_expression(ri, s):\n    \"\"\"\n    Compute a symbolic expression involving a basic mathematical operation with a symbol and a constant.\n\n    Parameters\n    ----------\n    ri : float\n        The constant value to substitute into the expression.\n    s : Symbol\n        The symbolic object used in the expression.\n\n    Returns\n    -------\n    Expr\n        The symbolic expression after substitution.\n\n    \"\"\"\n    r = sym.Symbol(\"r\")\n    core_expression = (1 - r) + r * s\n    return core_expression.subs({r: ri})\n</code></pre>"},{"location":"api/#patientflow.aggregate.create_symbols","title":"<code>create_symbols(n)</code>","text":"<p>Generate a sequence of symbolic objects intended for use in mathematical expressions.</p>"},{"location":"api/#patientflow.aggregate.create_symbols--parameters","title":"Parameters","text":"<p>n : int     Number of symbols to create.</p>"},{"location":"api/#patientflow.aggregate.create_symbols--returns","title":"Returns","text":"<p>tuple     A tuple containing the generated symbolic objects.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def create_symbols(n):\n    \"\"\"\n    Generate a sequence of symbolic objects intended for use in mathematical expressions.\n\n    Parameters\n    ----------\n    n : int\n        Number of symbols to create.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the generated symbolic objects.\n\n    \"\"\"\n    return symbols(f\"r0:{n}\")\n</code></pre>"},{"location":"api/#patientflow.aggregate.expression_subs","title":"<code>expression_subs(expression, n, predictions)</code>","text":"<p>Substitute values into a symbolic expression based on a mapping from symbols to predictions.</p>"},{"location":"api/#patientflow.aggregate.expression_subs--parameters","title":"Parameters","text":"<p>expression : Expr     The symbolic expression to perform substitution on. n : int     Number of symbols and corresponding predictions. predictions : list     List of numerical predictions to substitute.</p>"},{"location":"api/#patientflow.aggregate.expression_subs--returns","title":"Returns","text":"<p>Expr     The expression after performing the substitution.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def expression_subs(expression, n, predictions):\n    \"\"\"\n    Substitute values into a symbolic expression based on a mapping from symbols to predictions.\n\n    Parameters\n    ----------\n    expression : Expr\n        The symbolic expression to perform substitution on.\n    n : int\n        Number of symbols and corresponding predictions.\n    predictions : list\n        List of numerical predictions to substitute.\n\n    Returns\n    -------\n    Expr\n        The expression after performing the substitution.\n\n    \"\"\"\n    syms = create_symbols(n)\n    substitution = dict(zip(syms, predictions))\n    return expression.subs(substitution)\n</code></pre>"},{"location":"api/#patientflow.aggregate.get_prob_dist","title":"<code>get_prob_dist(snapshots_dict, X_test, y_test, model, weights=None)</code>","text":"<p>Calculate probability distributions for each snapshot date based on given model predictions.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist--parameters","title":"Parameters","text":"<p>snapshots_dict : dict     A dictionary mapping snapshot dates (as datetime objects) to indices in <code>X_test</code> and <code>y_test</code>     that correspond to the snapshots to be tested for each date. X_test : pandas.DataFrame     A DataFrame containing the test features for prediction. y_test : pandas.Series     A Series containing the true outcome values corresponding to the test features in <code>X_test</code>. model : any     A predictive model object with a <code>predict_proba</code> method that takes features from <code>X_test</code> and     optionally weights, and returns a probability distribution over possible outcomes. weights : pandas.Series, optional     A Series containing weights for the test data points, which may influence the prediction,     by default None. If provided, the weights should be indexed similarly to <code>X_test</code> and <code>y_test</code>.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist--returns","title":"Returns","text":"<p>dict     A dictionary where each key is a snapshot date and each value is the resulting probability     distribution for that date, obtained by applying the model on the corresponding test snapshots.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist--notes","title":"Notes","text":"<ul> <li>The function asserts that the length of the test features and outcomes are equal for each   snapshot before proceeding with predictions.</li> <li>It notifies the user of progress in processing snapshot dates, especially if there are more   than 10 snapshot dates.</li> </ul> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def get_prob_dist(snapshots_dict, X_test, y_test, model, weights=None):\n    \"\"\"\n    Calculate probability distributions for each snapshot date based on given model predictions.\n\n    Parameters\n    ----------\n    snapshots_dict : dict\n        A dictionary mapping snapshot dates (as datetime objects) to indices in `X_test` and `y_test`\n        that correspond to the snapshots to be tested for each date.\n    X_test : pandas.DataFrame\n        A DataFrame containing the test features for prediction.\n    y_test : pandas.Series\n        A Series containing the true outcome values corresponding to the test features in `X_test`.\n    model : any\n        A predictive model object with a `predict_proba` method that takes features from `X_test` and\n        optionally weights, and returns a probability distribution over possible outcomes.\n    weights : pandas.Series, optional\n        A Series containing weights for the test data points, which may influence the prediction,\n        by default None. If provided, the weights should be indexed similarly to `X_test` and `y_test`.\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a snapshot date and each value is the resulting probability\n        distribution for that date, obtained by applying the model on the corresponding test snapshots.\n\n    Notes\n    -----\n    - The function asserts that the length of the test features and outcomes are equal for each\n      snapshot before proceeding with predictions.\n    - It notifies the user of progress in processing snapshot dates, especially if there are more\n      than 10 snapshot dates.\n\n    \"\"\"\n    prob_dist_dict = {}\n    print(\n        f\"Calculating probability distributions for {len(snapshots_dict)} snapshot dates\"\n    )\n\n    if len(snapshots_dict) &gt; 10:\n        print(\"This may take a minute or more\")\n\n    # Initialize a counter for notifying the user every 10 snapshot dates processed\n    count = 0\n\n    for dt, snapshots_to_include in snapshots_dict.items():\n        if len(snapshots_to_include) == 0:\n            # Create an empty dictionary for the current snapshot date\n            prob_dist_dict[dt] = {\n                \"agg_predicted\": pd.DataFrame({\"agg_proba\": [1]}, index=[0]),\n                \"agg_observed\": 0,\n            }\n        else:\n            # Ensure the lengths of test features and outcomes are equal\n            assert len(X_test.loc[snapshots_to_include]) == len(\n                y_test.loc[snapshots_to_include]\n            ), \"Mismatch in lengths of X_test and y_test snapshots.\"\n\n            if weights is None:\n                prediction_moment_weights = None\n            else:\n                prediction_moment_weights = weights.loc[snapshots_to_include].values\n\n            # Compute the predicted and observed valuesfor the current snapshot date\n            prob_dist_dict[dt] = get_prob_dist_for_prediction_moment(\n                X_test=X_test.loc[snapshots_to_include],\n                y_test=y_test.loc[snapshots_to_include],\n                model=model,\n                weights=prediction_moment_weights,\n            )\n\n        # Increment the counter and notify the user every 10 snapshot dates processed\n        count += 1\n        if count % 10 == 0 and count != len(snapshots_dict):\n            print(f\"Processed {count} snapshot dates\")\n\n    print(f\"Processed {len(snapshots_dict)} snapshot dates\")\n\n    return prob_dist_dict\n</code></pre>"},{"location":"api/#patientflow.aggregate.get_prob_dist_for_prediction_moment","title":"<code>get_prob_dist_for_prediction_moment(X_test, model, weights=None, inference_time=False, y_test=None)</code>","text":"<p>Calculate both predicted distributions and observed values for a given date using test data.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist_for_prediction_moment--parameters","title":"Parameters","text":"<p>X_test : array-like     Test features for a specific snapshot date. model : object     A predictive model which should provide a <code>predict_proba</code> method. weights : array-like, optional     Weights to apply to the predictions for aggregate calculation. inference_time : bool, optional (default=False)     If True, do not calculate or return actual aggregate. y_test : array-like, optional     Actual outcomes corresponding to the test features. Required if inference_time is False.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist_for_prediction_moment--returns","title":"Returns","text":"<p>dict     A dictionary with keys 'agg_predicted' and, if inference_time is False, 'agg_observed' containing the     predicted and observed respectively for the snapshot date. Each is presented as a DataFrame or an integer.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist_for_prediction_moment--raises","title":"Raises","text":"<p>ValueError     If y_test is not provided when inference_time is False.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def get_prob_dist_for_prediction_moment(\n    X_test, model, weights=None, inference_time=False, y_test=None\n):\n    \"\"\"\n    Calculate both predicted distributions and observed values for a given date using test data.\n\n    Parameters\n    ----------\n    X_test : array-like\n        Test features for a specific snapshot date.\n    model : object\n        A predictive model which should provide a `predict_proba` method.\n    weights : array-like, optional\n        Weights to apply to the predictions for aggregate calculation.\n    inference_time : bool, optional (default=False)\n        If True, do not calculate or return actual aggregate.\n    y_test : array-like, optional\n        Actual outcomes corresponding to the test features. Required if inference_time is False.\n\n    Returns\n    -------\n    dict\n        A dictionary with keys 'agg_predicted' and, if inference_time is False, 'agg_observed' containing the\n        predicted and observed respectively for the snapshot date. Each is presented as a DataFrame or an integer.\n\n    Raises\n    ------\n    ValueError\n        If y_test is not provided when inference_time is False.\n\n    \"\"\"\n    if not inference_time and y_test is None:\n        raise ValueError(\"y_test must be provided if inference_time is False.\")\n\n    prediction_moment_dict = {}\n\n    if len(X_test) &gt; 0:\n        pred_proba = model_input_to_pred_proba(X_test, model)\n        agg_predicted = pred_proba_to_agg_predicted(pred_proba, weights)\n        prediction_moment_dict[\"agg_predicted\"] = agg_predicted\n\n        if not inference_time:\n            prediction_moment_dict[\"agg_observed\"] = sum(y_test)\n    else:\n        prediction_moment_dict[\"agg_predicted\"] = pd.DataFrame(\n            {\"agg_proba\": [1]}, index=[0]\n        )\n        if not inference_time:\n            prediction_moment_dict[\"agg_observed\"] = 0\n\n    return prediction_moment_dict\n</code></pre>"},{"location":"api/#patientflow.aggregate.model_input_to_pred_proba","title":"<code>model_input_to_pred_proba(model_input, model)</code>","text":"<p>Use a predictive model to convert model input data into predicted probabilities.</p>"},{"location":"api/#patientflow.aggregate.model_input_to_pred_proba--parameters","title":"Parameters","text":"<p>model_input : array-like     The input data to the model, typically as features used for predictions. model : object     A model object with a <code>predict_proba</code> method that computes probability estimates.</p>"},{"location":"api/#patientflow.aggregate.model_input_to_pred_proba--returns","title":"Returns","text":"<p>DataFrame     A pandas DataFrame containing the predicted probabilities for the positive class,     with one column labeled 'pred_proba'.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def model_input_to_pred_proba(model_input, model):\n    \"\"\"\n    Use a predictive model to convert model input data into predicted probabilities.\n\n    Parameters\n    ----------\n    model_input : array-like\n        The input data to the model, typically as features used for predictions.\n    model : object\n        A model object with a `predict_proba` method that computes probability estimates.\n\n    Returns\n    -------\n    DataFrame\n        A pandas DataFrame containing the predicted probabilities for the positive class,\n        with one column labeled 'pred_proba'.\n\n    \"\"\"\n    if len(model_input) == 0:\n        return pd.DataFrame(columns=[\"pred_proba\"])\n    else:\n        predictions = model.predict_proba(model_input)[:, 1]\n        return pd.DataFrame(\n            predictions, index=model_input.index, columns=[\"pred_proba\"]\n        )\n</code></pre>"},{"location":"api/#patientflow.aggregate.pred_proba_to_agg_predicted","title":"<code>pred_proba_to_agg_predicted(predictions_proba, weights=None)</code>","text":"<p>Convert individual probability predictions into aggregate predicted probability distribution using optional weights.</p>"},{"location":"api/#patientflow.aggregate.pred_proba_to_agg_predicted--parameters","title":"Parameters","text":"<p>predictions_proba : DataFrame     A DataFrame containing the probability predictions; must have a single column named 'pred_proba'. weights : array-like, optional     An array of weights, of the same length as the DataFrame rows, to apply to each prediction.</p>"},{"location":"api/#patientflow.aggregate.pred_proba_to_agg_predicted--returns","title":"Returns","text":"<p>DataFrame     A DataFrame with a single column 'agg_proba' showing the aggregated probability,     indexed from 0 to n, where n is the number of predictions.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def pred_proba_to_agg_predicted(predictions_proba, weights=None):\n    \"\"\"\n    Convert individual probability predictions into aggregate predicted probability distribution using optional weights.\n\n    Parameters\n    ----------\n    predictions_proba : DataFrame\n        A DataFrame containing the probability predictions; must have a single column named 'pred_proba'.\n    weights : array-like, optional\n        An array of weights, of the same length as the DataFrame rows, to apply to each prediction.\n\n    Returns\n    -------\n    DataFrame\n        A DataFrame with a single column 'agg_proba' showing the aggregated probability,\n        indexed from 0 to n, where n is the number of predictions.\n\n    \"\"\"\n    n = len(predictions_proba)\n\n    if n == 0:\n        agg_predicted_dict = {0: 1}\n    else:\n        local_proba = predictions_proba.copy()\n        if weights is not None:\n            local_proba[\"pred_proba\"] *= weights\n\n        syms = create_symbols(n)\n        expression = build_expression(syms, n)\n        expression = expression_subs(expression, n, local_proba[\"pred_proba\"])\n        agg_predicted_dict = {i: return_coeff(expression, i) for i in range(n + 1)}\n\n    agg_predicted = pd.DataFrame.from_dict(\n        agg_predicted_dict, orient=\"index\", columns=[\"agg_proba\"]\n    )\n    return agg_predicted\n</code></pre>"},{"location":"api/#patientflow.aggregate.return_coeff","title":"<code>return_coeff(expression, i)</code>","text":"<p>Extract the coefficient of a specified power from an expanded symbolic expression.</p>"},{"location":"api/#patientflow.aggregate.return_coeff--parameters","title":"Parameters","text":"<p>expression : Expr     The expression to expand and extract from. i : int     The power of the term whose coefficient is to be extracted.</p>"},{"location":"api/#patientflow.aggregate.return_coeff--returns","title":"Returns","text":"<p>number     The coefficient of the specified power in the expression.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def return_coeff(expression, i):\n    \"\"\"\n    Extract the coefficient of a specified power from an expanded symbolic expression.\n\n    Parameters\n    ----------\n    expression : Expr\n        The expression to expand and extract from.\n    i : int\n        The power of the term whose coefficient is to be extracted.\n\n    Returns\n    -------\n    number\n        The coefficient of the specified power in the expression.\n\n    \"\"\"\n    s = sym.Symbol(\"s\")\n    return expand(expression).coeff(s, i)\n</code></pre>"},{"location":"api/#patientflow.convert","title":"<code>convert</code>","text":"<p>Data Processing and Anonymization for Hospital Visit Records.</p> <p>This script provides functions to preprocess hospital visit data, including: - Calculating patient age on arrival and grouping by age ranges. - Shifting dates forward to anonymize visit data. - Mapping consultation codes to predefined consultation types. - Resampling arrival hours in a target dataset based on source data.</p> <p>The script can be run as a standalone program to adjust arrival hours in a target dataset using a source dataset's time distribution.</p>"},{"location":"api/#patientflow.convert--functions","title":"Functions","text":"<ul> <li>prepare_age_and_dates(df) : Calculates patient age on arrival and categorizes into age groups.</li> <li>shift_dates_into_future(df, yta, seed_path) : Shifts all date-related fields into the future for anonymization.</li> <li>map_consultations_to_types(df, name_mapping) : Maps consultation codes to their respective consultation types.</li> <li>resample_hours(df_source, df_target) : Resamples arrival hours in the target dataset based on the source dataset.</li> <li>main() : Command-line interface for resampling hours using input CSV files.</li> </ul>"},{"location":"api/#patientflow.convert--usage","title":"Usage","text":"<p>To run the script from the command line:     python3 convert.py --source  --target  --output"},{"location":"api/#patientflow.convert.main","title":"<code>main()</code>","text":"<p>Main function to resample hours from a source dataset to a target dataset.</p> <p>This function reads input CSV files, resamples arrival times in the target dataset based on the probability distribution from the source dataset, and saves the modified target dataset to an output file.</p> Source code in <code>src/patientflow/convert.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to resample hours from a source dataset to a target dataset.\n\n    This function reads input CSV files, resamples arrival times in the target\n    dataset based on the probability distribution from the source dataset, and\n    saves the modified target dataset to an output file.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Resample hours from source arrivals data to target data\"\n    )\n    parser.add_argument(\n        \"--source\",\n        default=\"data-public/inpatient_arrivals.csv\",\n        help=\"Source CSV file with original hour distribution\",\n    )\n    parser.add_argument(\n        \"--target\",\n        default=\"data-synthetic/inpatient_arrivals.csv\",\n        help=\"Target CSV file to modify\",\n    )\n    parser.add_argument(\n        \"--output\",\n        default=\"data-synthetic/inpatient_arrivals_modified.csv\",\n        help=\"Output file path\",\n    )\n\n    args = parser.parse_args()\n\n    df_source = pd.read_csv(args.source, parse_dates=[\"arrival_datetime\"])\n    df_target = pd.read_csv(args.target, parse_dates=[\"arrival_datetime\"])\n\n    df_new = resample_hours(df_source, df_target)\n\n    df_new.to_csv(args.output, index=False)\n    print(f\"Modified {len(df_new)} arrival times and saved to {args.output}\")\n</code></pre>"},{"location":"api/#patientflow.convert.map_consultations_to_types","title":"<code>map_consultations_to_types(df, name_mapping)</code>","text":"<p>Map consultation codes to their respective types using a predefined mapping.</p>"},{"location":"api/#patientflow.convert.map_consultations_to_types--parameters","title":"Parameters","text":"<p>df : pandas.DataFrame     The dataframe containing columns 'consultation_sequence' and 'final_sequence',     which store lists of consultation codes. name_mapping : pandas.DataFrame     A dataframe with 'code' and 'type' columns mapping consultation codes     to their respective types.</p>"},{"location":"api/#patientflow.convert.map_consultations_to_types--returns","title":"Returns","text":"<p>pandas.DataFrame     The modified dataframe with mapped consultation types.</p> Source code in <code>src/patientflow/convert.py</code> <pre><code>def map_consultations_to_types(df, name_mapping):\n    \"\"\"\n    Map consultation codes to their respective types using a predefined mapping.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The dataframe containing columns 'consultation_sequence' and 'final_sequence',\n        which store lists of consultation codes.\n    name_mapping : pandas.DataFrame\n        A dataframe with 'code' and 'type' columns mapping consultation codes\n        to their respective types.\n\n    Returns\n    -------\n    pandas.DataFrame\n        The modified dataframe with mapped consultation types.\n    \"\"\"\n    code_to_type = dict(zip(name_mapping[\"code\"], name_mapping[\"type\"]))\n\n    def map_codes_to_types(codes):\n        return [code_to_type.get(code, \"unknown\") for code in codes]\n\n    df[\"consultation_sequence\"] = df[\"consultation_sequence\"].apply(map_codes_to_types)\n    df[\"final_sequence\"] = df[\"final_sequence\"].apply(map_codes_to_types)\n\n    return df\n</code></pre>"},{"location":"api/#patientflow.convert.prepare_age_and_dates","title":"<code>prepare_age_and_dates(df)</code>","text":"<p>Prepare age and date-related features in the dataset.</p> <p>This function calculates the age of individuals on arrival based on their date of birth and arrival datetime. It also categorizes them into age groups. If <code>snapshot_datetime</code> exists in the dataframe, it computes the prediction time, snapshot date, and elapsed length of stay (LOS).</p>"},{"location":"api/#patientflow.convert.prepare_age_and_dates--parameters","title":"Parameters","text":"<p>df : pandas.DataFrame     A dataframe containing at least the columns 'date_of_birth' and     'arrival_datetime'. If 'snapshot_datetime' exists, additional     computations are performed.</p>"},{"location":"api/#patientflow.convert.prepare_age_and_dates--returns","title":"Returns","text":"<p>pandas.DataFrame     The modified dataframe with additional columns:     - 'age_on_arrival': Numeric representation of age at arrival.     - 'age_group': Categorical age group.     - 'prediction_time': Tuple representing the hour and minute of the snapshot.     - 'snapshot_date': Date extracted from 'snapshot_datetime'.     - 'elapsed_los': Time in seconds since arrival (if snapshot is available).</p> Source code in <code>src/patientflow/convert.py</code> <pre><code>def prepare_age_and_dates(df):\n    \"\"\"\n    Prepare age and date-related features in the dataset.\n\n    This function calculates the age of individuals on arrival based on their\n    date of birth and arrival datetime. It also categorizes them into age groups.\n    If `snapshot_datetime` exists in the dataframe, it computes the prediction\n    time, snapshot date, and elapsed length of stay (LOS).\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        A dataframe containing at least the columns 'date_of_birth' and\n        'arrival_datetime'. If 'snapshot_datetime' exists, additional\n        computations are performed.\n\n    Returns\n    -------\n    pandas.DataFrame\n        The modified dataframe with additional columns:\n        - 'age_on_arrival': Numeric representation of age at arrival.\n        - 'age_group': Categorical age group.\n        - 'prediction_time': Tuple representing the hour and minute of the snapshot.\n        - 'snapshot_date': Date extracted from 'snapshot_datetime'.\n        - 'elapsed_los': Time in seconds since arrival (if snapshot is available).\n    \"\"\"\n    df[\"age_on_arrival\"] = (\n        pd.to_timedelta(\n            (\n                pd.to_datetime(df[\"arrival_datetime\"]).dt.date\n                - pd.to_datetime(df[\"date_of_birth\"]).dt.date\n            )\n        ).dt.days\n        / 365.2425\n    ).apply(lambda x: np.floor(x) if pd.notna(x) else x)\n\n    bins = [-1, 18, 25, 35, 45, 55, 65, 75, 102]\n    labels = [\"0-17\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65-74\", \"75-102\"]\n    df[\"age_group\"] = pd.cut(df[\"age_on_arrival\"], bins=bins, labels=labels, right=True)\n\n    if \"snapshot_datetime\" in df.columns:\n        df[\"prediction_time\"] = (\n            df[\"snapshot_datetime\"]\n            .dt.strftime(\"%H,%M\")\n            .apply(lambda x: tuple(map(int, x.split(\",\"))))\n        )\n        df[\"snapshot_date\"] = pd.to_datetime(df[\"snapshot_datetime\"]).dt.date\n        df[\"elapsed_los\"] = (\n            df[\"snapshot_datetime\"] - df[\"arrival_datetime\"]\n        ).dt.total_seconds()\n\n    return df\n</code></pre>"},{"location":"api/#patientflow.convert.resample_hours","title":"<code>resample_hours(df_source, df_target)</code>","text":"<p>Resample arrival hours in <code>df_target</code> based on the probability distribution of arrival hours from <code>df_source</code>.</p>"},{"location":"api/#patientflow.convert.resample_hours--parameters","title":"Parameters","text":"<p>df_source : pandas.DataFrame     Source dataframe with 'arrival_datetime' column to derive the hour distribution. df_target : pandas.DataFrame     Target dataframe where arrival hours will be modified.</p>"},{"location":"api/#patientflow.convert.resample_hours--returns","title":"Returns","text":"<p>pandas.DataFrame     Modified target dataframe with new arrival hours.</p> Source code in <code>src/patientflow/convert.py</code> <pre><code>def resample_hours(df_source, df_target):\n    \"\"\"\n    Resample arrival hours in `df_target` based on the probability distribution\n    of arrival hours from `df_source`.\n\n    Parameters\n    ----------\n    df_source : pandas.DataFrame\n        Source dataframe with 'arrival_datetime' column to derive the hour distribution.\n    df_target : pandas.DataFrame\n        Target dataframe where arrival hours will be modified.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Modified target dataframe with new arrival hours.\n    \"\"\"\n    arrival_hours = df_source[\"arrival_datetime\"].dt.hour\n    hour_counts = arrival_hours.value_counts()\n    total_arrivals = len(arrival_hours)\n    hour_probabilities = hour_counts / total_arrivals\n\n    hours = np.array(hour_probabilities.index)\n    probabilities = np.array(hour_probabilities.values)\n\n    new_hours = np.random.choice(hours, size=len(df_target), p=probabilities)\n    new_datetimes = pd.Series(\n        [x.replace(hour=h) for x, h in zip(df_target[\"arrival_datetime\"], new_hours)]\n    )\n    df_target[\"arrival_datetime\"] = new_datetimes\n\n    return df_target\n</code></pre>"},{"location":"api/#patientflow.convert.shift_dates_into_future","title":"<code>shift_dates_into_future(df, yta, seed_path)</code>","text":"<p>Shift all date-related columns into the future to anonymize visit data.</p> <p>This function reads a random seed from a file, generates a random number of weeks to shift all date-related columns, and applies this shift.</p>"},{"location":"api/#patientflow.convert.shift_dates_into_future--parameters","title":"Parameters","text":"<p>df : pandas.DataFrame     The main dataset containing visit records with date columns. yta : pandas.DataFrame     Additional dataset with arrival and departure datetime fields to be shifted. seed_path : str     Path to the file containing the seed value.</p>"},{"location":"api/#patientflow.convert.shift_dates_into_future--returns","title":"Returns","text":"<p>tuple     A tuple containing the modified <code>df</code> and <code>yta</code> dataframes.</p> Source code in <code>src/patientflow/convert.py</code> <pre><code>def shift_dates_into_future(df, yta, seed_path):\n    \"\"\"\n    Shift all date-related columns into the future to anonymize visit data.\n\n    This function reads a random seed from a file, generates a random number\n    of weeks to shift all date-related columns, and applies this shift.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The main dataset containing visit records with date columns.\n    yta : pandas.DataFrame\n        Additional dataset with arrival and departure datetime fields to be shifted.\n    seed_path : str\n        Path to the file containing the seed value.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the modified `df` and `yta` dataframes.\n    \"\"\"\n    print(\"\\nConverting dates to anonymise visits. Current min and max snapshot dates:\")\n    print(df.snapshot_date.min())\n    print(df.snapshot_date.max())\n\n    with open(seed_path, \"r\") as file:\n        seed = int(file.read().strip())\n\n    np.random.seed(seed)\n    n = np.random.randint(1, 10 * 52)  # Random shift in weeks\n\n    df.loc[:, \"snapshot_date\"] += pd.Timedelta(days=n * 7)\n    df.loc[:, \"snapshot_datetime\"] += pd.Timedelta(days=n * 7)\n    df.loc[:, \"arrival_datetime\"] += pd.Timedelta(days=n * 7)\n    df.loc[:, \"departure_datetime\"] += pd.Timedelta(days=n * 7)\n\n    print(\"New min and max snapshot dates:\")\n    print(df.snapshot_date.min())\n    print(df.snapshot_date.max())\n\n    yta[\"arrival_datetime\"] += pd.Timedelta(days=n * 7)\n    yta[\"departure_datetime\"] += pd.Timedelta(days=n * 7)\n\n    return df, yta\n</code></pre>"},{"location":"api/#patientflow.evaluate","title":"<code>evaluate</code>","text":"<p>Patient Flow Evaluation Module</p> <p>This module provides functions for evaluating and comparing different prediction models for patient admissions in a healthcare setting. It includes utilities for calculating metrics such as Mean Absolute Error (MAE) and Mean Percentage Error (MPE), as well as functions for predicting admissions based on historical data and combining different prediction models.</p> <p>Key Features: - Evaluation of probability distribution-based prediction models - Calculation of observed admissions based on ED targets - Prediction using historical data from previous weeks - Combination and evaluation of multiple prediction models</p> <p>Main Functions: - calc_mae_mpe: Calculate MAE and MPE for probability distribution predictions - calculate_weighted_observed: Calculate actual admissions assuming ED targets are met - predict_using_previous_weeks: Predict admissions using average from previous weeks - evaluate_six_week_average: Evaluate the six-week average prediction model - evaluate_combined_model: Evaluate a combined prediction model</p>"},{"location":"api/#patientflow.evaluate.calc_mae_mpe","title":"<code>calc_mae_mpe(prob_dist_dict_all, use_most_probable=True)</code>","text":"<p>Calculate MAE and MPE for all prediction times in the given probability distribution dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>prob_dist_dict_all</code> <code>Dict[Any, Dict[Any, Dict[str, Any]]]</code> <p>Nested dictionary containing probability distributions.</p> required <code>use_most_probable</code> <code>bool</code> <p>Whether to use the most probable value or expected value. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]</code> <p>Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]: Dictionary of results for each prediction time.</p> Source code in <code>src/patientflow/evaluate.py</code> <pre><code>def calc_mae_mpe(\n    prob_dist_dict_all: Dict[Any, Dict[Any, Dict[str, Any]]],\n    use_most_probable: bool = True,\n) -&gt; Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]:\n    \"\"\"\n    Calculate MAE and MPE for all prediction times in the given probability distribution dictionary.\n\n    Args:\n        prob_dist_dict_all (Dict[Any, Dict[Any, Dict[str, Any]]]): Nested dictionary containing probability distributions.\n        use_most_probable (bool, optional): Whether to use the most probable value or expected value. Defaults to True.\n\n    Returns:\n        Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]: Dictionary of results for each prediction time.\n    \"\"\"\n    results: Dict[Any, Dict[str, Union[List[Union[int, float]], float]]] = {}\n\n    for _prediction_time in prob_dist_dict_all.keys():\n        expected_values: List[Union[int, float]] = []\n        observed_values: List[float] = []\n\n        for dt in prob_dist_dict_all[_prediction_time].keys():\n            preds: Dict[str, Any] = prob_dist_dict_all[_prediction_time][dt]\n\n            expected_value: Union[int, float] = (\n                int(preds[\"agg_predicted\"].idxmax().values[0])\n                if use_most_probable\n                else float(\n                    np.dot(\n                        preds[\"agg_predicted\"].index,\n                        preds[\"agg_predicted\"].values.flatten(),\n                    )\n                )\n            )\n\n            observed_value: float = float(preds[\"agg_observed\"])\n\n            expected_values.append(expected_value)\n            observed_values.append(observed_value)\n\n        results[_prediction_time] = calculate_results(expected_values, observed_values)\n\n    return results\n</code></pre>"},{"location":"api/#patientflow.evaluate.calculate_admission_probs_relative_to_prediction","title":"<code>calculate_admission_probs_relative_to_prediction(df, prediction_datetime, prediction_window, x1, y1, x2, y2, is_before=True)</code>","text":"<p>Calculate admission probabilities for arrivals relative to a prediction time window</p> <p>Parameters: df: DataFrame containing arrival_datetime column prediction_datetime: datetime for prediction window start prediction_window: window length in minutes x1, y1, x2, y2: parameters for aspirational curve is_before: boolean indicating if arrivals are before prediction time</p> <p>Returns: DataFrame with added probability columns</p> Source code in <code>src/patientflow/evaluate.py</code> <pre><code>def calculate_admission_probs_relative_to_prediction(\n    df, prediction_datetime, prediction_window, x1, y1, x2, y2, is_before=True\n):\n    \"\"\"\n    Calculate admission probabilities for arrivals relative to a prediction time window\n\n    Parameters:\n    df: DataFrame containing arrival_datetime column\n    prediction_datetime: datetime for prediction window start\n    prediction_window: window length in minutes\n    x1, y1, x2, y2: parameters for aspirational curve\n    is_before: boolean indicating if arrivals are before prediction time\n\n    Returns:\n    DataFrame with added probability columns\n    \"\"\"\n    result = df.copy()\n\n    if is_before:\n        result[\"hours_before_pred_window\"] = result[\"arrival_datetime\"].apply(\n            lambda x: (prediction_datetime - x).seconds / 3600\n        )\n        result[\"prob_admission_before_pred_window\"] = result[\n            \"hours_before_pred_window\"\n        ].apply(lambda x: get_y_from_aspirational_curve(x, x1, y1, x2, y2))\n        result[\"prob_admission_in_pred_window\"] = result[\n            \"hours_before_pred_window\"\n        ].apply(\n            lambda x: get_y_from_aspirational_curve(\n                x + prediction_window / 60, x1, y1, x2, y2\n            )\n            - get_y_from_aspirational_curve(x, x1, y1, x2, y2)\n        )\n    else:\n        result[\"hours_after_pred_window\"] = result[\"arrival_datetime\"].apply(\n            lambda x: (x - prediction_datetime).seconds / 3600\n        )\n        result[\"prob_admission_in_pred_window\"] = result[\n            \"hours_after_pred_window\"\n        ].apply(\n            lambda x: get_y_from_aspirational_curve(\n                (prediction_window / 60) - x, x1, y1, x2, y2\n            )\n        )\n\n    return result\n</code></pre>"},{"location":"api/#patientflow.evaluate.calculate_results","title":"<code>calculate_results(expected_values, observed_values)</code>","text":"<p>Calculate evaluation metrics based on expected and observed values.</p> <p>Parameters:</p> Name Type Description Default <code>expected_values</code> <code>List[Union[int, float]]</code> <p>List of expected values.</p> required <code>observed_values</code> <code>List[float]</code> <p>List of observed values.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[List[Union[int, float]], float]]</code> <p>Dict[str, Union[List[Union[int, float]], float]]: Dictionary containing expected values, observed values, MAE, and MPE.</p> Source code in <code>src/patientflow/evaluate.py</code> <pre><code>def calculate_results(\n    expected_values: List[Union[int, float]], observed_values: List[float]\n) -&gt; Dict[str, Union[List[Union[int, float]], float]]:\n    \"\"\"\n    Calculate evaluation metrics based on expected and observed values.\n\n    Args:\n        expected_values (List[Union[int, float]]): List of expected values.\n        observed_values (List[float]): List of observed values.\n\n    Returns:\n        Dict[str, Union[List[Union[int, float]], float]]: Dictionary containing expected values, observed values, MAE, and MPE.\n    \"\"\"\n    expected_array: np.ndarray = np.array(expected_values)\n    observed_array: np.ndarray = np.array(observed_values)\n\n    absolute_errors: np.ndarray = np.abs(expected_array - observed_array)\n    mae: float = float(np.mean(absolute_errors))\n\n    non_zero_mask: np.ndarray = observed_array != 0\n    filtered_absolute_errors: np.ndarray = absolute_errors[non_zero_mask]\n    filtered_observed_array: np.ndarray = observed_array[non_zero_mask]\n\n    percentage_errors: np.ndarray = (\n        filtered_absolute_errors / filtered_observed_array * 100\n    )\n    mpe: float = float(np.mean(percentage_errors))\n\n    return {\n        \"expected\": expected_values,\n        \"observed\": observed_values,\n        \"mae\": mae,\n        \"mpe\": mpe,\n    }\n</code></pre>"},{"location":"api/#patientflow.evaluate.calculate_weighted_observed","title":"<code>calculate_weighted_observed(df, dt, prediction_window, x1, y1, x2, y2, prediction_time)</code>","text":"<p>Calculate weighted observed admissions for a specific date and prediction window</p> <p>Parameters: df: DataFrame with arrival_datetime column dt: target date prediction_window: window length in minutes x1, y1, x2, y2: parameters for aspirational curve prediction_time: tuple of (hour, minute)</p> Source code in <code>src/patientflow/evaluate.py</code> <pre><code>def calculate_weighted_observed(\n    df, dt, prediction_window, x1, y1, x2, y2, prediction_time\n):\n    \"\"\"\n    Calculate weighted observed admissions for a specific date and prediction window\n\n    Parameters:\n    df: DataFrame with arrival_datetime column\n    dt: target date\n    prediction_window: window length in minutes\n    x1, y1, x2, y2: parameters for aspirational curve\n    prediction_time: tuple of (hour, minute)\n    \"\"\"\n    # Create prediction datetime\n    prediction_datetime = pd.to_datetime(dt).replace(\n        hour=prediction_time[0], minute=prediction_time[1]\n    )\n\n    # Filter for target date and get arrivals with probabilities\n    filtered_df = df[df[\"arrival_datetime\"].dt.date == dt]\n    arrived_before, arrived_after = get_arrivals_with_admission_probs(\n        filtered_df,\n        prediction_datetime,\n        prediction_window,\n        prediction_time,\n        x1,\n        y1,\n        x2,\n        y2,\n        target_date=dt,\n    )\n\n    # Calculate weighted sum\n    weighted_observed = (\n        arrived_before[\"prob_admission_in_pred_window\"].sum()\n        + arrived_after[\"prob_admission_in_pred_window\"].sum()\n    )\n\n    return weighted_observed\n</code></pre>"},{"location":"api/#patientflow.evaluate.combine_distributions","title":"<code>combine_distributions(dist1, dist2)</code>","text":"<p>Combine two probability distributions using convolution.</p> <p>Parameters:</p> Name Type Description Default <code>dist1</code> <code>DataFrame</code> <p>First probability distribution.</p> required <code>dist2</code> <code>DataFrame</code> <p>Second probability distribution.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Combined probability distribution.</p> Source code in <code>src/patientflow/evaluate.py</code> <pre><code>def combine_distributions(dist1: pd.DataFrame, dist2: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Combine two probability distributions using convolution.\n\n    Args:\n        dist1 (pd.DataFrame): First probability distribution.\n        dist2 (pd.DataFrame): Second probability distribution.\n\n    Returns:\n        pd.DataFrame: Combined probability distribution.\n    \"\"\"\n    arr1 = dist1.values\n    arr2 = dist2.values\n\n    combined = signal.convolve(arr1, arr2)\n    new_index = range(len(combined))\n\n    combined_df = pd.DataFrame(combined, index=new_index, columns=[\"agg_predicted\"])\n    combined_df[\"agg_predicted\"] = (\n        combined_df[\"agg_predicted\"] / combined_df[\"agg_predicted\"].sum()\n    )\n\n    return combined_df\n</code></pre>"},{"location":"api/#patientflow.evaluate.create_time_mask","title":"<code>create_time_mask(df, hour, minute)</code>","text":"<p>Create a mask for times before/after a specific hour:minute</p> Source code in <code>src/patientflow/evaluate.py</code> <pre><code>def create_time_mask(df, hour, minute):\n    \"\"\"Create a mask for times before/after a specific hour:minute\"\"\"\n    return (df[\"arrival_datetime\"].dt.hour &gt; hour) | (\n        (df[\"arrival_datetime\"].dt.hour == hour)\n        &amp; (df[\"arrival_datetime\"].dt.minute &gt; minute)\n    )\n</code></pre>"},{"location":"api/#patientflow.evaluate.evaluate_combined_model","title":"<code>evaluate_combined_model(prob_dist_dict_all, df, yta_preds, prediction_window, x1, y1, x2, y2, prediction_time, num_weeks, model_name, use_most_probable=True)</code>","text":"<p>Evaluate the combined prediction model.</p> <p>Parameters:</p> Name Type Description Default <code>prob_dist_dict_all</code> <code>Dict[Any, Dict[Any, Dict[str, Any]]]</code> <p>Nested dictionary containing probability distributions.</p> required <code>df</code> <code>DataFrame</code> <p>DataFrame containing patient data.</p> required <code>yta_preds</code> <code>DataFrame</code> <p>Yet-to-arrive predictions.</p> required <code>prediction_window</code> <code>int</code> <p>Prediction window in minutes.</p> required <code>x1</code> <code>float), y1 (float), x2 (float), y2 (float</code> <p>Parameters for aspirational curve.</p> required <code>prediction_time</code> <code>Tuple[int, int]</code> <p>Hour and minute of prediction.</p> required <code>num_weeks</code> <code>int</code> <p>Number of previous weeks to consider.</p> required <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <code>use_most_probable</code> <code>bool</code> <p>Whether to use the most probable value or expected value. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]</code> <p>Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]: Evaluation results.</p> Source code in <code>src/patientflow/evaluate.py</code> <pre><code>def evaluate_combined_model(\n    prob_dist_dict_all: Dict[Any, Dict[Any, Dict[str, Any]]],\n    df: pd.DataFrame,\n    yta_preds: pd.DataFrame,\n    prediction_window: int,\n    x1: float,\n    y1: float,\n    x2: float,\n    y2: float,\n    prediction_time: Tuple[int, int],\n    num_weeks: int,\n    model_name: str,\n    use_most_probable: bool = True,\n) -&gt; Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]:\n    \"\"\"\n    Evaluate the combined prediction model.\n\n    Args:\n        prob_dist_dict_all (Dict[Any, Dict[Any, Dict[str, Any]]]): Nested dictionary containing probability distributions.\n        df (pd.DataFrame): DataFrame containing patient data.\n        yta_preds (pd.DataFrame): Yet-to-arrive predictions.\n        prediction_window (int): Prediction window in minutes.\n        x1 (float), y1 (float), x2 (float), y2 (float): Parameters for aspirational curve.\n        prediction_time (Tuple[int, int]): Hour and minute of prediction.\n        num_weeks (int): Number of previous weeks to consider.\n        model_name (str): Name of the model.\n        use_most_probable (bool, optional): Whether to use the most probable value or expected value. Defaults to True.\n\n    Returns:\n        Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]: Evaluation results.\n    \"\"\"\n    expected_values: List[Union[int, float]] = []\n    observed_values: List[float] = []\n\n    model_name = get_model_key(model_name, prediction_time)\n\n    for dt in prob_dist_dict_all[model_name].keys():\n        in_ed_preds: Dict[str, Any] = prob_dist_dict_all[model_name][dt]\n        combined = combine_distributions(yta_preds, in_ed_preds[\"agg_predicted\"])\n\n        expected_value: Union[int, float] = (\n            int(combined[\"agg_predicted\"].idxmax())\n            if use_most_probable\n            else float(\n                np.dot(\n                    combined[\"agg_predicted\"].index,\n                    combined[\"agg_predicted\"].values.flatten(),\n                )\n            )\n        )\n\n        observed_value: float = float(\n            calculate_weighted_observed(\n                df, dt, prediction_window, x1, y1, x2, y2, prediction_time\n            )\n        )\n\n        expected_values.append(expected_value)\n        observed_values.append(observed_value)\n\n    results = {model_name: calculate_results(expected_values, observed_values)}\n    return results\n</code></pre>"},{"location":"api/#patientflow.evaluate.evaluate_six_week_average","title":"<code>evaluate_six_week_average(prob_dist_dict_all, df, prediction_window, x1, y1, x2, y2, prediction_time, num_weeks, model_name)</code>","text":"<p>Evaluate the six-week average prediction model.</p> <p>Parameters:</p> Name Type Description Default <code>prob_dist_dict_all</code> <code>Dict[Any, Dict[Any, Dict[str, Any]]]</code> <p>Nested dictionary containing probability distributions.</p> required <code>df</code> <code>DataFrame</code> <p>DataFrame containing patient data.</p> required <code>prediction_window</code> <code>int</code> <p>Prediction window in minutes.</p> required <code>x1</code> <code>float), y1 (float), x2 (float), y2 (float</code> <p>Parameters for aspirational curve.</p> required <code>prediction_time</code> <code>Tuple[int, int]</code> <p>Hour and minute of prediction.</p> required <code>num_weeks</code> <code>int</code> <p>Number of previous weeks to consider.</p> required <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <p>Returns:</p> Type Description <code>Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]</code> <p>Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]: Evaluation results.</p> Source code in <code>src/patientflow/evaluate.py</code> <pre><code>def evaluate_six_week_average(\n    prob_dist_dict_all: Dict[Any, Dict[Any, Dict[str, Any]]],\n    df: pd.DataFrame,\n    prediction_window: int,\n    x1: float,\n    y1: float,\n    x2: float,\n    y2: float,\n    prediction_time: Tuple[int, int],\n    num_weeks: int,\n    model_name: str,\n) -&gt; Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]:\n    \"\"\"\n    Evaluate the six-week average prediction model.\n\n    Args:\n        prob_dist_dict_all (Dict[Any, Dict[Any, Dict[str, Any]]]): Nested dictionary containing probability distributions.\n        df (pd.DataFrame): DataFrame containing patient data.\n        prediction_window (int): Prediction window in minutes.\n        x1 (float), y1 (float), x2 (float), y2 (float): Parameters for aspirational curve.\n        prediction_time (Tuple[int, int]): Hour and minute of prediction.\n        num_weeks (int): Number of previous weeks to consider.\n        model_name (str): Name of the model.\n\n    Returns:\n        Dict[Any, Dict[str, Union[List[Union[int, float]], float]]]: Evaluation results.\n    \"\"\"\n    expected_values: List[Union[int, float]] = []\n    observed_values: List[float] = []\n\n    model_name = get_model_key(model_name, prediction_time)\n\n    for dt in prob_dist_dict_all[model_name].keys():\n        expected_value: float = float(\n            predict_using_previous_weeks(\n                df, dt, prediction_window, x1, y1, x2, y2, prediction_time, num_weeks\n            )\n        )\n        observed_value: float = float(\n            calculate_weighted_observed(\n                df, dt, prediction_window, x1, y1, x2, y2, prediction_time\n            )\n        )\n\n        expected_values.append(expected_value)\n        observed_values.append(observed_value)\n\n    results = {model_name: calculate_results(expected_values, observed_values)}\n    return results\n</code></pre>"},{"location":"api/#patientflow.evaluate.get_arrivals_with_admission_probs","title":"<code>get_arrivals_with_admission_probs(df, prediction_datetime, prediction_window, prediction_time, x1, y1, x2, y2, date_range=None, target_date=None, target_weekday=None)</code>","text":"<p>Get arrivals before and after prediction time with their admission probabilities</p> <p>Parameters: df: DataFrame with arrival_datetime column prediction_datetime: datetime for prediction window start prediction_window: window length in minutes prediction_time: tuple of (hour, minute) x1, y1, x2, y2: parameters for aspirational curve date_range: optional tuple of (start_date, end_date) target_date: optional specific date to analyze target_weekday: optional specific weekday to filter for</p> <p>Returns: tuple of (arrived_before, arrived_after) DataFrames for specified time period</p> Source code in <code>src/patientflow/evaluate.py</code> <pre><code>def get_arrivals_with_admission_probs(\n    df,\n    prediction_datetime,\n    prediction_window,\n    prediction_time,\n    x1,\n    y1,\n    x2,\n    y2,\n    date_range=None,\n    target_date=None,\n    target_weekday=None,\n):\n    \"\"\"\n    Get arrivals before and after prediction time with their admission probabilities\n\n    Parameters:\n    df: DataFrame with arrival_datetime column\n    prediction_datetime: datetime for prediction window start\n    prediction_window: window length in minutes\n    prediction_time: tuple of (hour, minute)\n    x1, y1, x2, y2: parameters for aspirational curve\n    date_range: optional tuple of (start_date, end_date)\n    target_date: optional specific date to analyze\n    target_weekday: optional specific weekday to filter for\n\n    Returns:\n    tuple of (arrived_before, arrived_after) DataFrames for specified time period\n    \"\"\"\n    hour, minute = prediction_time\n\n    # Create base time masks\n    after_mask = create_time_mask(df, hour, minute)\n    before_mask = ~after_mask\n\n    # Add date and weekday conditions if specified\n    if date_range:\n        start_date, end_date = date_range\n        date_mask = (df[\"arrival_datetime\"].dt.date &gt;= start_date) &amp; (\n            df[\"arrival_datetime\"].dt.date &lt; end_date\n        )\n        if target_weekday is not None:\n            date_mask &amp;= df[\"arrival_datetime\"].dt.weekday == target_weekday\n\n        after_mask &amp;= date_mask\n        before_mask &amp;= date_mask\n\n    if target_date:\n        target_mask = df[\"arrival_datetime\"].dt.date == target_date\n        after_mask &amp;= target_mask\n        before_mask &amp;= target_mask\n\n    # Calculate probabilities for filtered groups\n    arrived_before = calculate_admission_probs_relative_to_prediction(\n        df[before_mask],\n        prediction_datetime,\n        prediction_window,\n        x1,\n        y1,\n        x2,\n        y2,\n        is_before=True,\n    )\n\n    arrived_after = calculate_admission_probs_relative_to_prediction(\n        df[after_mask],\n        prediction_datetime,\n        prediction_window,\n        x1,\n        y1,\n        x2,\n        y2,\n        is_before=False,\n    )\n\n    return arrived_before, arrived_after\n</code></pre>"},{"location":"api/#patientflow.evaluate.predict_using_previous_weeks","title":"<code>predict_using_previous_weeks(df, dt, prediction_window, x1, y1, x2, y2, prediction_time, num_weeks, weighted=True)</code>","text":"<p>Calculate predicted admissions remaining until midnight. Args:     df (pd.DataFrame): DataFrame containing patient data.     dt (datetime): Date for prediction.     prediction_time (Tuple[int, int]): Hour and minute of prediction.     num_weeks (int): Number of previous weeks to consider.     weighted(bool): Whether to weight the numbers according to aspirational ED targets Returns:     float: Predicted number of admissions remaining until midnight.</p> Source code in <code>src/patientflow/evaluate.py</code> <pre><code>def predict_using_previous_weeks(\n    df: pd.DataFrame,\n    dt: datetime,\n    prediction_window: int,\n    x1: float,\n    y1: float,\n    x2: float,\n    y2: float,\n    prediction_time: Tuple[int, int],\n    num_weeks: int,\n    weighted=True,\n) -&gt; float:\n    \"\"\"\n    Calculate predicted admissions remaining until midnight.\n    Args:\n        df (pd.DataFrame): DataFrame containing patient data.\n        dt (datetime): Date for prediction.\n        prediction_time (Tuple[int, int]): Hour and minute of prediction.\n        num_weeks (int): Number of previous weeks to consider.\n        weighted(bool): Whether to weight the numbers according to aspirational ED targets\n    Returns:\n        float: Predicted number of admissions remaining until midnight.\n    \"\"\"\n    prediction_datetime = pd.to_datetime(dt).replace(\n        hour=prediction_time[0], minute=prediction_time[1]\n    )\n    target_day_of_week = dt.weekday()\n\n    end_date = dt - timedelta(days=1)\n    start_date = end_date - timedelta(weeks=num_weeks)\n\n    if weighted:\n        # Create mask for historical data\n        historical_mask = (\n            (df[\"arrival_datetime\"].dt.date &gt;= start_date)\n            &amp; (df[\"arrival_datetime\"].dt.date &lt;= end_date)\n            &amp; (df[\"arrival_datetime\"].dt.weekday == target_day_of_week)\n        )\n\n        # Create explicit copy of filtered data\n        historical_data = df[historical_mask].copy()\n\n        # Calculate minutes until midnight\n        midnight_times = (\n            historical_data[\"arrival_datetime\"].dt.normalize()\n            + pd.Timedelta(days=1)\n            - pd.Timedelta(minutes=1)\n        )\n        historical_data.loc[:, \"minutes_to_midnight\"] = (\n            midnight_times - historical_data[\"arrival_datetime\"]\n        ).dt.total_seconds() / 60\n\n        # Calculate admission probabilities\n        historical_data.loc[:, \"admission_probability\"] = historical_data[\n            \"minutes_to_midnight\"\n        ].apply(lambda x: get_y_from_aspirational_curve(x / 60, x1, y1, x2, y2))\n\n        # Group by date and calculate average\n        historical_daily_sums = historical_data.groupby(\n            historical_data[\"arrival_datetime\"].dt.date\n        )[\"admission_probability\"].sum()\n        historical_average = historical_daily_sums.mean()\n\n        # Create mask for today's data\n        today_mask = (df[\"arrival_datetime\"].dt.date == dt) &amp; (\n            df[\"arrival_datetime\"] &lt; prediction_datetime\n        )\n\n        # Create explicit copy of today's filtered data\n        today_data = df[today_mask].copy()\n\n        # Calculate minutes until midnight for today's data\n        midnight_today = (\n            pd.to_datetime(dt).normalize()\n            + pd.Timedelta(days=1)\n            - pd.Timedelta(minutes=1)\n        )\n        today_data.loc[:, \"minutes_to_midnight\"] = (\n            midnight_today - today_data[\"arrival_datetime\"]\n        ).dt.total_seconds() / 60\n\n        # Calculate admission probabilities for today\n        today_data.loc[:, \"admission_probability\"] = today_data[\n            \"minutes_to_midnight\"\n        ].apply(lambda x: get_y_from_aspirational_curve(x / 60, x1, y1, x2, y2))\n\n        today_sum = today_data[\"admission_probability\"].sum()\n\n        still_to_admit = max(historical_average - today_sum, 0)\n\n    else:\n        # Original unweighted logic with explicit copies\n        historical_mask = (\n            (df[\"arrival_datetime\"].dt.date &gt;= start_date)\n            &amp; (df[\"arrival_datetime\"].dt.date &lt; end_date)\n            &amp; (df[\"arrival_datetime\"].dt.weekday == target_day_of_week)\n        )\n        historical_df = df[historical_mask].copy()\n        average_count = len(historical_df) / num_weeks\n\n        target_mask = (df[\"arrival_datetime\"].dt.date == dt) &amp; (\n            df[\"arrival_datetime\"] &lt; prediction_datetime\n        )\n        target_date_count = len(df[target_mask])\n\n        still_to_admit = max(average_count - target_date_count, 0)\n\n    return still_to_admit\n</code></pre>"},{"location":"api/#patientflow.load","title":"<code>load</code>","text":"<p>This module provides functionality for loading configuration files, data from CSV files, and trained machine learning models.</p> <p>It includes the following features:</p> <ul> <li>Loading Configurations: Parse YAML configuration files and extract necessary parameters for data processing and modeling.</li> <li>Data Handling: Load and preprocess data from CSV files, including optional operations like setting an index, sorting, and applying literal evaluation on columns.</li> <li>Model Management: Load saved machine learning models, customize model filenames based on time, and categorize DataFrame columns into predefined groups for analysis.</li> </ul> <p>The module handles common file and parsing errors, returning appropriate error messages or exceptions.</p>"},{"location":"api/#patientflow.load--functions","title":"Functions","text":"<p>parse_args:     Parses command-line arguments for training models. set_project_root:     Validates project root path from specified environment variable. load_config_file:     Load a YAML configuration file and extract key parameters. set_file_paths:     Sets up the file paths based on UCLH-specific or default parameters. set_data_file_names:     Set file locations based on UCLH-specific or default data sources. safe_literal_eval:     Safely evaluate string literals into Python objects when loading from csv. load_data:     Load and preprocess data from a CSV or pickle file. get_model_key:     Generate a model name based on the time of day. load_saved_model:     Load a machine learning model saved in a joblib file. get_dict_cols:     Categorize columns from a DataFrame into predefined groups for analysis.</p>"},{"location":"api/#patientflow.load.data_from_csv","title":"<code>data_from_csv(csv_path, index_column=None, sort_columns=None, eval_columns=None)</code>","text":"<p>Loads data from a CSV file, with optional transformations. LEGACY!</p> <p>This function loads a CSV file into a pandas DataFrame and provides the following optional features: - Setting a specified column as the index. - Sorting the DataFrame by one or more specified columns. - Applying safe literal evaluation to specified columns to handle string representations of Python objects.</p>"},{"location":"api/#patientflow.load.data_from_csv--parameters","title":"Parameters","text":"<p>csv_path : str     The relative or absolute path to the CSV file. index_column : str, optional     The column to set as the index of the DataFrame. If not provided, no index column is set. sort_columns : list of str, optional     A list of columns by which to sort the DataFrame. If not provided, the DataFrame is not sorted. eval_columns : list of str, optional     A list of columns to which <code>safe_literal_eval</code> should be applied. This is useful for columns containing     string representations of Python data structures (e.g., lists, dictionaries).</p>"},{"location":"api/#patientflow.load.data_from_csv--returns","title":"Returns","text":"<p>pd.DataFrame     A pandas DataFrame containing the loaded data with any specified transformations applied.</p>"},{"location":"api/#patientflow.load.data_from_csv--raises","title":"Raises","text":"<p>SystemExit     If the file cannot be found or another error occurs during loading or processing.</p>"},{"location":"api/#patientflow.load.data_from_csv--notes","title":"Notes","text":"<p>The function will terminate the program with a message if the file is not found or if any errors occur while loading the data. If sorting columns or applying <code>safe_literal_eval</code> fails, a warning message is printed, but execution continues.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def data_from_csv(csv_path, index_column=None, sort_columns=None, eval_columns=None):\n    \"\"\"\n    Loads data from a CSV file, with optional transformations. LEGACY!\n\n    This function loads a CSV file into a pandas DataFrame and provides the following optional features:\n    - Setting a specified column as the index.\n    - Sorting the DataFrame by one or more specified columns.\n    - Applying safe literal evaluation to specified columns to handle string representations of Python objects.\n\n    Parameters\n    ----------\n    csv_path : str\n        The relative or absolute path to the CSV file.\n    index_column : str, optional\n        The column to set as the index of the DataFrame. If not provided, no index column is set.\n    sort_columns : list of str, optional\n        A list of columns by which to sort the DataFrame. If not provided, the DataFrame is not sorted.\n    eval_columns : list of str, optional\n        A list of columns to which `safe_literal_eval` should be applied. This is useful for columns containing\n        string representations of Python data structures (e.g., lists, dictionaries).\n\n    Returns\n    -------\n    pd.DataFrame\n        A pandas DataFrame containing the loaded data with any specified transformations applied.\n\n    Raises\n    ------\n    SystemExit\n        If the file cannot be found or another error occurs during loading or processing.\n\n    Notes\n    -----\n    The function will terminate the program with a message if the file is not found or if any errors\n    occur while loading the data. If sorting columns or applying `safe_literal_eval` fails,\n    a warning message is printed, but execution continues.\n\n    \"\"\"\n    path = os.path.join(Path().home(), csv_path)\n\n    if not os.path.exists(path):\n        print(f\"Data file not found at path: {path}\")\n        sys.exit(1)\n\n    try:\n        df = pd.read_csv(path, parse_dates=True)\n    except FileNotFoundError:\n        print(f\"Data file not found at path: {path}\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"Error loading data: {e}\")\n        sys.exit(1)\n\n    if index_column:\n        try:\n            if df.index.name != index_column:\n                df = df.set_index(index_column)\n        except KeyError:\n            print(f\"Index column '{index_column}' not found in dataframe\")\n\n    if sort_columns:\n        try:\n            df.sort_values(sort_columns, inplace=True)\n        except KeyError:\n            print(\"One or more sort columns not found in dataframe\")\n\n    if eval_columns:\n        for column in eval_columns:\n            if column in df.columns:\n                try:\n                    df[column] = df[column].apply(safe_literal_eval)\n                except Exception as e:\n                    print(f\"Error applying safe_literal_eval to column '{column}': {e}\")\n\n    return df\n</code></pre>"},{"location":"api/#patientflow.load.get_dict_cols","title":"<code>get_dict_cols(df)</code>","text":"<p>Categorize DataFrame columns into predefined groups.</p>"},{"location":"api/#patientflow.load.get_dict_cols--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The DataFrame to categorize.</p>"},{"location":"api/#patientflow.load.get_dict_cols--returns","title":"Returns","text":"<p>dict     A dictionary where keys are column group names and values are lists of column names in each group.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def get_dict_cols(df):\n    \"\"\"\n    Categorize DataFrame columns into predefined groups.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The DataFrame to categorize.\n\n    Returns\n    -------\n    dict\n        A dictionary where keys are column group names and values are lists of column names in each group.\n    \"\"\"\n    not_used_in_training_vars = [\n        \"snapshot_id\",\n        \"snapshot_date\",\n        \"prediction_time\",\n        \"visit_number\",\n        \"training_validation_test\",\n        \"random_number\",\n    ]\n    arrival_and_demographic_vars = [\n        \"elapsed_los\",\n        \"sex\",\n        \"age_group\",\n        \"age_on_arrival\",\n        \"arrival_method\",\n    ]\n    summary_vars = [\n        \"num_obs\",\n        \"num_obs_events\",\n        \"num_obs_types\",\n        \"num_lab_batteries_ordered\",\n    ]\n\n    location_vars = []\n    observations_vars = []\n    labs_vars = []\n    consults_vars = [\n        \"has_consultation\",\n        \"consultation_sequence\",\n        \"final_sequence\",\n        \"specialty\",\n    ]\n    outcome_vars = [\"is_admitted\"]\n\n    for col in df.columns:\n        if (\n            col in not_used_in_training_vars\n            or col in arrival_and_demographic_vars\n            or col in summary_vars\n        ):\n            continue\n        elif \"visited\" in col or \"location\" in col:\n            location_vars.append(col)\n        elif \"num_obs\" in col or \"latest_obs\" in col:\n            observations_vars.append(col)\n        elif \"lab_orders\" in col or \"latest_lab_results\" in col:\n            labs_vars.append(col)\n        elif col in consults_vars or col in outcome_vars:\n            continue  # Already categorized\n        else:\n            print(f\"Column '{col}' did not match any predefined group\")\n\n    # Create a list of column groups\n    col_group_names = [\n        \"not used in training\",\n        \"arrival and demographic\",\n        \"summary\",\n        \"location\",\n        \"observations\",\n        \"lab orders and results\",\n        \"consults\",\n        \"outcome\",\n    ]\n\n    # Create a list of the column names within those groups\n    col_groups = [\n        not_used_in_training_vars,\n        arrival_and_demographic_vars,\n        summary_vars,\n        location_vars,\n        observations_vars,\n        labs_vars,\n        consults_vars,\n        outcome_vars,\n    ]\n\n    # Use dictionary to combine them\n    dict_col_groups = {\n        category: var_list for category, var_list in zip(col_group_names, col_groups)\n    }\n\n    return dict_col_groups\n</code></pre>"},{"location":"api/#patientflow.load.get_model_key","title":"<code>get_model_key(model_name, prediction_time)</code>","text":"<p>Create a model name based on the time of day.</p>"},{"location":"api/#patientflow.load.get_model_key--parameters","title":"Parameters","text":"<p>model_name : str     The base name of the model. prediction_time_ : tuple of int     A tuple representing the time of day (hour, minute).</p>"},{"location":"api/#patientflow.load.get_model_key--returns","title":"Returns","text":"<p>str     A string representing the model name based on the time of day.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def get_model_key(model_name, prediction_time):\n    \"\"\"\n    Create a model name based on the time of day.\n\n    Parameters\n    ----------\n    model_name : str\n        The base name of the model.\n    prediction_time_ : tuple of int\n        A tuple representing the time of day (hour, minute).\n\n    Returns\n    -------\n    str\n        A string representing the model name based on the time of day.\n    \"\"\"\n\n    hour_, min_ = prediction_time\n    min_ = f\"{min_}0\" if min_ % 60 == 0 else str(min_)\n    model_name = model_name + \"_\" + f\"{hour_:02}\" + min_\n    return model_name\n</code></pre>"},{"location":"api/#patientflow.load.load_config_file","title":"<code>load_config_file(config_file_path, return_start_end_dates=False)</code>","text":"<p>Load configuration from a YAML file.</p>"},{"location":"api/#patientflow.load.load_config_file--parameters","title":"Parameters","text":"<p>config_file_path : str     The path to the configuration file. return_start_end_dates : bool, optional     If True, return only the start and end dates from the file (default is False).</p>"},{"location":"api/#patientflow.load.load_config_file--returns","title":"Returns","text":"<p>dict or tuple or None     If <code>return_start_end_dates</code> is True, returns a tuple of start and end dates (str).     Otherwise, returns a dictionary containing the configuration parameters.     Returns None if an error occurs during file reading or parsing.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def load_config_file(\n    config_file_path: str, return_start_end_dates: bool = False\n) -&gt; Optional[Union[Dict[str, Any], Tuple[str, str]]]:\n    \"\"\"\n    Load configuration from a YAML file.\n\n    Parameters\n    ----------\n    config_file_path : str\n        The path to the configuration file.\n    return_start_end_dates : bool, optional\n        If True, return only the start and end dates from the file (default is False).\n\n    Returns\n    -------\n    dict or tuple or None\n        If `return_start_end_dates` is True, returns a tuple of start and end dates (str).\n        Otherwise, returns a dictionary containing the configuration parameters.\n        Returns None if an error occurs during file reading or parsing.\n    \"\"\"\n    try:\n        with open(config_file_path, \"r\") as file:\n            config = yaml.safe_load(file)\n    except FileNotFoundError:\n        print(f\"Error: The file '{config_file_path}' was not found.\")\n        return None\n    except yaml.YAMLError as e:\n        print(f\"Error parsing YAML file: {e}\")\n        return None\n\n    try:\n        if return_start_end_dates:\n            # load the dates used in saved data for uclh versions\n            if \"file_dates\" in config and config[\"file_dates\"]:\n                start_date, end_date = [str(item) for item in config[\"file_dates\"]]\n                return (start_date, end_date)\n            else:\n                print(\n                    \"Error: 'file_dates' key not found or empty in the configuration file.\"\n                )\n                return None\n\n        params: Dict[str, Any] = {}\n\n        if \"prediction_times\" in config:\n            params[\"prediction_times\"] = [\n                tuple(item) for item in config[\"prediction_times\"]\n            ]\n        else:\n            print(\"Error: 'prediction_times' key not found in the configuration file.\")\n            sys.exit(1)\n\n        if \"modelling_dates\" in config and len(config[\"modelling_dates\"]) == 4:\n            (\n                params[\"start_training_set\"],\n                params[\"start_validation_set\"],\n                params[\"start_test_set\"],\n                params[\"end_test_set\"],\n            ) = [item for item in config[\"modelling_dates\"]]\n        else:\n            print(\n                f\"Error: expecting 4 modelling dates and only got {len(config.get('modelling_dates', []))}\"\n            )\n            return None\n\n        params[\"x1\"] = float(config.get(\"x1\", 4))\n        params[\"y1\"] = float(config.get(\"y1\", 0.76))\n        params[\"x2\"] = float(config.get(\"x2\", 12))\n        params[\"y2\"] = float(config.get(\"y2\", 0.99))\n        params[\"prediction_window\"] = config.get(\"prediction_window\", 480)\n        params[\"epsilon\"] = config.get(\"epsilon\", 10**-7)\n        params[\"yta_time_interval\"] = config.get(\"yta_time_interval\", 15)\n\n        return params\n\n    except KeyError as e:\n        print(f\"Error: Missing key in the configuration file: {e}\")\n        return None\n    except ValueError as e:\n        print(f\"Error: Invalid value found in the configuration file: {e}\")\n        return None\n</code></pre>"},{"location":"api/#patientflow.load.load_data","title":"<code>load_data(data_file_path, file_name, index_column=None, sort_columns=None, eval_columns=None, home_path=None)</code>","text":"<p>Loads data from CSV or pickle file with optional transformations.</p>"},{"location":"api/#patientflow.load.load_data--parameters","title":"Parameters","text":"<p>data_file_path : str     Directory path containing the data file file_name : str     Name of the CSV or pickle file to load index_column : str, optional     Column to set as DataFrame index sort_columns : list of str, optional     Columns to sort DataFrame by eval_columns : list of str, optional     Columns to apply safe_literal_eval to home_path : str or Path, optional     Base path to use instead of user's home directory</p>"},{"location":"api/#patientflow.load.load_data--returns","title":"Returns","text":"<p>pd.DataFrame     Loaded and transformed DataFrame</p>"},{"location":"api/#patientflow.load.load_data--raises","title":"Raises","text":"<p>FileNotFoundError     If the specified file does not exist ValueError     If the file format is not supported or other processing errors occur</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def load_data(\n    data_file_path,\n    file_name,\n    index_column=None,\n    sort_columns=None,\n    eval_columns=None,\n    home_path=None,\n):\n    \"\"\"\n    Loads data from CSV or pickle file with optional transformations.\n\n    Parameters\n    ----------\n    data_file_path : str\n        Directory path containing the data file\n    file_name : str\n        Name of the CSV or pickle file to load\n    index_column : str, optional\n        Column to set as DataFrame index\n    sort_columns : list of str, optional\n        Columns to sort DataFrame by\n    eval_columns : list of str, optional\n        Columns to apply safe_literal_eval to\n    home_path : str or Path, optional\n        Base path to use instead of user's home directory\n\n    Returns\n    -------\n    pd.DataFrame\n        Loaded and transformed DataFrame\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified file does not exist\n    ValueError\n        If the file format is not supported or other processing errors occur\n    \"\"\"\n    from pathlib import Path\n\n    # Use provided home_path if available, otherwise default to user's home directory\n    base_path = Path(home_path) if home_path else Path.home()\n    path = base_path / data_file_path / file_name\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Data file not found at path: {path}\")\n\n    try:\n        if path.suffix.lower() == \".csv\":\n            df = pd.read_csv(path, parse_dates=True)\n        elif path.suffix.lower() == \".pkl\":\n            df = pd.read_pickle(path)\n        else:\n            raise ValueError(\n                f\"Unsupported file format: {path.suffix}. Must be .csv or .pkl\"\n            )\n    except Exception as e:\n        raise ValueError(f\"Error loading data: {str(e)}\")\n\n    if index_column and df.index.name != index_column:\n        try:\n            df = df.set_index(index_column)\n        except KeyError:\n            print(f\"Warning: Index column '{index_column}' not found in dataframe\")\n\n    if sort_columns:\n        try:\n            df.sort_values(sort_columns, inplace=True)\n        except KeyError:\n            print(\"Warning: One or more sort columns not found in dataframe\")\n\n    if eval_columns:\n        for column in eval_columns:\n            if column in df.columns:\n                try:\n                    df[column] = df[column].apply(safe_literal_eval)\n                except Exception as e:\n                    print(\n                        f\"Warning: Error applying safe_literal_eval to column '{column}': {str(e)}\"\n                    )\n\n    return df\n</code></pre>"},{"location":"api/#patientflow.load.load_saved_model","title":"<code>load_saved_model(model_file_path, model_name, prediction_time=None)</code>","text":"<p>Load a saved model from a file.</p>"},{"location":"api/#patientflow.load.load_saved_model--parameters","title":"Parameters","text":"<p>model_file_path : Path     The path to the directory where the model is saved. model_name : str     The base name of the model. prediction_time : tuple of int, optional     The time of day the model was trained for.</p>"},{"location":"api/#patientflow.load.load_saved_model--returns","title":"Returns","text":"<p>Any     The loaded model.</p>"},{"location":"api/#patientflow.load.load_saved_model--raises","title":"Raises","text":"<p>ModelLoadError     If the model file cannot be found or loaded.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def load_saved_model(model_file_path, model_name, prediction_time=None):\n    \"\"\"\n    Load a saved model from a file.\n\n    Parameters\n    ----------\n    model_file_path : Path\n        The path to the directory where the model is saved.\n    model_name : str\n        The base name of the model.\n    prediction_time : tuple of int, optional\n        The time of day the model was trained for.\n\n    Returns\n    -------\n    Any\n        The loaded model.\n\n    Raises\n    ------\n    ModelLoadError\n        If the model file cannot be found or loaded.\n    \"\"\"\n    if prediction_time:\n        # retrieve model based on the time of day it is trained for\n        model_name = get_model_key(model_name, prediction_time)\n\n    full_path = model_file_path / model_name\n    full_path = full_path.with_suffix(\".joblib\")\n\n    try:\n        model = load(full_path)\n        return model\n    except FileNotFoundError:\n        # print(f\"Model named {model_name} not found at path: {model_file_path}\")\n        raise ModelLoadError(\n            f\"Model named {model_name} not found at path: {model_file_path}\"\n        )\n    except Exception as e:\n        # print(f\"Error loading model: {e}\")\n        raise ModelLoadError(f\"Error loading model called {model_name}: {e}\")\n</code></pre>"},{"location":"api/#patientflow.load.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse command-line arguments for the training script.</p> <p>Returns:</p> Type Description <code>Namespace</code> <p>argparse.Namespace: The parsed arguments containing 'data_folder_name' and 'uclh' keys.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def parse_args() -&gt; argparse.Namespace:\n    \"\"\"\n    Parse command-line arguments for the training script.\n\n    Returns:\n        argparse.Namespace: The parsed arguments containing 'data_folder_name' and 'uclh' keys.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Train emergency demand models\")\n    parser.add_argument(\n        \"--data_folder_name\",\n        type=str,\n        default=\"data-synthetic\",\n        help=\"Location of data for training\",\n    )\n    parser.add_argument(\n        \"--uclh\",\n        type=lambda x: x.lower() in [\"true\", \"1\", \"yes\", \"y\"],\n        default=False,\n        help=\"Train using UCLH data (True) or Public data (False)\",\n    )\n    args = parser.parse_args()\n    return args\n</code></pre>"},{"location":"api/#patientflow.load.safe_literal_eval","title":"<code>safe_literal_eval(s)</code>","text":"<p>Safely evaluate a string literal into a Python object. Handles list-like strings by converting them to lists.</p>"},{"location":"api/#patientflow.load.safe_literal_eval--parameters","title":"Parameters","text":"<p>s : str     The string to evaluate.</p>"},{"location":"api/#patientflow.load.safe_literal_eval--returns","title":"Returns","text":"<p>Any, list, or None     The evaluated Python object if successful, a list if the input is list-like,     or None for empty/null values.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def safe_literal_eval(s):\n    \"\"\"\n    Safely evaluate a string literal into a Python object.\n    Handles list-like strings by converting them to lists.\n\n    Parameters\n    ----------\n    s : str\n        The string to evaluate.\n\n    Returns\n    -------\n    Any, list, or None\n        The evaluated Python object if successful, a list if the input is list-like,\n        or None for empty/null values.\n    \"\"\"\n    if pd.isna(s) or str(s).strip().lower() in [\"nan\", \"none\", \"\"]:\n        return None\n\n    if isinstance(s, str):\n        s = s.strip()\n        if s.startswith(\"[\") and s.endswith(\"]\"):\n            try:\n                # Remove square brackets and split by comma\n                items = s[1:-1].split(\",\")\n                # Strip whitespace from each item and remove empty strings\n                return [item.strip() for item in items if item.strip()]\n            except Exception:\n                # If the above fails, fall back to ast.literal_eval\n                pass\n\n    try:\n        return ast.literal_eval(s)\n    except (ValueError, SyntaxError):\n        # If ast.literal_eval fails, return the original string\n        return s\n</code></pre>"},{"location":"api/#patientflow.load.set_data_file_names","title":"<code>set_data_file_names(uclh, data_file_path, config_file_path=None)</code>","text":"<p>Set file locations based on UCLH or default data source.</p>"},{"location":"api/#patientflow.load.set_data_file_names--parameters","title":"Parameters","text":"<p>uclh : bool     If True, use UCLH-specific file locations. If False, use default file locations. data_file_path : Path     The base path to the data directory. config_file_path : str, optional     The path to the configuration file, required if <code>uclh</code> is True.</p>"},{"location":"api/#patientflow.load.set_data_file_names--returns","title":"Returns","text":"<p>tuple     Paths to the required files (visits, arrivals) based on the configuration.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def set_data_file_names(uclh, data_file_path, config_file_path=None):\n    \"\"\"\n    Set file locations based on UCLH or default data source.\n\n    Parameters\n    ----------\n    uclh : bool\n        If True, use UCLH-specific file locations. If False, use default file locations.\n    data_file_path : Path\n        The base path to the data directory.\n    config_file_path : str, optional\n        The path to the configuration file, required if `uclh` is True.\n\n    Returns\n    -------\n    tuple\n        Paths to the required files (visits, arrivals) based on the configuration.\n    \"\"\"\n    if not isinstance(data_file_path, Path):\n        data_file_path = Path(data_file_path)\n\n    if not uclh:\n        csv_filename = \"ed_visits.csv\"\n        yta_csv_filename = \"inpatient_arrivals.csv\"\n\n        visits_csv_path = data_file_path / csv_filename\n        yta_csv_path = data_file_path / yta_csv_filename\n\n        return visits_csv_path, yta_csv_path\n\n    else:\n        start_date, end_date = load_config_file(\n            config_file_path, return_start_end_dates=True\n        )\n        data_filename = (\n            \"uclh_visits_exc_beds_inc_minority_\"\n            + str(start_date)\n            + \"_\"\n            + str(end_date)\n            + \".pickle\"\n        )\n        csv_filename = \"uclh_ed_visits.csv\"\n        yta_filename = (\n            \"uclh_yet_to_arrive_\" + str(start_date) + \"_\" + str(end_date) + \".pickle\"\n        )\n        yta_csv_filename = \"uclh_inpatient_arrivals.csv\"\n\n        visits_path = data_file_path / data_filename\n        yta_path = data_file_path / yta_filename\n\n        visits_csv_path = data_file_path / csv_filename\n        yta_csv_path = data_file_path / yta_csv_filename\n\n    return visits_path, visits_csv_path, yta_path, yta_csv_path\n</code></pre>"},{"location":"api/#patientflow.load.set_file_paths","title":"<code>set_file_paths(project_root, data_folder_name, train_dttm=None, inference_time=False, config_file='config.yaml', prefix=None, verbose=True)</code>","text":"<p>Sets up the file paths</p> <p>Parameters:</p> Name Type Description Default <code>project_root</code> <code>Path</code> <p>Root path of the project</p> required <code>data_folder_name</code> <code>str</code> <p>Name of the folder where data files are located</p> required <code>train_dttm</code> <code>Optional[str]</code> <p>A string representation of the datetime at which training commenced. Defaults to None</p> <code>None</code> <code>inference_time</code> <code>bool</code> <p>A flag indicating whether it is inference time or not. Defaults to False</p> <code>False</code> <code>config_file</code> <code>str</code> <p>Name of config file. Defaults to \"config.yaml\"</p> <code>'config.yaml'</code> <code>prefix</code> <code>Optional[str]</code> <p>String to prefix model folder names. Defaults to None</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print path information. Defaults to True</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Path, Path, Path, Path]</code> <p>Contains (data_file_path, media_file_path, model_file_path, config_path)</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def set_file_paths(\n    project_root: Path,\n    data_folder_name: str,\n    train_dttm: Optional[str] = None,\n    inference_time: bool = False,\n    config_file: str = \"config.yaml\",\n    prefix: Optional[str] = None,\n    verbose: bool = True,\n) -&gt; Tuple[Path, Path, Path, Path]:\n    \"\"\"\n    Sets up the file paths\n\n    Args:\n        project_root (Path): Root path of the project\n        data_folder_name (str): Name of the folder where data files are located\n        train_dttm (Optional[str], optional): A string representation of the datetime at which training commenced. Defaults to None\n        inference_time (bool, optional): A flag indicating whether it is inference time or not. Defaults to False\n        config_file (str, optional): Name of config file. Defaults to \"config.yaml\"\n        prefix (Optional[str], optional): String to prefix model folder names. Defaults to None\n        verbose (bool, optional): Whether to print path information. Defaults to True\n\n    Returns:\n        tuple: Contains (data_file_path, media_file_path, model_file_path, config_path)\n    \"\"\"\n\n    config_path = Path(project_root) / config_file\n    if verbose:\n        print(f\"Configuration will be loaded from: {config_path}\")\n\n    data_file_path = Path(project_root) / data_folder_name\n    if verbose:\n        print(f\"Data files will be loaded from: {data_file_path}\")\n\n    model_id = data_folder_name.lstrip(\"data-\")\n    if prefix:\n        model_id = f\"{prefix}_{model_id}\"\n    if train_dttm:\n        model_id = f\"{model_id}_{train_dttm}\"\n\n    model_file_path = Path(project_root) / \"trained-models\" / model_id\n    media_file_path = model_file_path / \"media\"\n\n    if not inference_time:\n        if verbose:\n            print(f\"Trained models will be saved to: {model_file_path}\")\n        model_file_path.mkdir(parents=True, exist_ok=True)\n        (model_file_path / \"model-output\").mkdir(parents=False, exist_ok=True)\n        media_file_path.mkdir(parents=False, exist_ok=True)\n        if verbose:\n            print(f\"Images will be saved to: {media_file_path}\")\n\n    return data_file_path, media_file_path, model_file_path, config_path\n</code></pre>"},{"location":"api/#patientflow.load.set_project_root","title":"<code>set_project_root(env_var=None)</code>","text":"<p>Sets project root path from environment variable or infers it from current path.</p> <p>First checks specified environment variable for project root path. If not found, searches current path hierarchy for highest-level 'patientflow' directory.</p> <p>Parameters:</p> Name Type Description Default <code>env_var</code> <code>Optional[str]</code> <p>Name of environment variable containing project root path</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Validated project root path</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If environment variable not set and 'patientflow' not found in path</p> <code>NotADirectoryError</code> <p>If path doesn't exist</p> <code>TypeError</code> <p>If env_var is not None and not a string</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def set_project_root(env_var: Optional[str] = None) -&gt; Path:\n    \"\"\"\n    Sets project root path from environment variable or infers it from current path.\n\n    First checks specified environment variable for project root path.\n    If not found, searches current path hierarchy for highest-level 'patientflow' directory.\n\n    Args:\n        env_var (Optional[str]): Name of environment variable containing project root path\n\n    Returns:\n        Path: Validated project root path\n\n    Raises:\n        ValueError: If environment variable not set and 'patientflow' not found in path\n        NotADirectoryError: If path doesn't exist\n        TypeError: If env_var is not None and not a string\n    \"\"\"\n    # Only try to get env path if env_var is provided\n    env_path: Optional[str] = os.getenv(env_var) if env_var is not None else None\n    project_root: Optional[Path] = None\n\n    # Try getting from environment variable first\n    if env_path is not None:\n        try:\n            project_root = Path(env_path)\n            if not project_root.is_dir():\n                raise NotADirectoryError(f\"Path does not exist: {project_root}\")\n            print(f\"Project root from environment: {project_root}\")\n            return project_root\n        except (TypeError, ValueError) as e:\n            print(f\"Error converting {env_path} to Path: {e}\")\n            raise\n    else:\n        # If not in env var, try to infer from current path\n        current: Path = Path().absolute()\n\n        # Search through parents to find highest-level 'patientflow' directory\n        for parent in [current, *current.parents]:\n            if parent.name == \"patientflow\" and parent.is_dir():\n                project_root = parent\n                # Continue searching to find highest level\n\n        if project_root:\n            print(f\"Inferred project root: {project_root}\")\n            return project_root\n\n        print(\n            f\"Could not find project root - {env_var} not set and 'patientflow' not found in path\"\n        )\n        print(f\"\\nCurrent directory: {Path().absolute()}\")\n        if env_var:\n            print(f\"\\nRun one of these commands in a new cell to set {env_var}:\")\n            print(\"# Linux/Mac:\")\n            print(f\"%env {env_var}=/path/to/project\")\n            print(\"\\n# Windows:\")\n            print(f\"%env {env_var}=C:\\\\path\\\\to\\\\project\")\n        raise ValueError(\"Project root not found\")\n</code></pre>"},{"location":"api/#patientflow.mask","title":"<code>mask</code>","text":""},{"location":"api/#patientflow.mask.calculate_shift_delta","title":"<code>calculate_shift_delta(seed, min_weeks=520, max_weeks=520 * 2)</code>","text":"<p>Calculate a consistent time delta based on the provided seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed for consistent shifts</p> required <code>min_weeks</code> <code>int</code> <p>Minimum number of weeks to shift</p> <code>520</code> <code>max_weeks</code> <code>int</code> <p>Maximum number of weeks to shift</p> <code>520 * 2</code> <p>Returns:</p> Name Type Description <code>timedelta</code> <code>timedelta</code> <p>The time shift to apply</p> Source code in <code>src/patientflow/mask.py</code> <pre><code>def calculate_shift_delta(\n    seed: int, min_weeks: int = 520, max_weeks: int = 520 * 2\n) -&gt; timedelta:\n    \"\"\"\n    Calculate a consistent time delta based on the provided seed.\n\n    Args:\n        seed (int): Random seed for consistent shifts\n        min_weeks (int): Minimum number of weeks to shift\n        max_weeks (int): Maximum number of weeks to shift\n\n    Returns:\n        timedelta: The time shift to apply\n    \"\"\"\n    random.seed(seed)\n    weeks_to_add = random.randint(min_weeks, max_weeks)\n    return timedelta(weeks=weeks_to_add)\n</code></pre>"},{"location":"api/#patientflow.mask.hash_csn","title":"<code>hash_csn(df, salt)</code>","text":"<p>Consistently hash CSN values in a dataframe Returns a new dataframe with hashed CSN column</p> Source code in <code>src/patientflow/mask.py</code> <pre><code>def hash_csn(df, salt):\n    \"\"\"\n    Consistently hash CSN values in a dataframe\n    Returns a new dataframe with hashed CSN column\n    \"\"\"\n    # Create a copy to avoid modifying original\n    df_hashed = df.copy()\n\n    def hash_value(value):\n        if pd.isna(value):\n            return None\n        salted = f\"{str(value)}{salt}\".encode()\n        return hashlib.sha256(salted).hexdigest()[:12]\n\n    # Apply the hash function to the CSN column\n    df_hashed[\"csn\"] = df_hashed[\"csn\"].apply(hash_value)\n\n    return df_hashed\n</code></pre>"},{"location":"api/#patientflow.mask.shift_dates","title":"<code>shift_dates(data, seed, min_weeks=520, max_weeks=520 * 2, start_date=None, end_date=None)</code>","text":"<p>Shift dates either in a DataFrame or create shifted dates from a date range.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Timestamp], DatetimeIndex]</code> <p>Either a DataFrame with datetime columns to shift, or None if using start/end dates</p> required <code>seed</code> <code>int</code> <p>Random seed for consistent shifts</p> required <code>min_weeks</code> <code>int</code> <p>Minimum number of weeks to shift</p> <code>520</code> <code>max_weeks</code> <code>int</code> <p>Maximum number of weeks to shift</p> <code>520 * 2</code> <code>start_date</code> <code>Optional[Union[str, Timestamp]]</code> <p>Start date if generating a date range (only used if data is None)</p> <code>None</code> <code>end_date</code> <code>Optional[Union[str, Timestamp]]</code> <p>End date if generating a date range (only used if data is None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[DataFrame, List[Timestamp]]</code> <p>Union[pd.DataFrame, List[pd.Timestamp]]: Either the shifted DataFrame or list of shifted dates</p> Source code in <code>src/patientflow/mask.py</code> <pre><code>def shift_dates(\n    data: Union[pd.DataFrame, List[pd.Timestamp], pd.DatetimeIndex],\n    seed: int,\n    min_weeks: int = 520,\n    max_weeks: int = 520 * 2,\n    start_date: Optional[Union[str, pd.Timestamp]] = None,\n    end_date: Optional[Union[str, pd.Timestamp]] = None,\n) -&gt; Union[pd.DataFrame, List[pd.Timestamp]]:\n    \"\"\"\n    Shift dates either in a DataFrame or create shifted dates from a date range.\n\n    Args:\n        data: Either a DataFrame with datetime columns to shift, or None if using start/end dates\n        seed: Random seed for consistent shifts\n        min_weeks: Minimum number of weeks to shift\n        max_weeks: Maximum number of weeks to shift\n        start_date: Start date if generating a date range (only used if data is None)\n        end_date: End date if generating a date range (only used if data is None)\n\n    Returns:\n        Union[pd.DataFrame, List[pd.Timestamp]]: Either the shifted DataFrame or list of shifted dates\n    \"\"\"\n    shift_delta = calculate_shift_delta(seed, min_weeks, max_weeks)\n\n    # Case 1: Shifting DataFrame columns\n    if isinstance(data, pd.DataFrame):\n        df_copy = data.copy()\n        datetime_cols = df_copy.select_dtypes(\n            include=[\"datetime64[ns]\", \"datetime64\"]\n        ).columns\n        for col in datetime_cols:\n            df_copy[col] = df_copy[col].apply(\n                lambda x: x + shift_delta if pd.notna(x) else x\n            )\n        return df_copy\n\n    # Case 2: Generating shifted date range\n    elif start_date is not None and end_date is not None:\n        original_dates = pd.date_range(\n            start=start_date, end=end_date, freq=\"D\"\n        ).date.tolist()[:-1]\n        return [date + shift_delta for date in original_dates]\n\n    # Case 3: Shifting list of dates or DatetimeIndex\n    elif isinstance(data, (list, pd.DatetimeIndex)):\n        return [date + shift_delta for date in data]\n\n    else:\n        raise ValueError(\n            \"Must provide either a DataFrame, date range parameters, or list of dates\"\n        )\n</code></pre>"},{"location":"api/#patientflow.model_artifacts","title":"<code>model_artifacts</code>","text":""},{"location":"api/#patientflow.model_artifacts.FoldResults","title":"<code>FoldResults</code>  <code>dataclass</code>","text":"<p>Store evaluation metrics for a single fold.</p> Source code in <code>src/patientflow/model_artifacts.py</code> <pre><code>@dataclass\nclass FoldResults:\n    \"\"\"Store evaluation metrics for a single fold.\"\"\"\n\n    auc: float\n    logloss: float\n    auprc: float\n</code></pre>"},{"location":"api/#patientflow.model_artifacts.HyperParameterTrial","title":"<code>HyperParameterTrial</code>  <code>dataclass</code>","text":"<p>Container for a single hyperparameter tuning trial.</p> Source code in <code>src/patientflow/model_artifacts.py</code> <pre><code>@dataclass\nclass HyperParameterTrial:\n    \"\"\"Container for a single hyperparameter tuning trial.\"\"\"\n\n    parameters: Dict[str, Any]\n    cv_results: Dict[str, float]\n</code></pre>"},{"location":"api/#patientflow.model_artifacts.TrainedClassifier","title":"<code>TrainedClassifier</code>  <code>dataclass</code>","text":"<p>Container for trained model artifacts and their associated information.</p> Source code in <code>src/patientflow/model_artifacts.py</code> <pre><code>@dataclass\nclass TrainedClassifier:\n    \"\"\"Container for trained model artifacts and their associated information.\"\"\"\n\n    training_results: TrainingResults\n    pipeline: Optional[Pipeline] = None\n    calibrated_pipeline: Optional[Pipeline] = None\n</code></pre>"},{"location":"api/#patientflow.model_artifacts.TrainingResults","title":"<code>TrainingResults</code>  <code>dataclass</code>","text":"<p>Store comprehensive evaluation metrics and metadata from model training.</p> Source code in <code>src/patientflow/model_artifacts.py</code> <pre><code>@dataclass\nclass TrainingResults:\n    \"\"\"Store comprehensive evaluation metrics and metadata from model training.\"\"\"\n\n    prediction_time: Tuple[int, int]\n    training_info: Dict[str, Any] = field(default_factory=dict)\n    calibration_info: Dict[str, Any] = field(default_factory=dict)\n    test_results: Dict[str, float] = field(default_factory=dict)\n    balance_info: Dict[str, Union[bool, int, float]] = field(default_factory=dict)\n</code></pre>"},{"location":"api/#patientflow.predict","title":"<code>predict</code>","text":""},{"location":"api/#patientflow.predict.emergency_demand","title":"<code>emergency_demand</code>","text":""},{"location":"api/#patientflow.predict.emergency_demand.create_predictions","title":"<code>create_predictions(models, prediction_time, prediction_snapshots, specialties, prediction_window_hrs, x1, y1, x2, y2, cdf_cut_points)</code>","text":"<p>Create predictions for emergency demand for a single prediction moment.</p>"},{"location":"api/#patientflow.predict.emergency_demand.create_predictions--parameters","title":"Parameters","text":"<p>models : Tuple[TrainedClassifier, SequencePredictor, WeightedPoissonPredictor]     Tuple containing:     - classifier: TrainedClassifier containing admission predictions     - spec_model: SequencePredictor for specialty predictions     - yet_to_arrive_model: WeightedPoissonPredictor for yet-to-arrive predictions prediction_time : Tuple     Hour and minute of time for model inference prediction_snapshots : pd.DataFrame     DataFrame containing prediction snapshots specialties : List[str]     List of specialty names for predictions (e.g., ['surgical', 'medical']) prediction_window_hrs : float     Prediction window in hours x1 : float     X-coordinate of first point for probability curve y1 : float     Y-coordinate of first point for probability curve x2 : float     X-coordinate of second point for probability curve y2 : float     Y-coordinate of second point for probability curve cdf_cut_points : List[float]     List of cumulative distribution function cut points (e.g., [0.9, 0.7]) special_params : Optional[Dict[str, Any]], optional     Special handling parameters for categories, by default None</p>"},{"location":"api/#patientflow.predict.emergency_demand.create_predictions--returns","title":"Returns","text":"<p>Dict[str, Dict[str, List[int]]]     Nested dictionary containing predictions for each specialty:     {         'specialty_name': {             'in_ed': [pred1, pred2, ...],             'yet_to_arrive': [pred1, pred2, ...]         }     }</p>"},{"location":"api/#patientflow.predict.emergency_demand.create_predictions--notes","title":"Notes","text":"<p>The admissions models in the models dictionary must be ModelResults objects that contain either a 'pipeline' or 'calibrated_pipeline' attribute. The pipeline will be used for making predictions, with calibrated_pipeline taking precedence if both exist.</p> Source code in <code>src/patientflow/predict/emergency_demand.py</code> <pre><code>def create_predictions(\n    models: Tuple[TrainedClassifier, SequencePredictor, WeightedPoissonPredictor],\n    prediction_time: Tuple,\n    prediction_snapshots: pd.DataFrame,\n    specialties: List[str],\n    prediction_window_hrs: float,\n    x1: float,\n    y1: float,\n    x2: float,\n    y2: float,\n    cdf_cut_points: List[float],\n) -&gt; Dict[str, Dict[str, List[int]]]:\n    \"\"\"\n    Create predictions for emergency demand for a single prediction moment.\n\n    Parameters\n    ----------\n    models : Tuple[TrainedClassifier, SequencePredictor, WeightedPoissonPredictor]\n        Tuple containing:\n        - classifier: TrainedClassifier containing admission predictions\n        - spec_model: SequencePredictor for specialty predictions\n        - yet_to_arrive_model: WeightedPoissonPredictor for yet-to-arrive predictions\n    prediction_time : Tuple\n        Hour and minute of time for model inference\n    prediction_snapshots : pd.DataFrame\n        DataFrame containing prediction snapshots\n    specialties : List[str]\n        List of specialty names for predictions (e.g., ['surgical', 'medical'])\n    prediction_window_hrs : float\n        Prediction window in hours\n    x1 : float\n        X-coordinate of first point for probability curve\n    y1 : float\n        Y-coordinate of first point for probability curve\n    x2 : float\n        X-coordinate of second point for probability curve\n    y2 : float\n        Y-coordinate of second point for probability curve\n    cdf_cut_points : List[float]\n        List of cumulative distribution function cut points (e.g., [0.9, 0.7])\n    special_params : Optional[Dict[str, Any]], optional\n        Special handling parameters for categories, by default None\n\n    Returns\n    -------\n    Dict[str, Dict[str, List[int]]]\n        Nested dictionary containing predictions for each specialty:\n        {\n            'specialty_name': {\n                'in_ed': [pred1, pred2, ...],\n                'yet_to_arrive': [pred1, pred2, ...]\n            }\n        }\n\n    Notes\n    -----\n    The admissions models in the models dictionary must be ModelResults objects\n    that contain either a 'pipeline' or 'calibrated_pipeline' attribute. The pipeline\n    will be used for making predictions, with calibrated_pipeline taking precedence\n    if both exist.\n    \"\"\"\n    # Validate model types\n    classifier, spec_model, yet_to_arrive_model = models\n\n    if not isinstance(classifier, TrainedClassifier):\n        raise TypeError(\"First model must be of type TrainedClassifier\")\n    if not isinstance(spec_model, SequencePredictor):\n        raise TypeError(\"Second model must be of type SequencePredictor\")\n    if not isinstance(yet_to_arrive_model, WeightedPoissonPredictor):\n        raise TypeError(\"Third model must be of type WeightedPoissonPredictor\")\n\n    # Check that all models have been fit\n    if not hasattr(classifier, \"pipeline\") or classifier.pipeline is None:\n        raise ValueError(\"Classifier model has not been fit\")\n    if not hasattr(spec_model, \"weights\") or spec_model.weights is None:\n        raise ValueError(\"Specialty model has not been fit\")\n    if (\n        not hasattr(yet_to_arrive_model, \"prediction_window\")\n        or yet_to_arrive_model.prediction_window is None\n    ):\n        raise ValueError(\"Yet-to-arrive model has not been fit\")\n\n    # Validate that the correct models have been passed for the requested prediction time and prediction window\n    if not classifier.training_results.prediction_time == prediction_time:\n        raise ValueError(\n            \"Requested prediction time does not match the prediction time of the trained classifier\"\n        )\n    if not yet_to_arrive_model.prediction_window / 60 == prediction_window_hrs:\n        raise ValueError(\n            \"Requested prediction window does not match the prediction window of the trained yet-to-arrive model\"\n        )\n    if not set(yet_to_arrive_model.filters.keys()) == set(specialties):\n        raise ValueError(\n            \"Requested specialties do not match the specialties of the trained yet-to-arrive model\"\n        )\n\n    special_params = spec_model.special_params\n\n    if special_params:\n        special_category_func = special_params[\"special_category_func\"]\n        special_category_dict = special_params[\"special_category_dict\"]\n        special_func_map = special_params[\"special_func_map\"]\n    else:\n        special_category_func = special_category_dict = special_func_map = None\n\n    if special_category_dict is not None and not set(specialties) == set(\n        special_category_dict.keys()\n    ):\n        raise ValueError(\n            \"Requested specialties do not match the specialty dictionary defined in special_params\"\n        )\n\n    predictions: Dict[str, Dict[str, List[int]]] = {\n        specialty: {\"in_ed\": [], \"yet_to_arrive\": []} for specialty in specialties\n    }\n\n    # Use calibrated pipeline if available, otherwise use regular pipeline\n    if (\n        hasattr(classifier, \"calibrated_pipeline\")\n        and classifier.calibrated_pipeline is not None\n    ):\n        pipeline = classifier.calibrated_pipeline\n    else:\n        pipeline = classifier.pipeline\n\n    # Add missing columns expected by the model\n    prediction_snapshots = add_missing_columns(pipeline, prediction_snapshots)\n\n    # Get predictions of admissions for ED patients\n    prob_admission_after_ed = model_input_to_pred_proba(prediction_snapshots, pipeline)\n\n    # Get predictions of admission to specialty\n    prediction_snapshots.loc[:, \"specialty_prob\"] = get_specialty_probs(\n        specialties,\n        spec_model,\n        prediction_snapshots,\n        special_category_func=special_category_func,\n        special_category_dict=special_category_dict,\n    )\n\n    prediction_snapshots.loc[:, \"elapsed_los_hrs\"] = prediction_snapshots[\n        \"elapsed_los\"\n    ].apply(lambda x: x / 3600)\n\n    # Get probability of admission within prediction window\n    prob_admission_in_window = prediction_snapshots.apply(\n        lambda row: calculate_probability(\n            row[\"elapsed_los_hrs\"], prediction_window_hrs, x1, y1, x2, y2\n        ),\n        axis=1,\n    )\n\n    if special_func_map is None:\n        special_func_map = {\"default\": lambda row: True}\n\n    for specialty in specialties:\n        func = special_func_map.get(specialty, special_func_map[\"default\"])\n        non_zero_indices = prediction_snapshots[\n            prediction_snapshots.apply(func, axis=1)\n        ].index\n\n        filtered_prob_admission_after_ed = prob_admission_after_ed.loc[non_zero_indices]\n        prob_admission_to_specialty = prediction_snapshots[\"specialty_prob\"].apply(\n            lambda x: x[specialty]\n        )\n\n        filtered_prob_admission_to_specialty = prob_admission_to_specialty.loc[\n            non_zero_indices\n        ]\n        filtered_prob_admission_in_window = prob_admission_in_window.loc[\n            non_zero_indices\n        ]\n\n        filtered_weights = (\n            filtered_prob_admission_to_specialty * filtered_prob_admission_in_window\n        )\n\n        agg_predicted_in_ed = pred_proba_to_agg_predicted(\n            filtered_prob_admission_after_ed, weights=filtered_weights\n        )\n\n        prediction_context = {specialty: {\"prediction_time\": prediction_time}}\n        agg_predicted_yta = yet_to_arrive_model.predict(\n            prediction_context, x1, y1, x2, y2\n        )\n\n        predictions[specialty][\"in_ed\"] = [\n            index_of_sum(agg_predicted_in_ed[\"agg_proba\"].values.cumsum(), cut_point)\n            for cut_point in cdf_cut_points\n        ]\n        predictions[specialty][\"yet_to_arrive\"] = [\n            index_of_sum(\n                agg_predicted_yta[specialty][\"agg_proba\"].values.cumsum(), cut_point\n            )\n            for cut_point in cdf_cut_points\n        ]\n\n    return predictions\n</code></pre>"},{"location":"api/#patientflow.predict.emergency_demand.get_specialty_probs","title":"<code>get_specialty_probs(specialties, specialty_model, snapshots_df, special_category_func=None, special_category_dict=None)</code>","text":"<p>Calculate specialty probability distributions for patient visits based on their data.</p> <p>This function applies a predictive model to each row of the input DataFrame to compute specialty probability distributions. Optionally, it can classify certain rows as belonging to a special category (like pediatric cases) based on a user-defined function, applying a fixed probability distribution for these cases.</p>"},{"location":"api/#patientflow.predict.emergency_demand.get_specialty_probs--parameters","title":"Parameters","text":"str <p>List of specialty names for which predictions are required.</p> <p>specialty_model : object     Trained model for making specialty predictions. snapshots_df : pandas.DataFrame     DataFrame containing the data on which predictions are to be made. Must include     a 'consultation_sequence' column if no special_category_func is applied. special_category_func : callable, optional     A function that takes a DataFrame row (Series) as input and returns True if the row     belongs to a special category that requires a fixed probability distribution.     If not provided, no special categorization is applied. special_category_dict : dict, optional     A dictionary containing the fixed probability distribution for special category cases.     This dictionary is applied to rows identified by <code>special_category_func</code>. If     <code>special_category_func</code> is provided, this parameter must also be provided.</p>"},{"location":"api/#patientflow.predict.emergency_demand.get_specialty_probs--returns","title":"Returns","text":"<p>pandas.Series     A Series containing dictionaries as values. Each dictionary represents the probability     distribution of specialties for each patient visit.</p>"},{"location":"api/#patientflow.predict.emergency_demand.get_specialty_probs--raises","title":"Raises","text":"<p>ValueError     If <code>special_category_func</code> is provided but <code>special_category_dict</code> is None.</p>"},{"location":"api/#patientflow.predict.emergency_demand.get_specialty_probs--examples","title":"Examples","text":"<p>snapshots_df = pd.DataFrame({ ...     'consultation_sequence': [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], ...     'age': [5, 40, 70] ... }) def pediatric_case(row): ...     return row['age'] &lt; 18 special_dist = {'pediatrics': 0.9, 'general': 0.1} get_specialty_probs('model.pkl', snapshots_df, pediatric_case, special_dist) 0    {'pediatrics': 0.9, 'general': 0.1} 1    {'cardiology': 0.7, 'general': 0.3} 2    {'neurology': 0.8, 'general': 0.2} dtype: object</p> Source code in <code>src/patientflow/predict/emergency_demand.py</code> <pre><code>def get_specialty_probs(\n    specialties,\n    specialty_model,\n    snapshots_df,\n    special_category_func=None,\n    special_category_dict=None,\n):\n    \"\"\"\n    Calculate specialty probability distributions for patient visits based on their data.\n\n    This function applies a predictive model to each row of the input DataFrame to compute\n    specialty probability distributions. Optionally, it can classify certain rows as\n    belonging to a special category (like pediatric cases) based on a user-defined function,\n    applying a fixed probability distribution for these cases.\n\n    Parameters\n    ----------\n\n    specialties : str\n        List of specialty names for which predictions are required.\n    specialty_model : object\n        Trained model for making specialty predictions.\n    snapshots_df : pandas.DataFrame\n        DataFrame containing the data on which predictions are to be made. Must include\n        a 'consultation_sequence' column if no special_category_func is applied.\n    special_category_func : callable, optional\n        A function that takes a DataFrame row (Series) as input and returns True if the row\n        belongs to a special category that requires a fixed probability distribution.\n        If not provided, no special categorization is applied.\n    special_category_dict : dict, optional\n        A dictionary containing the fixed probability distribution for special category cases.\n        This dictionary is applied to rows identified by `special_category_func`. If\n        `special_category_func` is provided, this parameter must also be provided.\n\n    Returns\n    -------\n    pandas.Series\n        A Series containing dictionaries as values. Each dictionary represents the probability\n        distribution of specialties for each patient visit.\n\n    Raises\n    ------\n    ValueError\n        If `special_category_func` is provided but `special_category_dict` is None.\n\n    Examples\n    --------\n    &gt;&gt;&gt; snapshots_df = pd.DataFrame({\n    ...     'consultation_sequence': [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]],\n    ...     'age': [5, 40, 70]\n    ... })\n    &gt;&gt;&gt; def pediatric_case(row):\n    ...     return row['age'] &lt; 18\n    &gt;&gt;&gt; special_dist = {'pediatrics': 0.9, 'general': 0.1}\n    &gt;&gt;&gt; get_specialty_probs('model.pkl', snapshots_df, pediatric_case, special_dist)\n    0    {'pediatrics': 0.9, 'general': 0.1}\n    1    {'cardiology': 0.7, 'general': 0.3}\n    2    {'neurology': 0.8, 'general': 0.2}\n    dtype: object\n    \"\"\"\n\n    # Convert consultation_sequence to tuple if not already a tuple\n    if len(snapshots_df[\"consultation_sequence\"]) &gt; 0 and not isinstance(\n        snapshots_df[\"consultation_sequence\"].iloc[0], tuple\n    ):\n        snapshots_df.loc[:, \"consultation_sequence\"] = snapshots_df[\n            \"consultation_sequence\"\n        ].apply(lambda x: tuple(x) if x else ())\n\n    if special_category_func and not special_category_dict:\n        raise ValueError(\n            \"special_category_dict must be provided if special_category_func is specified.\"\n        )\n\n    # Function to determine the specialty probabilities\n    def determine_specialty(row):\n        if special_category_func and special_category_func(row):\n            return special_category_dict\n        else:\n            return specialty_model.predict(row[\"consultation_sequence\"])\n\n    # Apply the determine_specialty function to each row\n    specialty_prob_series = snapshots_df.apply(determine_specialty, axis=1)\n\n    # Find all unique keys used in any dictionary within the series\n    all_keys = set().union(\n        *(d.keys() for d in specialty_prob_series if isinstance(d, dict))\n    )\n\n    # Combine all_keys with the specialties requested\n    all_keys = set(all_keys).union(set(specialties))\n\n    # Ensure each dictionary contains all keys found, with default values of 0 for missing keys\n    specialty_prob_series = specialty_prob_series.apply(\n        lambda d: (\n            {key: d.get(key, 0) for key in all_keys} if isinstance(d, dict) else d\n        )\n    )\n\n    return specialty_prob_series\n</code></pre>"},{"location":"api/#patientflow.predict.emergency_demand.index_of_sum","title":"<code>index_of_sum(sequence, max_sum)</code>","text":"<p>Returns the index where the cumulative sum of a sequence of probabilities exceeds max_sum.</p> Source code in <code>src/patientflow/predict/emergency_demand.py</code> <pre><code>def index_of_sum(sequence: List[float], max_sum: float) -&gt; int:\n    \"\"\"Returns the index where the cumulative sum of a sequence of probabilities exceeds max_sum.\"\"\"\n    cumulative_sum = 0.0\n    for i, value in enumerate(sequence):\n        cumulative_sum += value\n        if cumulative_sum &gt;= 1 - max_sum:\n            return i\n    return len(sequence) - 1  # Return the last index if the sum doesn't exceed max_sum\n</code></pre>"},{"location":"api/#patientflow.predictors","title":"<code>predictors</code>","text":""},{"location":"api/#patientflow.predictors.sequence_predictor","title":"<code>sequence_predictor</code>","text":"<p>This module implements a <code>SequencePredictor</code> class that models and predicts the probability distribution of sequences in categorical data. The class builds a model based on training data, where input sequences are mapped to specific outcome categories. It provides methods to fit the model, compute sequence-based probabilities, and make predictions on an unseen datatset of input sequences.</p>"},{"location":"api/#patientflow.predictors.sequence_predictor--classes","title":"Classes","text":"<p>SequencePredictor : sklearn.base.BaseEstimator, sklearn.base.TransformerMixin     A model that predicts the probability of ending in different outcome categories based on input sequences.</p>"},{"location":"api/#patientflow.predictors.sequence_predictor.SequencePredictor","title":"<code>SequencePredictor</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>A class to model sequence-based predictions for categorical data using input and grouping sequences. This class implements both the <code>fit</code> and <code>predict</code> methods from the parent sklearn classes.</p>"},{"location":"api/#patientflow.predictors.sequence_predictor.SequencePredictor--parameters","title":"Parameters","text":"<p>input_var : str     Name of the column representing the input sequence in the DataFrame. grouping_var : str     Name of the column representing the grouping sequence in the DataFrame. outcome_var : str     Name of the column representing the outcome category in the DataFrame. apply_special_category_filtering : bool, default=True     Whether to filter out special categories of patients before fitting the model. admit_col : str, default='is_admitted'     Name of the column indicating whether a patient was admitted.</p>"},{"location":"api/#patientflow.predictors.sequence_predictor.SequencePredictor--attributes","title":"Attributes","text":"<p>weights : dict     A dictionary storing the probabilities of different input sequences leading to specific outcome categories. input_to_grouping_probs : pd.DataFrame     A DataFrame that stores the computed probabilities of input sequences being associated with different grouping sequences. special_params : dict, optional     The special category parameters used for filtering, only populated if apply_special_category_filtering=True. metrics : dict     A dictionary to store metrics related to the training process.</p> Source code in <code>src/patientflow/predictors/sequence_predictor.py</code> <pre><code>class SequencePredictor(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A class to model sequence-based predictions for categorical data using input and grouping sequences.\n    This class implements both the `fit` and `predict` methods from the parent sklearn classes.\n\n    Parameters\n    ----------\n    input_var : str\n        Name of the column representing the input sequence in the DataFrame.\n    grouping_var : str\n        Name of the column representing the grouping sequence in the DataFrame.\n    outcome_var : str\n        Name of the column representing the outcome category in the DataFrame.\n    apply_special_category_filtering : bool, default=True\n        Whether to filter out special categories of patients before fitting the model.\n    admit_col : str, default='is_admitted'\n        Name of the column indicating whether a patient was admitted.\n\n    Attributes\n    ----------\n    weights : dict\n        A dictionary storing the probabilities of different input sequences leading to specific outcome categories.\n    input_to_grouping_probs : pd.DataFrame\n        A DataFrame that stores the computed probabilities of input sequences being associated with different grouping sequences.\n    special_params : dict, optional\n        The special category parameters used for filtering, only populated if apply_special_category_filtering=True.\n    metrics : dict\n        A dictionary to store metrics related to the training process.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_var,\n        grouping_var,\n        outcome_var,\n        apply_special_category_filtering=True,\n        admit_col=\"is_admitted\",\n    ):\n        self.input_var = input_var\n        self.grouping_var = grouping_var\n        self.outcome_var = outcome_var\n        self.apply_special_category_filtering = apply_special_category_filtering\n        self.admit_col = admit_col\n        self.weights = None\n        self.special_params = None\n        self.metrics = {}\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the estimator.\"\"\"\n        class_name = self.__class__.__name__\n        return (\n            f\"{class_name}(\\n\"\n            f\"    input_var='{self.input_var}',\\n\"\n            f\"    grouping_var='{self.grouping_var}',\\n\"\n            f\"    outcome_var='{self.outcome_var}',\\n\"\n            f\"    apply_special_category_filtering={self.apply_special_category_filtering},\\n\"\n            f\"    admit_col='{self.admit_col}'\\n\"\n            f\")\"\n        )\n\n    def _preprocess_data(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Preprocesses the input data before fitting the model.\n\n        Steps include:\n        1. Selecting only admitted patients with a non-null specialty\n        2. Optionally filtering out special categories\n        3. Converting sequence columns to tuple format if they aren't already\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            DataFrame containing patient data.\n\n        Returns\n        -------\n        pd.DataFrame\n            Preprocessed DataFrame ready for model fitting.\n        \"\"\"\n        # Make a copy to avoid modifying the original\n        df = X.copy()\n\n        # Step 1: Select only admitted patients with a non-null specialty\n        if self.admit_col in df.columns:\n            df = df[df[self.admit_col] &amp; ~df[self.outcome_var].isnull()]\n\n        # Step 2: Optionally apply filtering for special categories\n        if self.apply_special_category_filtering:\n            # Get configuration for categorizing patients based on columns\n            self.special_params = create_special_category_objects(df.columns)\n\n            # Extract function that identifies non-special category patients\n            opposite_special_category_func = self.special_params[\"special_func_map\"][\n                \"default\"\n            ]\n\n            # Determine which category is the special category\n            special_category_key = next(\n                key\n                for key, value in self.special_params[\"special_category_dict\"].items()\n                if value == 1.0\n            )\n\n            # Filter out special category patients\n            df = df[\n                df.apply(opposite_special_category_func, axis=1)\n                &amp; (df[self.outcome_var] != special_category_key)\n            ]\n\n        # Step 3: Convert sequence columns to tuple format if not already tuples\n        # Process input variable\n        if self.input_var in df.columns:\n            df[self.input_var] = df[self.input_var].apply(\n                lambda x: tuple(x)\n                if (x is not None and not isinstance(x, tuple))\n                else ()\n                if x is None\n                else x\n            )\n\n        # Process grouping variable\n        if self.grouping_var in df.columns:\n            df[self.grouping_var] = df[self.grouping_var].apply(\n                lambda x: tuple(x)\n                if (x is not None and not isinstance(x, tuple))\n                else ()\n                if x is None\n                else x\n            )\n\n        return df\n\n    def fit(self, X: pd.DataFrame) -&gt; \"SequencePredictor\":\n        \"\"\"\n        Fits the predictor based on training data by computing the proportion of each input variable sequence\n        ending in specific outcome variable categories.\n\n        Automatically preprocesses the data before fitting.\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            A pandas DataFrame containing at least the columns specified by `input_var`, `grouping_var`, and `outcome_var`.\n\n        Returns\n        -------\n        self : SequencePredictor\n            The fitted SequencePredictor model with calculated probabilities for each sequence.\n        \"\"\"\n        # Store metrics about the training data\n        self.metrics[\"train_dttm\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n        self.metrics[\"train_set_no\"] = len(X)\n        if not X.empty:\n            self.metrics[\"start_date\"] = X[\"snapshot_date\"].min()\n            self.metrics[\"end_date\"] = X[\"snapshot_date\"].max()\n\n        # Preprocess the data\n        X = self._preprocess_data(X)\n\n        # derive the names of the observed outcome variables from the data\n        prop_keys = X[self.outcome_var].unique()\n\n        # For each sequence count the number of observed categories\n        X_grouped = (\n            X.groupby(self.grouping_var)[self.outcome_var]\n            .value_counts()\n            .unstack(fill_value=0)\n        )\n\n        # Handle null sequences by assigning them to a specific key\n        null_counts = (\n            X[X[self.grouping_var].isnull()][self.outcome_var]\n            .value_counts()\n            .to_frame()\n            .T\n        )\n        null_counts.index = [tuple()]\n\n        # Concatenate null sequence handling\n        X_grouped = pd.concat([X_grouped, null_counts])\n\n        # Calculate the total number of times each grouping sequence occurred\n        row_totals = X_grouped.sum(axis=1)\n\n        # Calculate for each grouping sequence, the proportion of ending with each observed specialty\n        proportions = X_grouped.div(row_totals, axis=0)\n\n        # Calculate the probability of each grouping sequence occurring in the original data\n        proportions[\"probability_of_grouping_sequence\"] = row_totals / row_totals.sum()\n\n        # Reweight probabilities of ending with each observed specialty\n        # by the likelihood of each grouping sequence occurring\n        for col in proportions.columns[\n            :-1\n        ]:  # Avoid the last column which is the 'probability_of_grouping_sequence'\n            proportions[col] *= proportions[\"probability_of_grouping_sequence\"]\n\n        # Convert final sequence to a string in order to conduct string searches on it\n        proportions[\"grouping_sequence_to_string\"] = (\n            proportions.reset_index()[\"index\"]\n            .apply(lambda x: \"-\".join(map(str, x)))\n            .values\n        )\n        # Row-wise function to return, for each input sequence,\n        # the proportion that end up in each final sequence and thereby\n        # the probability of it ending in any observed category\n        proportions[\"prob_input_var_ends_in_observed_specialty\"] = proportions[\n            \"grouping_sequence_to_string\"\n        ].apply(lambda x: self._string_match_input_var(x, proportions, prop_keys))\n\n        # Convert the prob_input_var_ends_in_observed_specialty column to a dictionary\n        result_dict = proportions[\"prob_input_var_ends_in_observed_specialty\"].to_dict()\n\n        # Clean the key to remove excess strint quotes\n        def clean_tuple_key(key):\n            if isinstance(key, tuple):\n                return tuple(\n                    ast.literal_eval(item)\n                    if item.startswith(\"'\") and item.endswith(\"'\")\n                    else item\n                    for item in key\n                )\n            return key\n\n        cleaned_dict = {clean_tuple_key(k): v for k, v in result_dict.items()}\n\n        # save prob_input_var_ends_in_observed_specialty as weights within the model\n        self.weights = cleaned_dict\n\n        # save the input to grouping probabilities for use as a reference\n        self.input_to_grouping_probs = self._probability_of_input_to_grouping_sequence(\n            X\n        )\n\n        return self\n\n    def _string_match_input_var(self, input_var_string, proportions, prop_keys):\n        \"\"\"\n        Matches a given input sequence string with grouped sequences (expressed as strings) in the dataset and aggregates\n        their probabilities for each outcome category. This function filters the data to\n        match only those rows where the *beginning* of the grouped sequence string\n        matches the given input sequence string, allowing for partial matches.\n        For instance, the sequence 'medical' will match 'medical, elderly' and 'medical, surgical'\n        as well as 'medical' on its own. It computes the total probabilities of any input sequence ending\n        in each outcome category, and normalizes these totals if possible.\n\n        Parameters\n        ----------\n        input_var_string : str\n            The sequence of inputs represented as a string, used to match against sequences in the proportions DataFrame.\n        proportions : pd.DataFrame\n            DataFrame containing proportions data with an additional column 'grouping_sequence_to_string'\n            which includes string representations of sequences.\n        prop_keys : np.array\n            Array of unique outcomes to consider in calculations.\n\n        Returns\n        -------\n        dict\n            A dictionary where keys are outcome names and values are the aggregated and normalized probabilities\n            of an input sequence ending in those outcomes.\n\n        \"\"\"\n        # Filter rows where the grouped sequence string starts with the input sequence string\n        props = proportions[\n            proportions[\"grouping_sequence_to_string\"].str.match(\"^\" + input_var_string)\n        ][prop_keys].sum()\n\n        # Sum of all probabilities to normalize them\n        props_total = props.sum()\n\n        # Handle cases where the total probability is zero to avoid division by zero\n        if props_total &gt; 0:\n            normalized_props = props / props_total\n        else:\n            normalized_props = (\n                props * 0\n            )  # Returns zero probabilities if no matches found\n\n        return dict(zip(prop_keys, normalized_props))\n\n    def _probability_of_input_to_grouping_sequence(self, X):\n        \"\"\"\n        Computes the probabilities of different input sequences leading to specific grouping sequences.\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            A pandas DataFrame containing at least the columns specified by `input_var` and `grouping_var`.\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame containing the probabilities of input sequences leading to grouping sequences.\n        \"\"\"\n        # For each input sequence count the number of grouping sequences\n        X_grouped = (\n            X.groupby(self.input_var)[self.grouping_var]\n            .value_counts()\n            .unstack(fill_value=0)\n        )\n\n        # # Calculate the total number of times each input sequence occurred\n        row_totals = X_grouped.sum(axis=1)\n\n        # # Calculate for each grouping sequence, the proportion of ending with each grouping sequence\n        proportions = X_grouped.div(row_totals, axis=0)\n\n        # # Calculate the probability of each input sequence occurring in the original data\n        proportions[\"probability_of_grouping_sequence\"] = row_totals / row_totals.sum()\n\n        return proportions\n\n    def predict(self, input_sequence: tuple[str, ...]) -&gt; Dict[str, float]:\n        \"\"\"\n        Predicts the probabilities of ending in various outcome categories for a given input sequence.\n\n        Parameters\n        ----------\n        input_sequence : tuple[str, ...]\n            A tuple containing the categories that have been observed for an entity in the order they\n            have been encountered. An empty tuple represents an entity with no observed categories.\n\n        Returns\n        -------\n        dict\n            A dictionary of categories and the probabilities that the input sequence will end in them.\n        \"\"\"\n        # Check for no tuple\n        if input_sequence is None or pd.isna(input_sequence):\n            return self.weights.get(tuple(), {})\n\n        # Return a direct lookup of probabilities if possible.\n        if input_sequence in self.weights:\n            return self.weights[input_sequence]\n\n        # Otherwise, if the sequence has multiple elements, work back looking for a match\n        while len(input_sequence) &gt; 1:\n            input_sequence_list = list(input_sequence)\n            input_sequence = tuple(input_sequence_list[:-1])  # remove last element\n\n            if input_sequence in self.weights:\n                return self.weights[input_sequence]\n\n        # If no relevant data is found:\n        return self.weights.get(tuple(), {})\n</code></pre>"},{"location":"api/#patientflow.predictors.sequence_predictor.SequencePredictor.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the estimator.</p> Source code in <code>src/patientflow/predictors/sequence_predictor.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a string representation of the estimator.\"\"\"\n    class_name = self.__class__.__name__\n    return (\n        f\"{class_name}(\\n\"\n        f\"    input_var='{self.input_var}',\\n\"\n        f\"    grouping_var='{self.grouping_var}',\\n\"\n        f\"    outcome_var='{self.outcome_var}',\\n\"\n        f\"    apply_special_category_filtering={self.apply_special_category_filtering},\\n\"\n        f\"    admit_col='{self.admit_col}'\\n\"\n        f\")\"\n    )\n</code></pre>"},{"location":"api/#patientflow.predictors.sequence_predictor.SequencePredictor.fit","title":"<code>fit(X)</code>","text":"<p>Fits the predictor based on training data by computing the proportion of each input variable sequence ending in specific outcome variable categories.</p> <p>Automatically preprocesses the data before fitting.</p>"},{"location":"api/#patientflow.predictors.sequence_predictor.SequencePredictor.fit--parameters","title":"Parameters","text":"<p>X : pd.DataFrame     A pandas DataFrame containing at least the columns specified by <code>input_var</code>, <code>grouping_var</code>, and <code>outcome_var</code>.</p>"},{"location":"api/#patientflow.predictors.sequence_predictor.SequencePredictor.fit--returns","title":"Returns","text":"<p>self : SequencePredictor     The fitted SequencePredictor model with calculated probabilities for each sequence.</p> Source code in <code>src/patientflow/predictors/sequence_predictor.py</code> <pre><code>def fit(self, X: pd.DataFrame) -&gt; \"SequencePredictor\":\n    \"\"\"\n    Fits the predictor based on training data by computing the proportion of each input variable sequence\n    ending in specific outcome variable categories.\n\n    Automatically preprocesses the data before fitting.\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n        A pandas DataFrame containing at least the columns specified by `input_var`, `grouping_var`, and `outcome_var`.\n\n    Returns\n    -------\n    self : SequencePredictor\n        The fitted SequencePredictor model with calculated probabilities for each sequence.\n    \"\"\"\n    # Store metrics about the training data\n    self.metrics[\"train_dttm\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    self.metrics[\"train_set_no\"] = len(X)\n    if not X.empty:\n        self.metrics[\"start_date\"] = X[\"snapshot_date\"].min()\n        self.metrics[\"end_date\"] = X[\"snapshot_date\"].max()\n\n    # Preprocess the data\n    X = self._preprocess_data(X)\n\n    # derive the names of the observed outcome variables from the data\n    prop_keys = X[self.outcome_var].unique()\n\n    # For each sequence count the number of observed categories\n    X_grouped = (\n        X.groupby(self.grouping_var)[self.outcome_var]\n        .value_counts()\n        .unstack(fill_value=0)\n    )\n\n    # Handle null sequences by assigning them to a specific key\n    null_counts = (\n        X[X[self.grouping_var].isnull()][self.outcome_var]\n        .value_counts()\n        .to_frame()\n        .T\n    )\n    null_counts.index = [tuple()]\n\n    # Concatenate null sequence handling\n    X_grouped = pd.concat([X_grouped, null_counts])\n\n    # Calculate the total number of times each grouping sequence occurred\n    row_totals = X_grouped.sum(axis=1)\n\n    # Calculate for each grouping sequence, the proportion of ending with each observed specialty\n    proportions = X_grouped.div(row_totals, axis=0)\n\n    # Calculate the probability of each grouping sequence occurring in the original data\n    proportions[\"probability_of_grouping_sequence\"] = row_totals / row_totals.sum()\n\n    # Reweight probabilities of ending with each observed specialty\n    # by the likelihood of each grouping sequence occurring\n    for col in proportions.columns[\n        :-1\n    ]:  # Avoid the last column which is the 'probability_of_grouping_sequence'\n        proportions[col] *= proportions[\"probability_of_grouping_sequence\"]\n\n    # Convert final sequence to a string in order to conduct string searches on it\n    proportions[\"grouping_sequence_to_string\"] = (\n        proportions.reset_index()[\"index\"]\n        .apply(lambda x: \"-\".join(map(str, x)))\n        .values\n    )\n    # Row-wise function to return, for each input sequence,\n    # the proportion that end up in each final sequence and thereby\n    # the probability of it ending in any observed category\n    proportions[\"prob_input_var_ends_in_observed_specialty\"] = proportions[\n        \"grouping_sequence_to_string\"\n    ].apply(lambda x: self._string_match_input_var(x, proportions, prop_keys))\n\n    # Convert the prob_input_var_ends_in_observed_specialty column to a dictionary\n    result_dict = proportions[\"prob_input_var_ends_in_observed_specialty\"].to_dict()\n\n    # Clean the key to remove excess strint quotes\n    def clean_tuple_key(key):\n        if isinstance(key, tuple):\n            return tuple(\n                ast.literal_eval(item)\n                if item.startswith(\"'\") and item.endswith(\"'\")\n                else item\n                for item in key\n            )\n        return key\n\n    cleaned_dict = {clean_tuple_key(k): v for k, v in result_dict.items()}\n\n    # save prob_input_var_ends_in_observed_specialty as weights within the model\n    self.weights = cleaned_dict\n\n    # save the input to grouping probabilities for use as a reference\n    self.input_to_grouping_probs = self._probability_of_input_to_grouping_sequence(\n        X\n    )\n\n    return self\n</code></pre>"},{"location":"api/#patientflow.predictors.sequence_predictor.SequencePredictor.predict","title":"<code>predict(input_sequence)</code>","text":"<p>Predicts the probabilities of ending in various outcome categories for a given input sequence.</p>"},{"location":"api/#patientflow.predictors.sequence_predictor.SequencePredictor.predict--parameters","title":"Parameters","text":"<p>input_sequence : tuple[str, ...]     A tuple containing the categories that have been observed for an entity in the order they     have been encountered. An empty tuple represents an entity with no observed categories.</p>"},{"location":"api/#patientflow.predictors.sequence_predictor.SequencePredictor.predict--returns","title":"Returns","text":"<p>dict     A dictionary of categories and the probabilities that the input sequence will end in them.</p> Source code in <code>src/patientflow/predictors/sequence_predictor.py</code> <pre><code>def predict(self, input_sequence: tuple[str, ...]) -&gt; Dict[str, float]:\n    \"\"\"\n    Predicts the probabilities of ending in various outcome categories for a given input sequence.\n\n    Parameters\n    ----------\n    input_sequence : tuple[str, ...]\n        A tuple containing the categories that have been observed for an entity in the order they\n        have been encountered. An empty tuple represents an entity with no observed categories.\n\n    Returns\n    -------\n    dict\n        A dictionary of categories and the probabilities that the input sequence will end in them.\n    \"\"\"\n    # Check for no tuple\n    if input_sequence is None or pd.isna(input_sequence):\n        return self.weights.get(tuple(), {})\n\n    # Return a direct lookup of probabilities if possible.\n    if input_sequence in self.weights:\n        return self.weights[input_sequence]\n\n    # Otherwise, if the sequence has multiple elements, work back looking for a match\n    while len(input_sequence) &gt; 1:\n        input_sequence_list = list(input_sequence)\n        input_sequence = tuple(input_sequence_list[:-1])  # remove last element\n\n        if input_sequence in self.weights:\n            return self.weights[input_sequence]\n\n    # If no relevant data is found:\n    return self.weights.get(tuple(), {})\n</code></pre>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor","title":"<code>weighted_poisson_predictor</code>","text":"<p>Weighted Poisson Predictor</p> <p>This module implements a custom predictor to estimate the number of hospital admissions within a specified prediction window using historical admission data. It applies Poisson and binomial distributions to forecast future admissions, excluding already arrived patients. The predictor accommodates different data filters for tailored predictions across various hospital settings.</p> Dependencies <ul> <li>pandas: For data manipulation and analysis, essential for handling the dataset used in predictions.</li> <li>datetime: For manipulating date and time objects, crucial for time-based predictions.</li> <li>sklearn: Utilizes BaseEstimator and TransformerMixin from scikit-learn for creating custom, interoperable predictors.</li> <li>Custom modules:<ul> <li>calculate.time_varying_arrival_rates: Computes time-varying arrival rates, for each specified interval within the prediction window.</li> <li>predict.admission_in_prediction_window: Calculates the probability of admission within a specified prediction window.</li> </ul> </li> </ul> <p>Classes:</p> Name Description <code>WeightedPoissonPredictor</code> <p>Predicts the number of admissions within a given prediction window based on historical data and Poisson-binomial distribution.</p> <code>Methods within WeightedPoissonPredictor</code> <ul> <li>init(self, filters=None): Initializes the predictor with optional data filters.</li> <li>filter_dataframe(self, df, filters): Applies filters to the dataset for targeted predictions.</li> <li>fit(self, train_df, prediction_window, yta_time_interval, prediction_times, json_file_path, reference_year, y=None): Trains the predictor using historical data and various parameters.</li> <li>predict(self, prediction_context): Predicts the number of admissions using the trained model.</li> </ul>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor.WeightedPoissonPredictor","title":"<code>WeightedPoissonPredictor</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>A class to predict an aspirational number of admissions within a specified prediction window. This prediction does not include patients who have already arrived and is based on historical data. The prediction uses a combination of Poisson and binomial distributions.</p> <p>Attributes:</p> Name Type Description <code>filters</code> <code>dict</code> <p>Optional filters for data categorization</p> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging</p> <code>metrics</code> <code>dict</code> <p>Stores metadata about the model and training data</p> <p>Methods     init(self, filters=None): Initializes the predictor with optional filters for data categorization.     filter_dataframe(self, df, filters): Filters the dataset based on specified criteria for targeted predictions.     fit(self, train_df, prediction_window, yta_time_interval, prediction_times, y=None): Trains the model using historical data and prediction parameters.     predict(self, prediction_context): Predicts the number of admissions for a given context after the model is trained.     get_weights(self): Retrieves the model parameters computed during fitting.</p> Source code in <code>src/patientflow/predictors/weighted_poisson_predictor.py</code> <pre><code>class WeightedPoissonPredictor(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A class to predict an aspirational number of admissions within a specified prediction window.\n    This prediction does not include patients who have already arrived and is based on historical data.\n    The prediction uses a combination of Poisson and binomial distributions.\n\n    Attributes:\n        filters (dict): Optional filters for data categorization\n        verbose (bool): Whether to enable verbose logging\n        metrics (dict): Stores metadata about the model and training data\n\n    Methods\n        __init__(self, filters=None): Initializes the predictor with optional filters for data categorization.\n        filter_dataframe(self, df, filters): Filters the dataset based on specified criteria for targeted predictions.\n        fit(self, train_df, prediction_window, yta_time_interval, prediction_times, y=None): Trains the model using historical data and prediction parameters.\n        predict(self, prediction_context): Predicts the number of admissions for a given context after the model is trained.\n        get_weights(self): Retrieves the model parameters computed during fitting.\n\n    \"\"\"\n\n    def __init__(self, filters=None, verbose=False):\n        \"\"\"\n        Initialize the WeightedPoissonPredictor with optional filters.\n\n        Args:\n            filters (dict, optional): A dictionary defining filters for different categories or specialties.\n                                    If None or empty, no filtering will be applied.\n            verbose (bool, optional): If True, enable info-level logging. Defaults to False.\n        \"\"\"\n        self.filters = filters if filters else {}\n        self.verbose = verbose\n        self.metrics = {}  # Add metrics dictionary to store metadata\n\n        if verbose:\n            # Configure logging for Jupyter notebook compatibility\n            import logging\n            import sys\n\n            # Create logger\n            self.logger = logging.getLogger(f\"{__name__}.WeightedPoissonPredictor\")\n\n            # Only set up handlers if they don't exist\n            if not self.logger.handlers:\n                self.logger.setLevel(logging.INFO if verbose else logging.WARNING)\n\n                # Create handler that writes to sys.stdout\n                handler = logging.StreamHandler(sys.stdout)\n                handler.setLevel(logging.INFO if verbose else logging.WARNING)\n\n                # Create a formatting configuration\n                formatter = logging.Formatter(\"%(message)s\")\n                handler.setFormatter(formatter)\n\n                # Add the handler to the logger\n                self.logger.addHandler(handler)\n\n                # Prevent propagation to root logger\n                self.logger.propagate = False\n\n        # Apply filters\n        self.filters = filters if filters else {}\n\n    def filter_dataframe(self, df: pd.DataFrame, filters: Dict) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply a set of filters to a dataframe.\n\n        Args:\n            df (pandas.DataFrame): The DataFrame to filter.\n            filters (dict): A dictionary where keys are column names and values are the criteria or function to filter by.\n\n        Returns:\n            pandas.DataFrame: A filtered DataFrame.\n\n        \"\"\"\n        filtered_df = df\n        for column, criteria in filters.items():\n            if callable(criteria):  # If the criteria is a function, apply it directly\n                filtered_df = filtered_df[filtered_df[column].apply(criteria)]\n            else:  # Otherwise, assume the criteria is a value or list of values for equality check\n                filtered_df = filtered_df[filtered_df[column] == criteria]\n        return filtered_df\n\n    def _calculate_parameters(\n        self, df, prediction_window, yta_time_interval, prediction_times, num_days\n    ):\n        \"\"\"\n        Calculate parameters required for the model.\n\n        Args:\n            df (pandas.DataFrame): The data frame to process.\n            prediction_window (int): The total prediction window for prediction.\n            yta_time_interval (int): The interval for splitting the prediction window.\n            prediction_times (list): Times of day at which predictions are made.\n            num_days (int): Number of days over which to calculate time-varying arrival rates\n\n        Returns:\n            dict: Calculated lambda_t parameters organized by time of day.\n\n        \"\"\"\n        Ntimes = int(prediction_window / yta_time_interval)\n        arrival_rates_dict = time_varying_arrival_rates(\n            df, yta_time_interval, num_days, verbose=self.verbose\n        )\n        prediction_time_dict = {}\n\n        for prediction_time_ in prediction_times:\n            prediction_time_hr, prediction_time_min = (\n                (prediction_time_, 0)\n                if isinstance(prediction_time_, int)\n                else prediction_time_\n            )\n            lambda_t = [\n                arrival_rates_dict[\n                    (\n                        datetime(1970, 1, 1, prediction_time_hr, prediction_time_min)\n                        + i * timedelta(minutes=yta_time_interval)\n                    ).time()\n                ]\n                for i in range(Ntimes)\n            ]\n            prediction_time_dict[(prediction_time_hr, prediction_time_min)] = {\n                \"lambda_t\": lambda_t\n            }\n\n        return prediction_time_dict\n\n    def fit(\n        self,\n        train_df: pd.DataFrame,\n        prediction_window: int,\n        yta_time_interval: int,\n        prediction_times: List[float],\n        num_days: int,\n        epsilon: float = 10**-7,\n        y: Optional[None] = None,\n    ) -&gt; \"WeightedPoissonPredictor\":\n        \"\"\"\n        Fits the model to the training data, computing necessary parameters for future predictions.\n\n        Args:\n            train_df (pandas.DataFrame):\n                The training dataset with historical admission data.\n            prediction_window (int):\n                The prediction prediction window in minutes.\n            yta_time_interval (int):\n                The interval in minutes for splitting the prediction window.\n            prediction_times (list):\n                Times of day at which predictions are made, in hours.\n            num_days (int):\n                 The number of days that the train_df spans\n            epsilon (float, optional):\n                A small value representing acceptable error rate to enable calculation of the maximum value of the random variable representing number of beds.\n            y (None, optional):\n                Ignored, present for compatibility with scikit-learn's fit method.\n\n        Returns:\n            WeightedPoissonPredictor: The instance itself, fitted with the training data.\n\n        \"\"\"\n        # Add error checking at the start of fit\n        if int(prediction_window / yta_time_interval) == 0:\n            raise ValueError(\n                f\"prediction_window ({prediction_window}) divided by yta_time_interval ({yta_time_interval}) must be greater than 1 to generate meaningful predictions\"\n            )\n\n        # Store prediction_window, yta_time_interval, and any other parameters as instance variables\n        self.prediction_window = prediction_window\n        self.yta_time_interval = yta_time_interval\n        self.epsilon = epsilon\n        self.prediction_times = [\n            tuple(x)\n            if isinstance(x, (list, np.ndarray))\n            else (x, 0)\n            if isinstance(x, (int, float))\n            else x\n            for x in prediction_times\n        ]\n\n        # Initialize yet_to_arrive_dict\n        self.weights = {}\n\n        # If there are filters specified, calculate and store the parameters directly with the respective spec keys\n        if self.filters:\n            for spec, filters in self.filters.items():\n                self.weights[spec] = self._calculate_parameters(\n                    self.filter_dataframe(train_df, filters),\n                    prediction_window,\n                    yta_time_interval,\n                    prediction_times,\n                    num_days,\n                )\n        else:\n            # If there are no filters, store the parameters with a generic key, like 'default' or 'unfiltered'\n            self.weights[\"default\"] = self._calculate_parameters(\n                train_df,\n                prediction_window,\n                yta_time_interval,\n                prediction_times,\n                num_days,\n            )\n\n        if self.verbose:\n            self.logger.info(\n                f\"Poisson Binomial Predictor trained for these times: {prediction_times}\"\n            )\n            self.logger.info(\n                f\"using prediction window of {prediction_window} minutes after the time of prediction\"\n            )\n            self.logger.info(\n                f\"and time interval of {yta_time_interval} minutes within the prediction window.\"\n            )\n            self.logger.info(f\"The error value for prediction will be {epsilon}\")\n            self.logger.info(\n                \"To see the weights saved by this model, used the get_weights() method\"\n            )\n\n        # Store metrics about the training data\n        self.metrics[\"train_dttm\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n        self.metrics[\"train_set_no\"] = len(train_df)\n        self.metrics[\"start_date\"] = train_df.index.min().date()\n        self.metrics[\"end_date\"] = train_df.index.max().date()\n        self.metrics[\"num_days\"] = num_days\n\n        return self\n\n    def get_weights(self):\n        \"\"\"\n        Returns the weights computed by the fit method.\n\n        Returns\n            dict: The weights.\n\n        \"\"\"\n        return self.weights\n\n    def predict(\n        self, prediction_context: Dict, x1: float, y1: float, x2: float, y2: float\n    ) -&gt; Dict:\n        \"\"\"\n        Predicts the number of admissions for the given context based on the fitted model.\n\n        Args:\n            prediction_context (dict): A dictionary defining the context for which predictions are to be made.\n                                       It should specify either a general context or one based on the applied filters.\n            x1 : float\n                The x-coordinate of the first transition point on the aspirational curve, where the growth phase ends and the decay phase begins.\n            y1 : float\n                The y-coordinate of the first transition point (x1), representing the target proportion of patients admitted by time x1.\n            x2 : float\n                The x-coordinate of the second transition point on the curve, beyond which all but a few patients are expected to be admitted.\n            y2 : float\n                The y-coordinate of the second transition point (x2), representing the target proportion of patients admitted by time x2.\n\n        Returns:\n            dict: A dictionary with predictions for each specified context.\n\n        \"\"\"\n        predictions = {}\n\n        NTimes = int(self.prediction_window / self.yta_time_interval)\n        # Calculate theta, probability of admission in prediction window\n\n        # for each time interval, calculate time remaining before end of window\n        time_remaining_before_end_of_window = self.prediction_window / 60 - np.arange(\n            0, self.prediction_window / 60, self.yta_time_interval / 60\n        )\n\n        # probability of admission in that time\n        theta = get_y_from_aspirational_curve(\n            time_remaining_before_end_of_window, x1, y1, x2, y2\n        )\n\n        for filter_key, filter_values in prediction_context.items():\n            try:\n                if filter_key not in self.weights:\n                    raise ValueError(\n                        f\"Filter key '{filter_key}' is not recognized in the model weights.\"\n                    )\n\n                prediction_time = filter_values.get(\"prediction_time\")\n                if prediction_time is None:\n                    raise ValueError(\n                        f\"No 'prediction_time' provided for filter '{filter_key}'.\"\n                    )\n\n                if prediction_time not in self.prediction_times:\n                    prediction_time = find_nearest_previous_prediction_time(\n                        prediction_time, self.prediction_times\n                    )\n\n                lambda_t = self.weights[filter_key][prediction_time].get(\"lambda_t\")\n                if lambda_t is None:\n                    raise ValueError(\n                        f\"No 'lambda_t' found for the time of day '{prediction_time}' under filter '{filter_key}'.\"\n                    )\n\n                predictions[filter_key] = poisson_binom_generating_function(\n                    NTimes, lambda_t, theta, self.epsilon\n                )\n\n            except KeyError as e:\n                raise KeyError(f\"Key error occurred: {e!s}\")\n\n        return predictions\n</code></pre>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor.WeightedPoissonPredictor.__init__","title":"<code>__init__(filters=None, verbose=False)</code>","text":"<p>Initialize the WeightedPoissonPredictor with optional filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>A dictionary defining filters for different categories or specialties.                     If None or empty, no filtering will be applied.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, enable info-level logging. Defaults to False.</p> <code>False</code> Source code in <code>src/patientflow/predictors/weighted_poisson_predictor.py</code> <pre><code>def __init__(self, filters=None, verbose=False):\n    \"\"\"\n    Initialize the WeightedPoissonPredictor with optional filters.\n\n    Args:\n        filters (dict, optional): A dictionary defining filters for different categories or specialties.\n                                If None or empty, no filtering will be applied.\n        verbose (bool, optional): If True, enable info-level logging. Defaults to False.\n    \"\"\"\n    self.filters = filters if filters else {}\n    self.verbose = verbose\n    self.metrics = {}  # Add metrics dictionary to store metadata\n\n    if verbose:\n        # Configure logging for Jupyter notebook compatibility\n        import logging\n        import sys\n\n        # Create logger\n        self.logger = logging.getLogger(f\"{__name__}.WeightedPoissonPredictor\")\n\n        # Only set up handlers if they don't exist\n        if not self.logger.handlers:\n            self.logger.setLevel(logging.INFO if verbose else logging.WARNING)\n\n            # Create handler that writes to sys.stdout\n            handler = logging.StreamHandler(sys.stdout)\n            handler.setLevel(logging.INFO if verbose else logging.WARNING)\n\n            # Create a formatting configuration\n            formatter = logging.Formatter(\"%(message)s\")\n            handler.setFormatter(formatter)\n\n            # Add the handler to the logger\n            self.logger.addHandler(handler)\n\n            # Prevent propagation to root logger\n            self.logger.propagate = False\n\n    # Apply filters\n    self.filters = filters if filters else {}\n</code></pre>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor.WeightedPoissonPredictor.filter_dataframe","title":"<code>filter_dataframe(df, filters)</code>","text":"<p>Apply a set of filters to a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to filter.</p> required <code>filters</code> <code>dict</code> <p>A dictionary where keys are column names and values are the criteria or function to filter by.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A filtered DataFrame.</p> Source code in <code>src/patientflow/predictors/weighted_poisson_predictor.py</code> <pre><code>def filter_dataframe(self, df: pd.DataFrame, filters: Dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply a set of filters to a dataframe.\n\n    Args:\n        df (pandas.DataFrame): The DataFrame to filter.\n        filters (dict): A dictionary where keys are column names and values are the criteria or function to filter by.\n\n    Returns:\n        pandas.DataFrame: A filtered DataFrame.\n\n    \"\"\"\n    filtered_df = df\n    for column, criteria in filters.items():\n        if callable(criteria):  # If the criteria is a function, apply it directly\n            filtered_df = filtered_df[filtered_df[column].apply(criteria)]\n        else:  # Otherwise, assume the criteria is a value or list of values for equality check\n            filtered_df = filtered_df[filtered_df[column] == criteria]\n    return filtered_df\n</code></pre>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor.WeightedPoissonPredictor.fit","title":"<code>fit(train_df, prediction_window, yta_time_interval, prediction_times, num_days, epsilon=10 ** -7, y=None)</code>","text":"<p>Fits the model to the training data, computing necessary parameters for future predictions.</p> <p>Parameters:</p> Name Type Description Default <code>train_df</code> <code>DataFrame</code> <p>The training dataset with historical admission data.</p> required <code>prediction_window</code> <code>int</code> <p>The prediction prediction window in minutes.</p> required <code>yta_time_interval</code> <code>int</code> <p>The interval in minutes for splitting the prediction window.</p> required <code>prediction_times</code> <code>list</code> <p>Times of day at which predictions are made, in hours.</p> required <code>num_days</code> <code>int</code> <p>The number of days that the train_df spans</p> required <code>epsilon</code> <code>float</code> <p>A small value representing acceptable error rate to enable calculation of the maximum value of the random variable representing number of beds.</p> <code>10 ** -7</code> <code>y</code> <code>None</code> <p>Ignored, present for compatibility with scikit-learn's fit method.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>WeightedPoissonPredictor</code> <code>WeightedPoissonPredictor</code> <p>The instance itself, fitted with the training data.</p> Source code in <code>src/patientflow/predictors/weighted_poisson_predictor.py</code> <pre><code>def fit(\n    self,\n    train_df: pd.DataFrame,\n    prediction_window: int,\n    yta_time_interval: int,\n    prediction_times: List[float],\n    num_days: int,\n    epsilon: float = 10**-7,\n    y: Optional[None] = None,\n) -&gt; \"WeightedPoissonPredictor\":\n    \"\"\"\n    Fits the model to the training data, computing necessary parameters for future predictions.\n\n    Args:\n        train_df (pandas.DataFrame):\n            The training dataset with historical admission data.\n        prediction_window (int):\n            The prediction prediction window in minutes.\n        yta_time_interval (int):\n            The interval in minutes for splitting the prediction window.\n        prediction_times (list):\n            Times of day at which predictions are made, in hours.\n        num_days (int):\n             The number of days that the train_df spans\n        epsilon (float, optional):\n            A small value representing acceptable error rate to enable calculation of the maximum value of the random variable representing number of beds.\n        y (None, optional):\n            Ignored, present for compatibility with scikit-learn's fit method.\n\n    Returns:\n        WeightedPoissonPredictor: The instance itself, fitted with the training data.\n\n    \"\"\"\n    # Add error checking at the start of fit\n    if int(prediction_window / yta_time_interval) == 0:\n        raise ValueError(\n            f\"prediction_window ({prediction_window}) divided by yta_time_interval ({yta_time_interval}) must be greater than 1 to generate meaningful predictions\"\n        )\n\n    # Store prediction_window, yta_time_interval, and any other parameters as instance variables\n    self.prediction_window = prediction_window\n    self.yta_time_interval = yta_time_interval\n    self.epsilon = epsilon\n    self.prediction_times = [\n        tuple(x)\n        if isinstance(x, (list, np.ndarray))\n        else (x, 0)\n        if isinstance(x, (int, float))\n        else x\n        for x in prediction_times\n    ]\n\n    # Initialize yet_to_arrive_dict\n    self.weights = {}\n\n    # If there are filters specified, calculate and store the parameters directly with the respective spec keys\n    if self.filters:\n        for spec, filters in self.filters.items():\n            self.weights[spec] = self._calculate_parameters(\n                self.filter_dataframe(train_df, filters),\n                prediction_window,\n                yta_time_interval,\n                prediction_times,\n                num_days,\n            )\n    else:\n        # If there are no filters, store the parameters with a generic key, like 'default' or 'unfiltered'\n        self.weights[\"default\"] = self._calculate_parameters(\n            train_df,\n            prediction_window,\n            yta_time_interval,\n            prediction_times,\n            num_days,\n        )\n\n    if self.verbose:\n        self.logger.info(\n            f\"Poisson Binomial Predictor trained for these times: {prediction_times}\"\n        )\n        self.logger.info(\n            f\"using prediction window of {prediction_window} minutes after the time of prediction\"\n        )\n        self.logger.info(\n            f\"and time interval of {yta_time_interval} minutes within the prediction window.\"\n        )\n        self.logger.info(f\"The error value for prediction will be {epsilon}\")\n        self.logger.info(\n            \"To see the weights saved by this model, used the get_weights() method\"\n        )\n\n    # Store metrics about the training data\n    self.metrics[\"train_dttm\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    self.metrics[\"train_set_no\"] = len(train_df)\n    self.metrics[\"start_date\"] = train_df.index.min().date()\n    self.metrics[\"end_date\"] = train_df.index.max().date()\n    self.metrics[\"num_days\"] = num_days\n\n    return self\n</code></pre>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor.WeightedPoissonPredictor.get_weights","title":"<code>get_weights()</code>","text":"<p>Returns the weights computed by the fit method.</p> <p>Returns     dict: The weights.</p> Source code in <code>src/patientflow/predictors/weighted_poisson_predictor.py</code> <pre><code>def get_weights(self):\n    \"\"\"\n    Returns the weights computed by the fit method.\n\n    Returns\n        dict: The weights.\n\n    \"\"\"\n    return self.weights\n</code></pre>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor.WeightedPoissonPredictor.predict","title":"<code>predict(prediction_context, x1, y1, x2, y2)</code>","text":"<p>Predicts the number of admissions for the given context based on the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_context</code> <code>dict</code> <p>A dictionary defining the context for which predictions are to be made.                        It should specify either a general context or one based on the applied filters.</p> required <code>x1</code> <p>float The x-coordinate of the first transition point on the aspirational curve, where the growth phase ends and the decay phase begins.</p> required <code>y1</code> <p>float The y-coordinate of the first transition point (x1), representing the target proportion of patients admitted by time x1.</p> required <code>x2</code> <p>float The x-coordinate of the second transition point on the curve, beyond which all but a few patients are expected to be admitted.</p> required <code>y2</code> <p>float The y-coordinate of the second transition point (x2), representing the target proportion of patients admitted by time x2.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>A dictionary with predictions for each specified context.</p> Source code in <code>src/patientflow/predictors/weighted_poisson_predictor.py</code> <pre><code>def predict(\n    self, prediction_context: Dict, x1: float, y1: float, x2: float, y2: float\n) -&gt; Dict:\n    \"\"\"\n    Predicts the number of admissions for the given context based on the fitted model.\n\n    Args:\n        prediction_context (dict): A dictionary defining the context for which predictions are to be made.\n                                   It should specify either a general context or one based on the applied filters.\n        x1 : float\n            The x-coordinate of the first transition point on the aspirational curve, where the growth phase ends and the decay phase begins.\n        y1 : float\n            The y-coordinate of the first transition point (x1), representing the target proportion of patients admitted by time x1.\n        x2 : float\n            The x-coordinate of the second transition point on the curve, beyond which all but a few patients are expected to be admitted.\n        y2 : float\n            The y-coordinate of the second transition point (x2), representing the target proportion of patients admitted by time x2.\n\n    Returns:\n        dict: A dictionary with predictions for each specified context.\n\n    \"\"\"\n    predictions = {}\n\n    NTimes = int(self.prediction_window / self.yta_time_interval)\n    # Calculate theta, probability of admission in prediction window\n\n    # for each time interval, calculate time remaining before end of window\n    time_remaining_before_end_of_window = self.prediction_window / 60 - np.arange(\n        0, self.prediction_window / 60, self.yta_time_interval / 60\n    )\n\n    # probability of admission in that time\n    theta = get_y_from_aspirational_curve(\n        time_remaining_before_end_of_window, x1, y1, x2, y2\n    )\n\n    for filter_key, filter_values in prediction_context.items():\n        try:\n            if filter_key not in self.weights:\n                raise ValueError(\n                    f\"Filter key '{filter_key}' is not recognized in the model weights.\"\n                )\n\n            prediction_time = filter_values.get(\"prediction_time\")\n            if prediction_time is None:\n                raise ValueError(\n                    f\"No 'prediction_time' provided for filter '{filter_key}'.\"\n                )\n\n            if prediction_time not in self.prediction_times:\n                prediction_time = find_nearest_previous_prediction_time(\n                    prediction_time, self.prediction_times\n                )\n\n            lambda_t = self.weights[filter_key][prediction_time].get(\"lambda_t\")\n            if lambda_t is None:\n                raise ValueError(\n                    f\"No 'lambda_t' found for the time of day '{prediction_time}' under filter '{filter_key}'.\"\n                )\n\n            predictions[filter_key] = poisson_binom_generating_function(\n                NTimes, lambda_t, theta, self.epsilon\n            )\n\n        except KeyError as e:\n            raise KeyError(f\"Key error occurred: {e!s}\")\n\n    return predictions\n</code></pre>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor.aggregate_probabilities","title":"<code>aggregate_probabilities(lam, kmax, theta, time_index)</code>","text":"<p>Aggregate probabilities for a range of values using the weighted Poisson-Binomial distribution.</p> <p>Parameters lam (numpy.ndarray): An array of lambda values for each time interval. kmax (int): The maximum number of events to consider. theta (numpy.ndarray): An array of theta values for each time interval. time_index (int): The current time index for which to calculate probabilities.</p> <p>Returns numpy.ndarray: Aggregated probabilities for the given time index.</p> Source code in <code>src/patientflow/predictors/weighted_poisson_predictor.py</code> <pre><code>def aggregate_probabilities(lam, kmax, theta, time_index):\n    \"\"\"\n    Aggregate probabilities for a range of values using the weighted Poisson-Binomial distribution.\n\n    Parameters\n    lam (numpy.ndarray): An array of lambda values for each time interval.\n    kmax (int): The maximum number of events to consider.\n    theta (numpy.ndarray): An array of theta values for each time interval.\n    time_index (int): The current time index for which to calculate probabilities.\n\n    Returns\n    numpy.ndarray: Aggregated probabilities for the given time index.\n\n    \"\"\"\n    if kmax &lt; 0 or time_index &lt; 0 or len(lam) &lt;= time_index or len(theta) &lt;= time_index:\n        raise ValueError(\"Invalid kmax, time_index, or array lengths.\")\n\n    probabilities_matrix = np.zeros((kmax + 1, kmax + 1))\n    for i in range(kmax + 1):\n        probabilities_matrix[: i + 1, i] = weighted_poisson_binomial(\n            i, lam[time_index], theta[time_index]\n        )\n    return probabilities_matrix.sum(axis=1)\n</code></pre>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor.convolute_distributions","title":"<code>convolute_distributions(dist_a, dist_b)</code>","text":"<p>Convolutes two probability distributions represented as dataframes.</p> <p>Parameters dist_a (pd.DataFrame): The first distribution with columns ['sum', 'prob']. dist_b (pd.DataFrame): The second distribution with columns ['sum', 'prob'].</p> <p>Returns pd.DataFrame: The convoluted distribution.</p> Source code in <code>src/patientflow/predictors/weighted_poisson_predictor.py</code> <pre><code>def convolute_distributions(dist_a, dist_b):\n    \"\"\"\n    Convolutes two probability distributions represented as dataframes.\n\n    Parameters\n    dist_a (pd.DataFrame): The first distribution with columns ['sum', 'prob'].\n    dist_b (pd.DataFrame): The second distribution with columns ['sum', 'prob'].\n\n    Returns\n    pd.DataFrame: The convoluted distribution.\n\n    \"\"\"\n    if not {\"sum\", \"prob\"}.issubset(dist_a.columns) or not {\n        \"sum\",\n        \"prob\",\n    }.issubset(dist_b.columns):\n        raise ValueError(\"DataFrames must contain 'sum' and 'prob' columns.\")\n\n    sums = [x + y for x in dist_a[\"sum\"] for y in dist_b[\"sum\"]]\n    probs = [x * y for x in dist_a[\"prob\"] for y in dist_b[\"prob\"]]\n    result = pd.DataFrame(zip(sums, probs), columns=[\"sum\", \"prob\"])\n    return result.groupby(\"sum\")[\"prob\"].sum().reset_index()\n</code></pre>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor.find_nearest_previous_prediction_time","title":"<code>find_nearest_previous_prediction_time(requested_time, prediction_times)</code>","text":"<p>Finds the nearest previous time of day in 'prediction_times' relative to the requested time. If the requested time is earlier than all times in 'prediction_times', the function returns the latest time in 'prediction_times'.</p> <p>Parameters:</p> Name Type Description Default <code>requested_time</code> <code>tuple</code> <p>The requested time as (hour, minute).</p> required <code>prediction_times</code> <code>list</code> <p>List of available prediction times.</p> required <p>Returns:</p> Name Type Description <code>closest_prediction_time</code> <code>tuple</code> <p>The closest previous time of day from 'prediction_times'.</p> Source code in <code>src/patientflow/predictors/weighted_poisson_predictor.py</code> <pre><code>def find_nearest_previous_prediction_time(requested_time, prediction_times):\n    \"\"\"\n    Finds the nearest previous time of day in 'prediction_times' relative to the requested time.\n    If the requested time is earlier than all times in 'prediction_times', the function returns\n    the latest time in 'prediction_times'.\n\n    Args:\n        requested_time (tuple): The requested time as (hour, minute).\n        prediction_times (list): List of available prediction times.\n\n    Returns:\n        closest_prediction_time (tuple): The closest previous time of day from 'prediction_times'.\n\n    \"\"\"\n    if requested_time in prediction_times:\n        return requested_time\n\n    original_prediction_time = requested_time\n    requested_datetime = datetime.strptime(\n        f\"{requested_time[0]:02d}:{requested_time[1]:02d}\", \"%H:%M\"\n    )\n    closest_prediction_time = max(\n        prediction_times,\n        key=lambda prediction_time_time: datetime.strptime(\n            f\"{prediction_time_time[0]:02d}:{prediction_time_time[1]:02d}\",\n            \"%H:%M\",\n        ),\n    )\n    min_diff = float(\"inf\")\n\n    for prediction_time_time in prediction_times:\n        prediction_time_datetime = datetime.strptime(\n            f\"{prediction_time_time[0]:02d}:{prediction_time_time[1]:02d}\",\n            \"%H:%M\",\n        )\n        diff = (requested_datetime - prediction_time_datetime).total_seconds()\n\n        # If the difference is negative, it means the prediction_time_time is ahead of the requested_time,\n        # hence we calculate the difference by considering a day's wrap around.\n        if diff &lt; 0:\n            diff += 24 * 60 * 60  # Add 24 hours in seconds\n\n        if 0 &lt;= diff &lt; min_diff:\n            closest_prediction_time = prediction_time_time\n            min_diff = diff\n\n    warnings.warn(\n        f\"Time of day requested of {original_prediction_time} was not in model training. \"\n        f\"Reverting to predictions for {closest_prediction_time}.\"\n    )\n\n    return closest_prediction_time\n</code></pre>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor.poisson_binom_generating_function","title":"<code>poisson_binom_generating_function(NTimes, lambda_t, theta, epsilon)</code>","text":"<p>Generate a distribution based on the aggregate of Poisson and Binomial distributions over time intervals.</p> <p>Parameters NTimes (int): The number of time intervals. lambda_t (numpy.ndarray): An array of lambda values for each time interval. theta (numpy.ndarray): An array of theta values for each time interval. epsilon (float): The desired error threshold.</p> <p>Returns pd.DataFrame: The generated distribution.</p> Source code in <code>src/patientflow/predictors/weighted_poisson_predictor.py</code> <pre><code>def poisson_binom_generating_function(NTimes, lambda_t, theta, epsilon):\n    \"\"\"\n    Generate a distribution based on the aggregate of Poisson and Binomial distributions over time intervals.\n\n    Parameters\n    NTimes (int): The number of time intervals.\n    lambda_t (numpy.ndarray): An array of lambda values for each time interval.\n    theta (numpy.ndarray): An array of theta values for each time interval.\n    epsilon (float): The desired error threshold.\n\n    Returns\n    pd.DataFrame: The generated distribution.\n\n    \"\"\"\n\n    if NTimes &lt;= 0 or epsilon &lt;= 0 or epsilon &gt;= 1:\n        raise ValueError(\"Ensure NTimes &gt; 0 and 0 &lt; epsilon &lt; 1.\")\n\n    maxlam = max(lambda_t)\n    kmax = int(poisson.ppf(1 - epsilon, maxlam))\n    distribution = np.zeros((kmax + 1, NTimes))\n\n    for j in range(NTimes):\n        distribution[:, j] = aggregate_probabilities(lambda_t, kmax, theta, j)\n\n    df_list = [\n        pd.DataFrame({\"sum\": range(kmax + 1), \"prob\": distribution[:, j]})\n        for j in range(NTimes)\n    ]\n    total_distribution = df_list[0]\n\n    for df in df_list[1:]:\n        total_distribution = convolute_distributions(total_distribution, df)\n\n    total_distribution = total_distribution.rename(\n        columns={\"prob\": \"agg_proba\"}\n    ).set_index(\"sum\")\n\n    return total_distribution\n</code></pre>"},{"location":"api/#patientflow.predictors.weighted_poisson_predictor.weighted_poisson_binomial","title":"<code>weighted_poisson_binomial(i, lam, theta)</code>","text":"<p>Calculate weighted probabilities using Poisson and Binomial distributions.</p> <p>Parameters i (int): The upper bound of the range for the binomial distribution. lam (float): The lambda parameter for the Poisson distribution. theta (float): The probability of success for the binomial distribution.</p> <p>Returns numpy.ndarray: An array of weighted probabilities.</p> Source code in <code>src/patientflow/predictors/weighted_poisson_predictor.py</code> <pre><code>def weighted_poisson_binomial(i, lam, theta):\n    \"\"\"\n    Calculate weighted probabilities using Poisson and Binomial distributions.\n\n    Parameters\n    i (int): The upper bound of the range for the binomial distribution.\n    lam (float): The lambda parameter for the Poisson distribution.\n    theta (float): The probability of success for the binomial distribution.\n\n    Returns\n    numpy.ndarray: An array of weighted probabilities.\n\n    \"\"\"\n    if i &lt; 0 or lam &lt; 0 or not 0 &lt;= theta &lt;= 1:\n        raise ValueError(\"Ensure i &gt;= 0, lam &gt;= 0, and 0 &lt;= theta &lt;= 1.\")\n\n    arr_seq = np.arange(i + 1)\n    probabilities = binom.pmf(arr_seq, i, theta)\n    return poisson.pmf(i, lam) * probabilities\n</code></pre>"},{"location":"api/#patientflow.prepare","title":"<code>prepare</code>","text":"<p>Module for preparing data, loading models, and organizing snapshots for inference.</p> <p>This module provides functionality to load a trained model, prepare data for making predictions, calculate arrival rates, and organize snapshot data. It allows for selecting one snapshot per visit, filtering snapshots by prediction time, and mapping snapshot dates to corresponding indices.</p>"},{"location":"api/#patientflow.prepare--functions","title":"Functions","text":"<p>prepare_for_inference(model_file_path, model_name, prediction_time=None,                       model_only=False, df=None, data_path=None,                       single_snapshot_per_visit=True, index_column='snapshot_id',                       sort_columns=None, eval_columns=None,                       exclude_from_training_data=None)     Loads a model and prepares data for inference.</p> <p>select_one_snapshot_per_visit(df, visit_col, seed=42)     Selects one snapshot per visit based on a random number and returns the filtered DataFrame.</p> <p>get_snapshots_at_prediction_time(df, prediction_time, exclude_columns, single_snapshot_per_visit=True)     Filters the DataFrame by prediction time and optionally selects one snapshot per visit.</p> <p>prepare_snapshots_dict(df, start_dt=None, end_dt=None)     Prepares a dictionary mapping snapshot dates to their corresponding snapshot indices.</p> <p>calculate_time_varying_arrival_rates(df, yta_time_interval)     Calculates the time-varying arrival rates for a dataset indexed by datetime.</p>"},{"location":"api/#patientflow.prepare.SpecialCategoryParams","title":"<code>SpecialCategoryParams</code>","text":"<p>A picklable implementation of special category parameters for patient classification.</p> <p>This class identifies pediatric patients based on available age-related columns in the dataset and provides functions to categorize patients accordingly. It's designed to be serializable with pickle by implementing the reduce method.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>list</code> <p>List of column names from the dataset</p> <code>method_type</code> <code>str</code> <p>The method used for age detection ('age_on_arrival' or 'age_group')</p> <code>special_category_dict</code> <code>dict</code> <p>Default category values mapping</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>class SpecialCategoryParams:\n    \"\"\"\n    A picklable implementation of special category parameters for patient classification.\n\n    This class identifies pediatric patients based on available age-related columns\n    in the dataset and provides functions to categorize patients accordingly.\n    It's designed to be serializable with pickle by implementing the __reduce__ method.\n\n    Attributes:\n        columns (list): List of column names from the dataset\n        method_type (str): The method used for age detection ('age_on_arrival' or 'age_group')\n        special_category_dict (dict): Default category values mapping\n    \"\"\"\n\n    def __init__(self, columns):\n        \"\"\"\n        Initialize the SpecialCategoryParams object.\n\n        Parameters:\n            columns (list or pandas.Index): Column names from the dataset\n                used to determine the appropriate age identification method\n\n        Raises:\n            ValueError: If neither 'age_on_arrival' nor 'age_group' columns are found\n        \"\"\"\n        self.columns = columns\n        self.special_category_dict = {\n            \"medical\": 0.0,\n            \"surgical\": 0.0,\n            \"haem/onc\": 0.0,\n            \"paediatric\": 1.0,\n        }\n\n        if \"age_on_arrival\" in columns:\n            self.method_type = \"age_on_arrival\"\n        elif \"age_group\" in columns:\n            self.method_type = \"age_group\"\n        else:\n            raise ValueError(\"Unknown data format: could not find expected age columns\")\n\n    def special_category_func(self, row):\n        \"\"\"\n        Identify if a patient is pediatric based on age data.\n\n        Parameters:\n            row (dict or pandas.Series): A row of patient data containing either\n                'age_on_arrival' or 'age_group'\n\n        Returns:\n            bool: True if the patient is pediatric (age &lt; 18 or age_group is '0-17'),\n                 False otherwise\n        \"\"\"\n        if self.method_type == \"age_on_arrival\":\n            return row[\"age_on_arrival\"] &lt; 18\n        else:  # age_group\n            return row[\"age_group\"] == \"0-17\"\n\n    def opposite_special_category_func(self, row):\n        \"\"\"\n        Identify if a patient is NOT pediatric.\n\n        Parameters:\n            row (dict or pandas.Series): A row of patient data\n\n        Returns:\n            bool: True if the patient is NOT pediatric, False if they are pediatric\n        \"\"\"\n        return not self.special_category_func(row)\n\n    def get_params_dict(self):\n        \"\"\"\n        Get the special parameter dictionary in the format expected by the application.\n\n        Returns:\n            dict: A dictionary containing:\n                - 'special_category_func': Function to identify pediatric patients\n                - 'special_category_dict': Default category values\n                - 'special_func_map': Mapping of category names to detection functions\n        \"\"\"\n        return {\n            \"special_category_func\": self.special_category_func,\n            \"special_category_dict\": self.special_category_dict,\n            \"special_func_map\": {\n                \"paediatric\": self.special_category_func,\n                \"default\": self.opposite_special_category_func,\n            },\n        }\n\n    def __reduce__(self):\n        \"\"\"\n        Support for pickle serialization.\n\n        Returns:\n            tuple: A tuple containing:\n                - The class itself (to be called as a function)\n                - A tuple of arguments to pass to the class constructor\n        \"\"\"\n        return (self.__class__, (self.columns,))\n</code></pre>"},{"location":"api/#patientflow.prepare.SpecialCategoryParams.__init__","title":"<code>__init__(columns)</code>","text":"<p>Initialize the SpecialCategoryParams object.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list or Index</code> <p>Column names from the dataset used to determine the appropriate age identification method</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither 'age_on_arrival' nor 'age_group' columns are found</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def __init__(self, columns):\n    \"\"\"\n    Initialize the SpecialCategoryParams object.\n\n    Parameters:\n        columns (list or pandas.Index): Column names from the dataset\n            used to determine the appropriate age identification method\n\n    Raises:\n        ValueError: If neither 'age_on_arrival' nor 'age_group' columns are found\n    \"\"\"\n    self.columns = columns\n    self.special_category_dict = {\n        \"medical\": 0.0,\n        \"surgical\": 0.0,\n        \"haem/onc\": 0.0,\n        \"paediatric\": 1.0,\n    }\n\n    if \"age_on_arrival\" in columns:\n        self.method_type = \"age_on_arrival\"\n    elif \"age_group\" in columns:\n        self.method_type = \"age_group\"\n    else:\n        raise ValueError(\"Unknown data format: could not find expected age columns\")\n</code></pre>"},{"location":"api/#patientflow.prepare.SpecialCategoryParams.__reduce__","title":"<code>__reduce__()</code>","text":"<p>Support for pickle serialization.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing: - The class itself (to be called as a function) - A tuple of arguments to pass to the class constructor</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def __reduce__(self):\n    \"\"\"\n    Support for pickle serialization.\n\n    Returns:\n        tuple: A tuple containing:\n            - The class itself (to be called as a function)\n            - A tuple of arguments to pass to the class constructor\n    \"\"\"\n    return (self.__class__, (self.columns,))\n</code></pre>"},{"location":"api/#patientflow.prepare.SpecialCategoryParams.get_params_dict","title":"<code>get_params_dict()</code>","text":"<p>Get the special parameter dictionary in the format expected by the application.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing: - 'special_category_func': Function to identify pediatric patients - 'special_category_dict': Default category values - 'special_func_map': Mapping of category names to detection functions</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def get_params_dict(self):\n    \"\"\"\n    Get the special parameter dictionary in the format expected by the application.\n\n    Returns:\n        dict: A dictionary containing:\n            - 'special_category_func': Function to identify pediatric patients\n            - 'special_category_dict': Default category values\n            - 'special_func_map': Mapping of category names to detection functions\n    \"\"\"\n    return {\n        \"special_category_func\": self.special_category_func,\n        \"special_category_dict\": self.special_category_dict,\n        \"special_func_map\": {\n            \"paediatric\": self.special_category_func,\n            \"default\": self.opposite_special_category_func,\n        },\n    }\n</code></pre>"},{"location":"api/#patientflow.prepare.SpecialCategoryParams.opposite_special_category_func","title":"<code>opposite_special_category_func(row)</code>","text":"<p>Identify if a patient is NOT pediatric.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>dict or Series</code> <p>A row of patient data</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the patient is NOT pediatric, False if they are pediatric</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def opposite_special_category_func(self, row):\n    \"\"\"\n    Identify if a patient is NOT pediatric.\n\n    Parameters:\n        row (dict or pandas.Series): A row of patient data\n\n    Returns:\n        bool: True if the patient is NOT pediatric, False if they are pediatric\n    \"\"\"\n    return not self.special_category_func(row)\n</code></pre>"},{"location":"api/#patientflow.prepare.SpecialCategoryParams.special_category_func","title":"<code>special_category_func(row)</code>","text":"<p>Identify if a patient is pediatric based on age data.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>dict or Series</code> <p>A row of patient data containing either 'age_on_arrival' or 'age_group'</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the patient is pediatric (age &lt; 18 or age_group is '0-17'),  False otherwise</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def special_category_func(self, row):\n    \"\"\"\n    Identify if a patient is pediatric based on age data.\n\n    Parameters:\n        row (dict or pandas.Series): A row of patient data containing either\n            'age_on_arrival' or 'age_group'\n\n    Returns:\n        bool: True if the patient is pediatric (age &lt; 18 or age_group is '0-17'),\n             False otherwise\n    \"\"\"\n    if self.method_type == \"age_on_arrival\":\n        return row[\"age_on_arrival\"] &lt; 18\n    else:  # age_group\n        return row[\"age_group\"] == \"0-17\"\n</code></pre>"},{"location":"api/#patientflow.prepare.assign_mrns","title":"<code>assign_mrns(df, start_training_set, start_validation_set, start_test_set, end_test_set, col_name='arrival_datetime')</code>","text":"<p>Probabilistically assign MRNs to train/validation/test sets.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with mrn, encounter, and temporal columns</p> required <code>start_training_set</code> <code>date</code> <p>Start date for training period</p> required <code>start_validation_set</code> <code>date</code> <p>Start date for validation period</p> required <code>start_test_set</code> <code>date</code> <p>Start date for test period</p> required <code>end_test_set</code> <code>date</code> <p>End date for test period</p> required <code>col_name</code> <code>str</code> <p>Column name for temporal splitting</p> <code>'arrival_datetime'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with MRN assignments based on weighted random sampling</p> Notes <ul> <li>Counts encounters in each time period per MRN</li> <li>Randomly assigns each MRN to one set, weighted by their temporal distribution</li> <li>MRN with 70% encounters in training, 30% in validation has 70% chance of training assignment</li> </ul> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def assign_mrns(\n    df: pd.DataFrame,\n    start_training_set: date,\n    start_validation_set: date,\n    start_test_set: date,\n    end_test_set: date,\n    col_name: str = \"arrival_datetime\",\n) -&gt; pd.DataFrame:\n    \"\"\"Probabilistically assign MRNs to train/validation/test sets.\n\n    Args:\n        df: DataFrame with mrn, encounter, and temporal columns\n        start_training_set: Start date for training period\n        start_validation_set: Start date for validation period\n        start_test_set: Start date for test period\n        end_test_set: End date for test period\n        col_name: Column name for temporal splitting\n\n    Returns:\n        DataFrame with MRN assignments based on weighted random sampling\n\n    Notes:\n        - Counts encounters in each time period per MRN\n        - Randomly assigns each MRN to one set, weighted by their temporal distribution\n        - MRN with 70% encounters in training, 30% in validation has 70% chance of training assignment\n    \"\"\"\n    mrns: pd.DataFrame = df.groupby([\"mrn\", \"encounter\"])[col_name].max().reset_index()\n\n    # Filter out MRNs outside temporal bounds\n    pre_training_mrns = mrns[mrns[col_name].dt.date &lt; start_training_set]\n    post_test_mrns = mrns[mrns[col_name].dt.date &gt;= end_test_set]\n\n    if len(pre_training_mrns) &gt; 0:\n        print(\n            f\"Filtered out {len(pre_training_mrns)} MRNs with only pre-training visits\"\n        )\n    if len(post_test_mrns) &gt; 0:\n        print(f\"Filtered out {len(post_test_mrns)} MRNs with only post-test visits\")\n\n    valid_mrns = mrns[\n        (mrns[col_name].dt.date &gt;= start_training_set)\n        &amp; (mrns[col_name].dt.date &lt; end_test_set)\n    ]\n    mrns = valid_mrns\n\n    mrns[\"training_set\"] = (mrns[col_name].dt.date &gt;= start_training_set) &amp; (\n        mrns[col_name].dt.date &lt; start_validation_set\n    )\n    mrns[\"validation_set\"] = (mrns[col_name].dt.date &gt;= start_validation_set) &amp; (\n        mrns[col_name].dt.date &lt; start_test_set\n    )\n    mrns[\"test_set\"] = (mrns[col_name].dt.date &gt;= start_test_set) &amp; (\n        mrns[col_name].dt.date &lt; end_test_set\n    )\n\n    mrns = mrns.groupby(\"mrn\")[[\"training_set\", \"validation_set\", \"test_set\"]].sum()\n    mrns[\"training_validation_test\"] = mrns.apply(apply_set, axis=1)\n\n    print(\n        f\"\\nMRN Set Overlaps (before random assignment):\"\n        f\"\\nTrain-Valid: {mrns[mrns.training_set * mrns.validation_set != 0].shape[0]} of {mrns[mrns.training_set + mrns.validation_set &gt; 0].shape[0]}\"\n        f\"\\nValid-Test: {mrns[mrns.validation_set * mrns.test_set != 0].shape[0]} of {mrns[mrns.validation_set + mrns.test_set &gt; 0].shape[0]}\"\n        f\"\\nTrain-Test: {mrns[mrns.training_set * mrns.test_set != 0].shape[0]} of {mrns[mrns.training_set + mrns.test_set &gt; 0].shape[0]}\"\n        f\"\\nAll Sets: {mrns[mrns.training_set * mrns.validation_set * mrns.test_set != 0].shape[0]} of {mrns.shape[0]} total MRNs\"\n    )\n\n    return mrns\n</code></pre>"},{"location":"api/#patientflow.prepare.create_special_category_objects","title":"<code>create_special_category_objects(columns)</code>","text":"<p>Creates a configuration for categorizing patients with special handling for pediatric cases.</p>"},{"location":"api/#patientflow.prepare.create_special_category_objects--parameters","title":"Parameters:","text":"<p>columns : list or pandas.Index     The column names available in the dataset. Used to determine which age format is present.</p>"},{"location":"api/#patientflow.prepare.create_special_category_objects--returns","title":"Returns:","text":"<p>dict     A dictionary containing special category configuration.</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def create_special_category_objects(columns):\n    \"\"\"\n    Creates a configuration for categorizing patients with special handling for pediatric cases.\n\n    Parameters:\n    -----------\n    columns : list or pandas.Index\n        The column names available in the dataset. Used to determine which age format is present.\n\n    Returns:\n    --------\n    dict\n        A dictionary containing special category configuration.\n    \"\"\"\n    # Create the class instance and return its parameter dictionary\n    params_obj = SpecialCategoryParams(columns)\n    return params_obj.get_params_dict()\n</code></pre>"},{"location":"api/#patientflow.prepare.create_temporal_splits","title":"<code>create_temporal_splits(df, start_train, start_valid, start_test, end_test, col_name='arrival_datetime')</code>","text":"<p>Split dataset into temporal train/validation/test sets.</p> <p>Creates temporal data splits using primary datetime column and optional snapshot dates. Handles MRN (patient ID) grouping if present to prevent data leakage.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe</p> required <code>start_train</code> <code>date</code> <p>Training start (inclusive)</p> required <code>start_valid</code> <code>date</code> <p>Validation start (inclusive)</p> required <code>start_test</code> <code>date</code> <p>Test start (inclusive)</p> required <code>end_test</code> <code>date</code> <p>Test end (exclusive)</p> required <code>col_name</code> <code>str</code> <p>Primary datetime column for splitting</p> <code>'arrival_datetime'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>(train_df, valid_df, test_df) Split dataframes</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def create_temporal_splits(\n    df: pd.DataFrame,\n    start_train: date,\n    start_valid: date,\n    start_test: date,\n    end_test: date,\n    col_name: str = \"arrival_datetime\",\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Split dataset into temporal train/validation/test sets.\n\n    Creates temporal data splits using primary datetime column and optional snapshot dates.\n    Handles MRN (patient ID) grouping if present to prevent data leakage.\n\n    Args:\n        df: Input dataframe\n        start_train: Training start (inclusive)\n        start_valid: Validation start (inclusive)\n        start_test: Test start (inclusive)\n        end_test: Test end (exclusive)\n        col_name: Primary datetime column for splitting\n\n    Returns:\n        tuple: (train_df, valid_df, test_df) Split dataframes\n    \"\"\"\n\n    def get_date_value(series: pd.Series) -&gt; pd.Series:\n        \"\"\"Convert timestamp or date column to date, handling both types\"\"\"\n        try:\n            return pd.to_datetime(series).dt.date\n        except (AttributeError, TypeError):\n            return series\n\n    if \"mrn\" in df.columns:\n        set_assignment: pd.DataFrame = assign_mrns(\n            df, start_train, start_valid, start_test, end_test, col_name\n        )\n        mrn_sets: Dict[str, Set] = {\n            k: set(set_assignment[set_assignment.training_validation_test == v].index)\n            for k, v in {\"train\": \"train\", \"valid\": \"valid\", \"test\": \"test\"}.items()\n        }\n\n    splits: List[pd.DataFrame] = []\n    for start, end, mrn_key in [\n        (start_train, start_valid, \"train\"),\n        (start_valid, start_test, \"valid\"),\n        (start_test, end_test, \"test\"),\n    ]:\n        mask = (get_date_value(df[col_name]) &gt;= start) &amp; (\n            get_date_value(df[col_name]) &lt; end\n        )\n\n        if \"snapshot_date\" in df.columns:\n            mask &amp;= (get_date_value(df.snapshot_date) &gt;= start) &amp; (\n                get_date_value(df.snapshot_date) &lt; end\n            )\n\n        if \"mrn\" in df.columns:\n            mask &amp;= df.mrn.isin(mrn_sets[mrn_key])\n\n        splits.append(df[mask].copy())\n\n    print(f\"Split sizes: {[len(split) for split in splits]}\")\n    return tuple(splits)\n</code></pre>"},{"location":"api/#patientflow.prepare.create_yta_filters","title":"<code>create_yta_filters(df)</code>","text":"<p>Create specialty filters for categorizing patients by specialty and age group.</p> <p>This function generates a dictionary of filters based on specialty categories, with special handling for pediatric patients. It uses the SpecialCategoryParams class to determine which specialties correspond to pediatric care.</p>"},{"location":"api/#patientflow.prepare.create_yta_filters--parameters","title":"Parameters:","text":"<p>df : pandas.DataFrame     DataFrame containing patient data with columns that include either     'age_on_arrival' or 'age_group' for pediatric classification</p>"},{"location":"api/#patientflow.prepare.create_yta_filters--returns","title":"Returns:","text":"<p>dict     A dictionary mapping specialty names to filter configurations.     Each configuration contains:     - For pediatric specialty: {\"is_child\": True}     - For other specialties: {\"specialty\": specialty_name, \"is_child\": False}</p>"},{"location":"api/#patientflow.prepare.create_yta_filters--examples","title":"Examples:","text":"<p>df = pd.DataFrame({'patient_id': [1, 2], 'age_on_arrival': [10, 40]}) filters = create_yta_filters(df) print(filters['paediatric']) {'is_child': True} print(filters['medical']) {'specialty': 'medical', 'is_child': False}</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def create_yta_filters(df):\n    \"\"\"\n    Create specialty filters for categorizing patients by specialty and age group.\n\n    This function generates a dictionary of filters based on specialty categories,\n    with special handling for pediatric patients. It uses the SpecialCategoryParams\n    class to determine which specialties correspond to pediatric care.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        DataFrame containing patient data with columns that include either\n        'age_on_arrival' or 'age_group' for pediatric classification\n\n    Returns:\n    --------\n    dict\n        A dictionary mapping specialty names to filter configurations.\n        Each configuration contains:\n        - For pediatric specialty: {\"is_child\": True}\n        - For other specialties: {\"specialty\": specialty_name, \"is_child\": False}\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; df = pd.DataFrame({'patient_id': [1, 2], 'age_on_arrival': [10, 40]})\n    &gt;&gt;&gt; filters = create_yta_filters(df)\n    &gt;&gt;&gt; print(filters['paediatric'])\n    {'is_child': True}\n    &gt;&gt;&gt; print(filters['medical'])\n    {'specialty': 'medical', 'is_child': False}\n    \"\"\"\n    # Get the special category parameters using the picklable implementation\n    special_params = create_special_category_objects(df.columns)\n\n    # Extract necessary data from the special_params\n    special_category_dict = special_params[\"special_category_dict\"]\n\n    # Create the specialty_filters dictionary\n    specialty_filters = {}\n\n    for specialty, is_paediatric_flag in special_category_dict.items():\n        if is_paediatric_flag == 1.0:\n            # For the paediatric specialty, set `is_child` to True\n            specialty_filters[specialty] = {\"is_child\": True}\n        else:\n            # For other specialties, set `is_child` to False\n            specialty_filters[specialty] = {\"specialty\": specialty, \"is_child\": False}\n\n    return specialty_filters\n</code></pre>"},{"location":"api/#patientflow.prepare.get_snapshots_at_prediction_time","title":"<code>get_snapshots_at_prediction_time(df, prediction_time, exclude_columns, single_snapshot_per_visit=True, visit_col='visit_number', label_col='is_admitted')</code>","text":"<p>Get snapshots of data at a specific prediction time with configurable visit and label columns.</p>"},{"location":"api/#patientflow.prepare.get_snapshots_at_prediction_time--parameters","title":"Parameters:","text":"<p>df : pandas.DataFrame     Input DataFrame containing the data prediction_time : str or datetime     The specific prediction time to filter for exclude_columns : list     List of columns to exclude from the final DataFrame single_snapshot_per_visit : bool, default=True     Whether to select only one snapshot per visit visit_col : str, default=\"visit_number\"     Name of the column containing visit identifiers label_col : str, default=\"is_admitted\"     Name of the column containing the target labels</p>"},{"location":"api/#patientflow.prepare.get_snapshots_at_prediction_time--returns","title":"Returns:","text":"<p>tuple(pandas.DataFrame, pandas.Series)     Processed DataFrame and corresponding labels</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def get_snapshots_at_prediction_time(\n    df,\n    prediction_time,\n    exclude_columns,\n    single_snapshot_per_visit=True,\n    visit_col=\"visit_number\",\n    label_col=\"is_admitted\",\n):\n    \"\"\"\n    Get snapshots of data at a specific prediction time with configurable visit and label columns.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        Input DataFrame containing the data\n    prediction_time : str or datetime\n        The specific prediction time to filter for\n    exclude_columns : list\n        List of columns to exclude from the final DataFrame\n    single_snapshot_per_visit : bool, default=True\n        Whether to select only one snapshot per visit\n    visit_col : str, default=\"visit_number\"\n        Name of the column containing visit identifiers\n    label_col : str, default=\"is_admitted\"\n        Name of the column containing the target labels\n\n    Returns:\n    --------\n    tuple(pandas.DataFrame, pandas.Series)\n        Processed DataFrame and corresponding labels\n    \"\"\"\n\n    # Filter by the time of day while keeping the original index\n    df_tod = df[df[\"prediction_time\"] == prediction_time].copy()\n\n    if single_snapshot_per_visit:\n        # Group by visit_col and get the row with the maximum 'random_number'\n        df_single = select_one_snapshot_per_visit(df_tod, visit_col)\n        # Create label array with the same index\n        y = df_single.pop(label_col).astype(int)\n        # Drop specified columns and ensure we do not reset the index\n        df_single.drop(columns=exclude_columns, inplace=True)\n        return df_single, y\n    else:\n        # Directly modify df_tod without resetting the index\n        df_tod.drop(\n            columns=[\"random_number\"] + exclude_columns, inplace=True, errors=\"ignore\"\n        )\n        y = df_tod.pop(label_col).astype(int)\n        return df_tod, y\n</code></pre>"},{"location":"api/#patientflow.prepare.prepare_for_inference","title":"<code>prepare_for_inference(model_file_path, model_name, prediction_time=None, model_only=False, df=None, data_path=None, single_snapshot_per_visit=True, index_column='snapshot_id', sort_columns=['visit_number', 'snapshot_date', 'prediction_time'], eval_columns=['prediction_time', 'consultation_sequence', 'final_sequence'], exclude_from_training_data=['visit_number', 'snapshot_date', 'prediction_time'])</code>","text":"<p>Load a trained model and prepare data for making predictions.</p> <p>This function retrieves a trained model from a specified file path and, if requested, prepares the data required for inference. The data can be provided either as a DataFrame or as a file path to a CSV file. The function allows filtering and processing of the data to match the model's requirements. If available, it will use the calibrated pipeline instead of the regular pipeline.</p>"},{"location":"api/#patientflow.prepare.prepare_for_inference--parameters","title":"Parameters","text":"<p>model_file_path : str     The file path where the trained model is saved. model_name : str     The name of the model to be loaded. prediction_time : str, optional     The time at which predictions are to be made. This is used to filter     the data for the relevant time snapshot. model_only : bool, optional     If True, only the model is returned. If False, both the prepared data     and the model are returned. Default is False. df : pandas.DataFrame, optional     The DataFrame containing the data to be used for inference. If not     provided, data_path must be specified. data_path : str, optional     The file path to a CSV file containing the data to be used for inference.     Ignored if <code>df</code> is provided. single_snapshot_per_visit : bool, optional     If True, only a single snapshot per visit is considered. Default is True. index_column : str, optional     The name of the index column in the data. Default is 'snapshot_id'. sort_columns : list of str, optional     The columns to sort the data by. Default is [\"visit_number\", \"snapshot_date\", \"prediction_time\"]. eval_columns : list of str, optional     The columns that require literal evaluation of their content when loading from csv.     Default is [\"prediction_time\", \"consultation_sequence\", \"final_sequence\"]. exclude_from_training_data : list of str, optional     The columns to be excluded from the training data. Default is [\"visit_number\", \"snapshot_date\", \"prediction_time\"].</p>"},{"location":"api/#patientflow.prepare.prepare_for_inference--returns","title":"Returns","text":"<p>model : object     The loaded model (calibrated pipeline if available, otherwise regular pipeline). X_test : pandas.DataFrame, optional     The features prepared for testing, returned only if model_only is False. y_test : pandas.Series, optional     The labels corresponding to X_test, returned only if model_only is False.</p>"},{"location":"api/#patientflow.prepare.prepare_for_inference--raises","title":"Raises","text":"<p>KeyError     If the 'training_validation_test' column is not found in the provided DataFrame.</p>"},{"location":"api/#patientflow.prepare.prepare_for_inference--notes","title":"Notes","text":"<ul> <li>Either <code>df</code> or <code>data_path</code> must be provided. If neither is provided or if <code>df</code>   is empty, the function will print an error message and return None.</li> <li>The function will automatically use a calibrated pipeline if one is available   in the model, otherwise it will fall back to the regular pipeline.</li> </ul> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def prepare_for_inference(\n    model_file_path,\n    model_name,\n    prediction_time=None,\n    model_only=False,\n    df=None,\n    data_path=None,\n    single_snapshot_per_visit=True,\n    index_column=\"snapshot_id\",\n    sort_columns=[\"visit_number\", \"snapshot_date\", \"prediction_time\"],\n    eval_columns=[\"prediction_time\", \"consultation_sequence\", \"final_sequence\"],\n    exclude_from_training_data=[\"visit_number\", \"snapshot_date\", \"prediction_time\"],\n):\n    \"\"\"\n    Load a trained model and prepare data for making predictions.\n\n    This function retrieves a trained model from a specified file path and,\n    if requested, prepares the data required for inference. The data can be\n    provided either as a DataFrame or as a file path to a CSV file. The function\n    allows filtering and processing of the data to match the model's requirements.\n    If available, it will use the calibrated pipeline instead of the regular pipeline.\n\n    Parameters\n    ----------\n    model_file_path : str\n        The file path where the trained model is saved.\n    model_name : str\n        The name of the model to be loaded.\n    prediction_time : str, optional\n        The time at which predictions are to be made. This is used to filter\n        the data for the relevant time snapshot.\n    model_only : bool, optional\n        If True, only the model is returned. If False, both the prepared data\n        and the model are returned. Default is False.\n    df : pandas.DataFrame, optional\n        The DataFrame containing the data to be used for inference. If not\n        provided, data_path must be specified.\n    data_path : str, optional\n        The file path to a CSV file containing the data to be used for inference.\n        Ignored if `df` is provided.\n    single_snapshot_per_visit : bool, optional\n        If True, only a single snapshot per visit is considered. Default is True.\n    index_column : str, optional\n        The name of the index column in the data. Default is 'snapshot_id'.\n    sort_columns : list of str, optional\n        The columns to sort the data by. Default is [\"visit_number\", \"snapshot_date\", \"prediction_time\"].\n    eval_columns : list of str, optional\n        The columns that require literal evaluation of their content when loading from csv.\n        Default is [\"prediction_time\", \"consultation_sequence\", \"final_sequence\"].\n    exclude_from_training_data : list of str, optional\n        The columns to be excluded from the training data. Default is [\"visit_number\", \"snapshot_date\", \"prediction_time\"].\n\n    Returns\n    -------\n    model : object\n        The loaded model (calibrated pipeline if available, otherwise regular pipeline).\n    X_test : pandas.DataFrame, optional\n        The features prepared for testing, returned only if model_only is False.\n    y_test : pandas.Series, optional\n        The labels corresponding to X_test, returned only if model_only is False.\n\n    Raises\n    ------\n    KeyError\n        If the 'training_validation_test' column is not found in the provided DataFrame.\n\n    Notes\n    -----\n    - Either `df` or `data_path` must be provided. If neither is provided or if `df`\n      is empty, the function will print an error message and return None.\n    - The function will automatically use a calibrated pipeline if one is available\n      in the model, otherwise it will fall back to the regular pipeline.\n    \"\"\"\n\n    # retrieve model trained for this time of day\n    model = load_saved_model(model_file_path, model_name, prediction_time)\n\n    # Use calibrated pipeline if available, otherwise use regular pipeline\n    if hasattr(model, \"calibrated_pipeline\") and model.calibrated_pipeline is not None:\n        pipeline = model.calibrated_pipeline\n    else:\n        pipeline = model.pipeline\n\n    if model_only:\n        return pipeline\n\n    if data_path:\n        df = data_from_csv(data_path, index_column, sort_columns, eval_columns)\n    elif df is None or df.empty:\n        print(\"Please supply a dataset if not passing a data path\")\n        return None\n\n    try:\n        test_df = (\n            df[df.training_validation_test == \"test\"]\n            .drop(columns=\"training_validation_test\")\n            .copy()\n        )\n    except KeyError:\n        print(\"Column training_validation_test not found in dataframe\")\n        return None\n\n    X_test, y_test = get_snapshots_at_prediction_time(\n        test_df,\n        prediction_time,\n        exclude_from_training_data,\n        single_snapshot_per_visit,\n    )\n\n    return X_test, y_test, pipeline\n</code></pre>"},{"location":"api/#patientflow.prepare.prepare_snapshots_dict","title":"<code>prepare_snapshots_dict(df, start_dt=None, end_dt=None)</code>","text":"<p>Prepares a dictionary mapping snapshot dates to their corresponding snapshot indices.</p> <p>Args: df (pd.DataFrame): DataFrame containing at least a 'snapshot_date' column which represents the dates. start_dt (datetime.date): Start date (optional) end_dt (datetime.date): End date (optional)</p> <p>Returns: dict: A dictionary where keys are dates and values are arrays of indices corresponding to each date's snapshots. A array can be empty if there are no snapshots associated with a date</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def prepare_snapshots_dict(df, start_dt=None, end_dt=None):\n    \"\"\"\n    Prepares a dictionary mapping snapshot dates to their corresponding snapshot indices.\n\n    Args:\n    df (pd.DataFrame): DataFrame containing at least a 'snapshot_date' column which represents the dates.\n    start_dt (datetime.date): Start date (optional)\n    end_dt (datetime.date): End date (optional)\n\n    Returns:\n    dict: A dictionary where keys are dates and values are arrays of indices corresponding to each date's snapshots.\n    A array can be empty if there are no snapshots associated with a date\n\n    \"\"\"\n    # Ensure 'snapshot_date' is in the DataFrame\n    if \"snapshot_date\" not in df.columns:\n        raise ValueError(\"DataFrame must include a 'snapshot_date' column\")\n\n    # Group the DataFrame by 'snapshot_date' and collect the indices for each group\n    snapshots_dict = {\n        date: group.index.tolist() for date, group in df.groupby(\"snapshot_date\")\n    }\n\n    # If start_dt and end_dt are specified, add any missing keys from prediction_dates\n    if start_dt:\n        prediction_dates = pd.date_range(\n            start=start_dt, end=end_dt, freq=\"D\"\n        ).date.tolist()[:-1]\n        for dt in prediction_dates:\n            if dt not in snapshots_dict:\n                snapshots_dict[dt] = []\n\n    return snapshots_dict\n</code></pre>"},{"location":"api/#patientflow.train","title":"<code>train</code>","text":""},{"location":"api/#patientflow.train.classifiers","title":"<code>classifiers</code>","text":""},{"location":"api/#patientflow.train.classifiers.chronological_cross_validation","title":"<code>chronological_cross_validation(pipeline, X, y, n_splits=5)</code>","text":"<p>Perform time series cross-validation with multiple metrics.</p> Source code in <code>src/patientflow/train/classifiers.py</code> <pre><code>def chronological_cross_validation(\n    pipeline: Pipeline, X: DataFrame, y: Series, n_splits: int = 5\n) -&gt; Dict[str, float]:\n    \"\"\"Perform time series cross-validation with multiple metrics.\"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n\n    train_metrics: List[FoldResults] = []\n    valid_metrics: List[FoldResults] = []\n\n    for train_idx, valid_idx in tscv.split(X):\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n\n        pipeline.fit(X_train, y_train)\n        train_preds = pipeline.predict_proba(X_train)[:, 1]\n        valid_preds = pipeline.predict_proba(X_valid)[:, 1]\n\n        train_metrics.append(evaluate_predictions(y_train, train_preds))\n        valid_metrics.append(evaluate_predictions(y_valid, valid_preds))\n\n    def aggregate_metrics(metrics_list: List[FoldResults]) -&gt; Dict[str, float]:\n        return {\n            field: np.mean([getattr(m, field) for m in metrics_list])\n            for field in FoldResults.__dataclass_fields__\n        }\n\n    train_means = aggregate_metrics(train_metrics)\n    valid_means = aggregate_metrics(valid_metrics)\n\n    return {f\"train_{metric}\": value for metric, value in train_means.items()} | {\n        f\"valid_{metric}\": value for metric, value in valid_means.items()\n    }\n</code></pre>"},{"location":"api/#patientflow.train.classifiers.create_balance_info","title":"<code>create_balance_info(is_balanced, original_size, balanced_size, original_positive_rate, balanced_positive_rate, majority_to_minority_ratio)</code>","text":"<p>Create a dictionary with balance information.</p> Source code in <code>src/patientflow/train/classifiers.py</code> <pre><code>def create_balance_info(\n    is_balanced: bool,\n    original_size: int,\n    balanced_size: int,\n    original_positive_rate: float,\n    balanced_positive_rate: float,\n    majority_to_minority_ratio: float,\n) -&gt; Dict[str, Union[bool, int, float]]:\n    \"\"\"Create a dictionary with balance information.\"\"\"\n    return {\n        \"is_balanced\": is_balanced,\n        \"original_size\": original_size,\n        \"balanced_size\": balanced_size,\n        \"original_positive_rate\": original_positive_rate,\n        \"balanced_positive_rate\": balanced_positive_rate,\n        \"majority_to_minority_ratio\": majority_to_minority_ratio,\n    }\n</code></pre>"},{"location":"api/#patientflow.train.classifiers.create_column_transformer","title":"<code>create_column_transformer(df, ordinal_mappings=None)</code>","text":"<p>Create a column transformer for a dataframe with dynamic column handling.</p> Source code in <code>src/patientflow/train/classifiers.py</code> <pre><code>def create_column_transformer(\n    df: DataFrame, ordinal_mappings: Optional[Dict[str, List[Any]]] = None\n) -&gt; ColumnTransformer:\n    \"\"\"Create a column transformer for a dataframe with dynamic column handling.\"\"\"\n    transformers: List[\n        Tuple[str, Union[OrdinalEncoder, OneHotEncoder, StandardScaler], List[str]]\n    ] = []\n\n    if ordinal_mappings is None:\n        ordinal_mappings = {}\n\n    for col in df.columns:\n        if col in ordinal_mappings:\n            transformers.append(\n                (\n                    col,\n                    OrdinalEncoder(\n                        categories=[ordinal_mappings[col]],\n                        handle_unknown=\"use_encoded_value\",\n                        unknown_value=np.nan,\n                    ),\n                    [col],\n                )\n            )\n        elif df[col].dtype == \"object\" or (\n            df[col].dtype == \"bool\" or df[col].nunique() == 2\n        ):\n            transformers.append((col, OneHotEncoder(handle_unknown=\"ignore\"), [col]))\n        else:\n            transformers.append((col, StandardScaler(), [col]))\n\n    return ColumnTransformer(transformers)\n</code></pre>"},{"location":"api/#patientflow.train.classifiers.evaluate_model","title":"<code>evaluate_model(pipeline, X_test, y_test)</code>","text":"<p>Evaluate model on test set.</p> Source code in <code>src/patientflow/train/classifiers.py</code> <pre><code>def evaluate_model(\n    pipeline: Pipeline, X_test: DataFrame, y_test: Series\n) -&gt; Dict[str, float]:\n    \"\"\"Evaluate model on test set.\"\"\"\n    y_test_pred = pipeline.predict_proba(X_test)[:, 1]\n    return {\n        \"test_auc\": roc_auc_score(y_test, y_test_pred),\n        \"test_logloss\": log_loss(y_test, y_test_pred),\n        \"test_auprc\": average_precision_score(y_test, y_test_pred),\n    }\n</code></pre>"},{"location":"api/#patientflow.train.classifiers.evaluate_predictions","title":"<code>evaluate_predictions(y_true, y_pred)</code>","text":"<p>Calculate multiple metrics for given predictions.</p> Source code in <code>src/patientflow/train/classifiers.py</code> <pre><code>def evaluate_predictions(\n    y_true: npt.NDArray[np.int_], y_pred: npt.NDArray[np.float64]\n) -&gt; FoldResults:\n    \"\"\"Calculate multiple metrics for given predictions.\"\"\"\n    return FoldResults(\n        auc=roc_auc_score(y_true, y_pred),\n        logloss=log_loss(y_true, y_pred),\n        auprc=average_precision_score(y_true, y_pred),\n    )\n</code></pre>"},{"location":"api/#patientflow.train.classifiers.get_dataset_metadata","title":"<code>get_dataset_metadata(X_train, X_valid, X_test, y_train, y_valid, y_test)</code>","text":"<p>Get dataset sizes and class balances.</p> Source code in <code>src/patientflow/train/classifiers.py</code> <pre><code>def get_dataset_metadata(\n    X_train: DataFrame,\n    X_valid: DataFrame,\n    X_test: DataFrame,\n    y_train: Series,\n    y_valid: Series,\n    y_test: Series,\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Get dataset sizes and class balances.\"\"\"\n    return {\n        \"train_valid_test_set_no\": {\n            \"train_set_no\": len(X_train),\n            \"valid_set_no\": len(X_valid),\n            \"test_set_no\": len(X_test),\n        },\n        \"train_valid_test_class_balance\": {\n            \"y_train_class_balance\": calculate_class_balance(y_train),\n            \"y_valid_class_balance\": calculate_class_balance(y_valid),\n            \"y_test_class_balance\": calculate_class_balance(y_test),\n        },\n    }\n</code></pre>"},{"location":"api/#patientflow.train.classifiers.get_feature_metadata","title":"<code>get_feature_metadata(pipeline)</code>","text":"<p>Extract feature names and importances from pipeline.</p>"},{"location":"api/#patientflow.train.classifiers.get_feature_metadata--parameters","title":"Parameters","text":"<p>pipeline : Pipeline     Sklearn pipeline containing feature transformer and classifier</p>"},{"location":"api/#patientflow.train.classifiers.get_feature_metadata--returns","title":"Returns","text":"<p>FeatureMetadata     Dictionary containing feature names and their importance scores (if available)</p>"},{"location":"api/#patientflow.train.classifiers.get_feature_metadata--raises","title":"Raises","text":"<p>AttributeError     If the classifier doesn't support feature importance</p> Source code in <code>src/patientflow/train/classifiers.py</code> <pre><code>def get_feature_metadata(pipeline: Pipeline) -&gt; FeatureMetadata:\n    \"\"\"\n    Extract feature names and importances from pipeline.\n\n    Parameters\n    ----------\n    pipeline : Pipeline\n        Sklearn pipeline containing feature transformer and classifier\n\n    Returns\n    -------\n    FeatureMetadata\n        Dictionary containing feature names and their importance scores (if available)\n\n    Raises\n    ------\n    AttributeError\n        If the classifier doesn't support feature importance\n    \"\"\"\n    transformed_cols = pipeline.named_steps[\n        \"feature_transformer\"\n    ].get_feature_names_out()\n    classifier = pipeline.named_steps[\"classifier\"]\n\n    # Try different common feature importance attributes\n    if hasattr(classifier, \"feature_importances_\"):\n        importances = classifier.feature_importances_\n    elif hasattr(classifier, \"coef_\"):\n        importances = (\n            np.abs(classifier.coef_[0])\n            if classifier.coef_.ndim &gt; 1\n            else np.abs(classifier.coef_)\n        )\n    else:\n        raise AttributeError(\"Classifier doesn't provide feature importance scores\")\n\n    return {\n        \"feature_names\": [col.split(\"__\")[-1] for col in transformed_cols],\n        \"feature_importances\": importances.tolist(),\n    }\n</code></pre>"},{"location":"api/#patientflow.train.classifiers.initialise_model","title":"<code>initialise_model(model_class, params, xgb_specific_params={'n_jobs': -1, 'eval_metric': 'logloss', 'enable_categorical': True})</code>","text":"<p>Initialize a model with given hyperparameters.</p>"},{"location":"api/#patientflow.train.classifiers.initialise_model--parameters","title":"Parameters","text":"<p>model_class : Type     The classifier class to instantiate params : Dict[str, Any]     Model-specific parameters to set xgb_specific_params : Dict[str, Any], optional     XGBoost-specific default parameters</p>"},{"location":"api/#patientflow.train.classifiers.initialise_model--returns","title":"Returns","text":"<p>Any     Initialized model instance</p> Source code in <code>src/patientflow/train/classifiers.py</code> <pre><code>def initialise_model(\n    model_class: Type,\n    params: Dict[str, Any],\n    xgb_specific_params: Dict[str, Any] = {\n        \"n_jobs\": -1,\n        \"eval_metric\": \"logloss\",\n        \"enable_categorical\": True,\n    },\n) -&gt; Any:\n    \"\"\"\n    Initialize a model with given hyperparameters.\n\n    Parameters\n    ----------\n    model_class : Type\n        The classifier class to instantiate\n    params : Dict[str, Any]\n        Model-specific parameters to set\n    xgb_specific_params : Dict[str, Any], optional\n        XGBoost-specific default parameters\n\n    Returns\n    -------\n    Any\n        Initialized model instance\n    \"\"\"\n    if model_class == XGBClassifier:\n        model = model_class(**xgb_specific_params)\n        model.set_params(**params)\n    else:\n        model = model_class(**params)\n\n    return model\n</code></pre>"},{"location":"api/#patientflow.train.classifiers.train_classifier","title":"<code>train_classifier(train_visits, valid_visits, test_visits, prediction_time, exclude_from_training_data, grid, ordinal_mappings, visit_col, model_class=XGBClassifier, use_balanced_training=True, majority_to_minority_ratio=1.0, calibrate_probabilities=True, calibration_method='sigmoid')</code>","text":"<p>Train a single model including data preparation and balancing.</p>"},{"location":"api/#patientflow.train.classifiers.train_classifier--parameters","title":"Parameters:","text":"<p>train_visits : DataFrame     Training visits dataset valid_visits : DataFrame     Validation visits dataset test_visits : DataFrame     Test visits dataset prediction_time : Tuple[int, int]     The prediction time point to use exclude_from_training_data : List[str]     Columns to exclude from training grid : Dict[str, List[Any]]     Parameter grid for hyperparameter tuning ordinal_mappings : Dict[str, List[Any]]     Mappings for ordinal categorical features visit_col : str     Name of the visit column model_class : Type, optional     The classifier class to use. Must be sklearn-compatible with fit() and predict_proba().     Defaults to XGBClassifier. use_balanced_training : bool, default=True     Whether to use balanced training data majority_to_minority_ratio : float, default=1.0     Ratio of majority to minority class samples calibrate_probabilities : bool, default=True     Whether to apply probability calibration to the best model calibration_method : str, default='isotonic'     Method for probability calibration ('isotonic' or 'sigmoid')</p>"},{"location":"api/#patientflow.train.classifiers.train_classifier--returns","title":"Returns:","text":"<p>TrainedClassifier     Trained model, including metrics, and feature information</p> Source code in <code>src/patientflow/train/classifiers.py</code> <pre><code>def train_classifier(\n    train_visits: DataFrame,\n    valid_visits: DataFrame,\n    test_visits: DataFrame,\n    prediction_time: Tuple[int, int],\n    exclude_from_training_data: List[str],\n    grid: Dict[str, List[Any]],\n    ordinal_mappings: Dict[str, List[Any]],\n    visit_col: str,\n    model_class: Type = XGBClassifier,\n    use_balanced_training: bool = True,\n    majority_to_minority_ratio: float = 1.0,\n    calibrate_probabilities: bool = True,\n    calibration_method: str = \"sigmoid\",\n) -&gt; TrainedClassifier:\n    \"\"\"\n    Train a single model including data preparation and balancing.\n\n    Parameters:\n    -----------\n    train_visits : DataFrame\n        Training visits dataset\n    valid_visits : DataFrame\n        Validation visits dataset\n    test_visits : DataFrame\n        Test visits dataset\n    prediction_time : Tuple[int, int]\n        The prediction time point to use\n    exclude_from_training_data : List[str]\n        Columns to exclude from training\n    grid : Dict[str, List[Any]]\n        Parameter grid for hyperparameter tuning\n    ordinal_mappings : Dict[str, List[Any]]\n        Mappings for ordinal categorical features\n    visit_col : str\n        Name of the visit column\n    model_class : Type, optional\n        The classifier class to use. Must be sklearn-compatible with fit() and predict_proba().\n        Defaults to XGBClassifier.\n    use_balanced_training : bool, default=True\n        Whether to use balanced training data\n    majority_to_minority_ratio : float, default=1.0\n        Ratio of majority to minority class samples\n    calibrate_probabilities : bool, default=True\n        Whether to apply probability calibration to the best model\n    calibration_method : str, default='isotonic'\n        Method for probability calibration ('isotonic' or 'sigmoid')\n\n    Returns:\n    --------\n    TrainedClassifier\n        Trained model, including metrics, and feature information\n    \"\"\"\n    # Get snapshots for each set\n    X_train, y_train = get_snapshots_at_prediction_time(\n        train_visits, prediction_time, exclude_from_training_data, visit_col=visit_col\n    )\n    X_valid, y_valid = get_snapshots_at_prediction_time(\n        valid_visits, prediction_time, exclude_from_training_data, visit_col=visit_col\n    )\n    X_test, y_test = get_snapshots_at_prediction_time(\n        test_visits, prediction_time, exclude_from_training_data, visit_col=visit_col\n    )\n\n    # Get dataset metadata before any balancing\n    dataset_metadata = get_dataset_metadata(\n        X_train, X_valid, X_test, y_train, y_valid, y_test\n    )\n\n    # Store original size and positive rate before any balancing\n    original_size = len(X_train)\n    original_positive_rate = y_train.mean()\n\n    if use_balanced_training:\n        pos_indices = y_train[y_train == 1].index\n        neg_indices = y_train[y_train == 0].index\n\n        n_pos = len(pos_indices)\n        n_neg = int(n_pos * majority_to_minority_ratio)\n\n        neg_indices_sampled = np.random.choice(\n            neg_indices, size=min(n_neg, len(neg_indices)), replace=False\n        )\n\n        train_balanced_indices = np.concatenate([pos_indices, neg_indices_sampled])\n        np.random.shuffle(train_balanced_indices)\n\n        X_train = X_train.loc[train_balanced_indices]\n        y_train = y_train.loc[train_balanced_indices]\n\n    # Create balance info after any balancing is done\n    balance_info = create_balance_info(\n        is_balanced=use_balanced_training,\n        original_size=original_size,\n        balanced_size=len(X_train),\n        original_positive_rate=original_positive_rate,\n        balanced_positive_rate=y_train.mean(),\n        majority_to_minority_ratio=majority_to_minority_ratio\n        if use_balanced_training\n        else 1.0,\n    )\n\n    # Initialize best training results with default values\n    best_training = TrainingResults(\n        prediction_time=prediction_time,\n        balance_info=balance_info,\n        # Other fields will use their default empty dictionaries\n    )\n\n    # Initialize best model container\n    best_model = TrainedClassifier(\n        training_results=best_training,\n        pipeline=None,\n        calibrated_pipeline=None,\n    )\n\n    trials_list: List[HyperParameterTrial] = []\n    best_logloss = float(\"inf\")\n\n    for params in ParameterGrid(grid):\n        # Initialize model based on provided class\n        model = initialise_model(model_class, params)\n\n        column_transformer = create_column_transformer(X_train, ordinal_mappings)\n        pipeline = Pipeline(\n            [(\"feature_transformer\", column_transformer), (\"classifier\", model)]\n        )\n\n        cv_results = chronological_cross_validation(\n            pipeline, X_train, y_train, n_splits=5\n        )\n        # Store trial results\n        trials_list.append(\n            HyperParameterTrial(\n                parameters=params.copy(),  # Make a copy to ensure immutability\n                cv_results=cv_results,\n            )\n        )\n\n        if cv_results[\"valid_logloss\"] &lt; best_logloss:\n            best_logloss = cv_results[\"valid_logloss\"]\n            best_model.pipeline = pipeline\n\n            # Get feature metadata if available\n            try:\n                feature_metadata = get_feature_metadata(pipeline)\n                has_feature_importance = True\n            except (AttributeError, NotImplementedError):\n                feature_metadata = {\n                    \"feature_names\": column_transformer.get_feature_names_out().tolist(),\n                    \"feature_importances\": [],\n                }\n                has_feature_importance = False\n\n            # Update training results\n            best_training.training_info = {\n                \"cv_trials\": trials_list,\n                \"features\": {\n                    \"names\": feature_metadata[\"feature_names\"],\n                    \"importances\": feature_metadata[\"feature_importances\"],\n                    \"has_importance_values\": has_feature_importance,\n                },\n                \"dataset_info\": dataset_metadata,\n            }\n\n            if calibrate_probabilities:\n                best_training.calibration_info = {\"method\": calibration_method}\n\n    # Apply probability calibration to the best model if requested\n    if calibrate_probabilities and best_model.pipeline is not None:\n        best_feature_transformer = best_model.pipeline.named_steps[\n            \"feature_transformer\"\n        ]\n        best_classifier = best_model.pipeline.named_steps[\"classifier\"]\n\n        X_valid_transformed = best_feature_transformer.transform(X_valid)\n\n        calibrated_classifier = CalibratedClassifierCV(\n            estimator=best_classifier,\n            method=calibration_method,\n            cv=\"prefit\",\n        )\n        calibrated_classifier.fit(X_valid_transformed, y_valid)\n\n        calibrated_pipeline = Pipeline(\n            [\n                (\"feature_transformer\", best_feature_transformer),\n                (\"classifier\", calibrated_classifier),\n            ]\n        )\n\n        best_model.calibrated_pipeline = calibrated_pipeline\n        best_training.test_results = evaluate_model(calibrated_pipeline, X_test, y_test)\n\n    else:\n        best_training.test_results = evaluate_model(best_model.pipeline, X_test, y_test)\n\n    return best_model\n</code></pre>"},{"location":"api/#patientflow.train.classifiers.train_multiple_classifiers","title":"<code>train_multiple_classifiers(train_visits, valid_visits, test_visits, grid, exclude_from_training_data, ordinal_mappings, prediction_times, model_name='admissions', visit_col='visit_number', calibrate_probabilities=True, calibration_method='isotonic', use_balanced_training=True, majority_to_minority_ratio=1.0)</code>","text":"<p>Train admission prediction models for multiple prediction times.</p> Source code in <code>src/patientflow/train/classifiers.py</code> <pre><code>def train_multiple_classifiers(\n    train_visits: DataFrame,\n    valid_visits: DataFrame,\n    test_visits: DataFrame,\n    grid: Dict[str, List[Any]],\n    exclude_from_training_data: List[str],\n    ordinal_mappings: Dict[str, List[Any]],\n    prediction_times: List[Tuple[int, int]],\n    model_name: str = \"admissions\",\n    visit_col: str = \"visit_number\",\n    calibrate_probabilities: bool = True,\n    calibration_method: str = \"isotonic\",\n    use_balanced_training: bool = True,\n    majority_to_minority_ratio: float = 1.0,\n) -&gt; Dict[str, TrainedClassifier]:\n    \"\"\"Train admission prediction models for multiple prediction times.\"\"\"\n    trained_models: Dict[str, TrainedClassifier] = {}\n\n    for prediction_time in prediction_times:\n        print(f\"\\nProcessing: {prediction_time}\")\n        model_key = get_model_key(model_name, prediction_time)\n\n        # Train model with the new simplified interface\n        best_model = train_classifier(\n            train_visits,\n            valid_visits,\n            test_visits,\n            prediction_time,\n            exclude_from_training_data,\n            grid,\n            ordinal_mappings,\n            visit_col,\n            use_balanced_training=use_balanced_training,\n            majority_to_minority_ratio=majority_to_minority_ratio,\n            calibrate_probabilities=calibrate_probabilities,\n            calibration_method=calibration_method,\n        )\n\n        trained_models[model_key] = best_model\n\n    return trained_models\n</code></pre>"},{"location":"api/#patientflow.train.emergency_demand","title":"<code>emergency_demand</code>","text":""},{"location":"api/#patientflow.train.emergency_demand.main","title":"<code>main(data_folder_name=None)</code>","text":"<p>Main entry point for training patient flow models.</p> <p>Parameters:</p> Name Type Description Default <code>data_folder_name</code> <code>str</code> <p>Name of data folder</p> <code>None</code> Source code in <code>src/patientflow/train/emergency_demand.py</code> <pre><code>def main(data_folder_name=None):\n    \"\"\"\n    Main entry point for training patient flow models.\n\n    Args:\n        data_folder_name (str, optional): Name of data folder\n    \"\"\"\n    # Parse arguments if not provided\n    if data_folder_name is None:\n        args = parse_args()\n        data_folder_name = (\n            data_folder_name if data_folder_name is not None else args.data_folder_name\n        )\n    print(f\"Loading data from folder: {data_folder_name}\")\n\n    project_root = set_project_root()\n\n    # Set file locations\n    data_file_path, _, model_file_path, config_path = set_file_paths(\n        project_root=project_root,\n        inference_time=False,\n        train_dttm=None,\n        data_folder_name=data_folder_name,\n        config_file=\"config.yaml\",\n    )\n\n    # Load parameters\n    config = load_config_file(config_path)\n\n    # Extract parameters\n    prediction_times = config[\"prediction_times\"]\n    start_training_set = config[\"start_training_set\"]\n    start_validation_set = config[\"start_validation_set\"]\n    start_test_set = config[\"start_test_set\"]\n    end_test_set = config[\"end_test_set\"]\n    prediction_window = config[\"prediction_window\"]\n    epsilon = float(config[\"epsilon\"])\n    yta_time_interval = config[\"yta_time_interval\"]\n    x1, y1, x2, y2 = config[\"x1\"], config[\"y1\"], config[\"x2\"], config[\"y2\"]\n\n    # Load data\n    ed_visits = load_data(\n        data_file_path=data_file_path,\n        file_name=\"ed_visits.csv\",\n        index_column=\"snapshot_id\",\n        sort_columns=[\"visit_number\", \"snapshot_date\", \"prediction_time\"],\n        eval_columns=[\"prediction_time\", \"consultation_sequence\", \"final_sequence\"],\n    )\n    inpatient_arrivals = load_data(\n        data_file_path=data_file_path, file_name=\"inpatient_arrivals.csv\"\n    )\n\n    # Create snapshot date\n    ed_visits[\"snapshot_date\"] = pd.to_datetime(ed_visits[\"snapshot_date\"]).dt.date\n\n    # Set up model parameters\n    grid_params = {\"n_estimators\": [30], \"subsample\": [0.7], \"colsample_bytree\": [0.7]}\n\n    exclude_columns = [\n        \"visit_number\",\n        \"snapshot_date\",\n        \"prediction_time\",\n        \"specialty\",\n        \"consultation_sequence\",\n        \"final_sequence\",\n    ]\n\n    ordinal_mappings = {\n        \"age_group\": [\n            \"0-17\",\n            \"18-24\",\n            \"25-34\",\n            \"35-44\",\n            \"45-54\",\n            \"55-64\",\n            \"65-74\",\n            \"75-102\",\n        ],\n        \"latest_acvpu\": [\"A\", \"C\", \"V\", \"P\", \"U\"],\n        \"latest_obs_manchester_triage_acuity\": [\n            \"Blue\",\n            \"Green\",\n            \"Yellow\",\n            \"Orange\",\n            \"Red\",\n        ],\n        \"latest_obs_objective_pain_score\": [\n            \"Nil\",\n            \"Mild\",\n            \"Moderate\",\n            \"Severe\\\\E\\\\Very Severe\",\n        ],\n        \"latest_obs_level_of_consciousness\": [\"A\", \"C\", \"V\", \"P\", \"U\"],\n    }\n\n    specialties = [\"surgical\", \"haem/onc\", \"medical\", \"paediatric\"]\n    cdf_cut_points = [0.9, 0.7]\n    curve_params = (x1, y1, x2, y2)\n    random_seed = 42\n\n    # Call train_all_models with prepared parameters\n    train_all_models(\n        visits=ed_visits,\n        start_training_set=start_training_set,\n        start_validation_set=start_validation_set,\n        start_test_set=start_test_set,\n        end_test_set=end_test_set,\n        yta=inpatient_arrivals,\n        model_file_path=model_file_path,\n        prediction_times=prediction_times,\n        prediction_window=prediction_window,\n        yta_time_interval=yta_time_interval,\n        epsilon=epsilon,\n        curve_params=curve_params,\n        grid_params=grid_params,\n        exclude_columns=exclude_columns,\n        ordinal_mappings=ordinal_mappings,\n        specialties=specialties,\n        cdf_cut_points=cdf_cut_points,\n        random_seed=random_seed,\n    )\n\n    return\n</code></pre>"},{"location":"api/#patientflow.train.emergency_demand.test_real_time_predictions","title":"<code>test_real_time_predictions(visits, models, prediction_window, specialties, cdf_cut_points, curve_params, random_seed)</code>","text":"<p>Test real-time predictions by selecting a random sample from the visits dataset and generating predictions using the trained models.</p>"},{"location":"api/#patientflow.train.emergency_demand.test_real_time_predictions--parameters","title":"Parameters","text":"<p>visits : pd.DataFrame     DataFrame containing visit data with columns including 'prediction_time',     'snapshot_date', and other required features for predictions. models : Tuple[Dict[str, TrainedClassifier], SequencePredictor, WeightedPoissonPredictor]     Tuple containing:     - trained_classifiers: TrainedClassifier containing admission predictions     - spec_model: SequencePredictor for specialty predictions     - yet_to_arrive_model: WeightedPoissonPredictor for yet-to-arrive predictions prediction_window : int     Size of the prediction window in minutes for which to generate forecasts. specialties : list[str]     List of specialty names to generate predictions for (e.g., ['surgical',     'medical', 'paediatric']). cdf_cut_points : list[float]     List of probability thresholds for cumulative distribution function     cut points (e.g., [0.9, 0.7]). curve_params : tuple[float, float, float, float]     Parameters (x1, y1, x2, y2) defining the curve used for predictions. random_seed : int     Random seed for reproducible sampling of test cases.</p>"},{"location":"api/#patientflow.train.emergency_demand.test_real_time_predictions--returns","title":"Returns","text":"<p>dict     Dictionary containing:     - 'prediction_time': str, The time point for which predictions were made     - 'prediction_date': str, The date for which predictions were made     - 'realtime_preds': dict, The generated predictions for the sample</p>"},{"location":"api/#patientflow.train.emergency_demand.test_real_time_predictions--raises","title":"Raises","text":"<p>Exception     If real-time inference fails, with detailed error message printed before     system exit.</p>"},{"location":"api/#patientflow.train.emergency_demand.test_real_time_predictions--notes","title":"Notes","text":"<p>The function selects a single random row from the visits DataFrame and generates predictions for that specific time point using all provided models. The predictions are made using the create_predictions() function with the specified parameters.</p> Source code in <code>src/patientflow/train/emergency_demand.py</code> <pre><code>def test_real_time_predictions(\n    visits,\n    models: Tuple[\n        Dict[str, TrainedClassifier], SequencePredictor, WeightedPoissonPredictor\n    ],\n    prediction_window,\n    specialties,\n    cdf_cut_points,\n    curve_params,\n    random_seed,\n):\n    \"\"\"\n    Test real-time predictions by selecting a random sample from the visits dataset\n    and generating predictions using the trained models.\n\n    Parameters\n    ----------\n    visits : pd.DataFrame\n        DataFrame containing visit data with columns including 'prediction_time',\n        'snapshot_date', and other required features for predictions.\n    models : Tuple[Dict[str, TrainedClassifier], SequencePredictor, WeightedPoissonPredictor]\n        Tuple containing:\n        - trained_classifiers: TrainedClassifier containing admission predictions\n        - spec_model: SequencePredictor for specialty predictions\n        - yet_to_arrive_model: WeightedPoissonPredictor for yet-to-arrive predictions\n    prediction_window : int\n        Size of the prediction window in minutes for which to generate forecasts.\n    specialties : list[str]\n        List of specialty names to generate predictions for (e.g., ['surgical',\n        'medical', 'paediatric']).\n    cdf_cut_points : list[float]\n        List of probability thresholds for cumulative distribution function\n        cut points (e.g., [0.9, 0.7]).\n    curve_params : tuple[float, float, float, float]\n        Parameters (x1, y1, x2, y2) defining the curve used for predictions.\n    random_seed : int\n        Random seed for reproducible sampling of test cases.\n\n    Returns\n    -------\n    dict\n        Dictionary containing:\n        - 'prediction_time': str, The time point for which predictions were made\n        - 'prediction_date': str, The date for which predictions were made\n        - 'realtime_preds': dict, The generated predictions for the sample\n\n    Raises\n    ------\n    Exception\n        If real-time inference fails, with detailed error message printed before\n        system exit.\n\n    Notes\n    -----\n    The function selects a single random row from the visits DataFrame and\n    generates predictions for that specific time point using all provided models.\n    The predictions are made using the create_predictions() function with the\n    specified parameters.\n    \"\"\"\n    # Select random test set row\n    random_row = visits.sample(n=1, random_state=random_seed)\n    prediction_time = random_row.prediction_time.values[0]\n    prediction_date = random_row.snapshot_date.values[0]\n\n    # Get prediction snapshots\n    prediction_snapshots = visits[\n        (visits.prediction_time == prediction_time)\n        &amp; (visits.snapshot_date == prediction_date)\n    ]\n\n    trained_classifiers, spec_model, yet_to_arrive_model = models\n\n    # Find the model matching the required prediction time\n    classifier = None\n    for model_key, trained_model in trained_classifiers.items():\n        if trained_model.training_results.prediction_time == prediction_time:\n            classifier = trained_model\n            break\n\n    if classifier is None:\n        raise ValueError(f\"No model found for prediction time {prediction_time}\")\n\n    try:\n        x1, y1, x2, y2 = curve_params\n        _ = create_predictions(\n            models=(classifier, spec_model, yet_to_arrive_model),\n            prediction_time=prediction_time,\n            prediction_snapshots=prediction_snapshots,\n            specialties=specialties,\n            prediction_window_hrs=prediction_window / 60,\n            cdf_cut_points=cdf_cut_points,\n            x1=x1,\n            y1=y1,\n            x2=x2,\n            y2=y2,\n        )\n        print(\"Real-time inference ran correctly\")\n    except Exception as e:\n        print(f\"Real-time inference failed due to this error: {str(e)}\")\n        sys.exit(1)\n\n    return\n</code></pre>"},{"location":"api/#patientflow.train.emergency_demand.train_all_models","title":"<code>train_all_models(visits, start_training_set, start_validation_set, start_test_set, end_test_set, yta, prediction_times, prediction_window, yta_time_interval, epsilon, grid_params, exclude_columns, ordinal_mappings, random_seed, visit_col='visit_number', specialties=None, cdf_cut_points=None, curve_params=None, model_file_path=None, save_models=True, test_realtime=True)</code>","text":"<p>Train and evaluate patient flow models.</p>"},{"location":"api/#patientflow.train.emergency_demand.train_all_models--parameters","title":"Parameters","text":"<p>visits : pd.DataFrame     DataFrame containing visit data. yta : pd.DataFrame     DataFrame containing yet-to-arrive data. prediction_times : list     List of times for making predictions. prediction_window : int     Prediction window size in minutes. yta_time_interval : int     Interval size for yet-to-arrive predictions in minutes. epsilon : float     Epsilon parameter for model training. grid_params : dict     Hyperparameter grid for model training. exclude_columns : list     Columns to exclude during training. ordinal_mappings : dict     Ordinal variable mappings for categorical features. random_seed : int     Random seed for reproducibility. visit_col : str, optional     Name of column in dataset that is used to identify a hospital visit (eg visit_number, csn). specialties : list, optional     List of specialties to consider. Required if test_realtime is True. cdf_cut_points : list, optional     CDF cut points for predictions. Required if test_realtime is True. curve_params : tuple, optional     Curve parameters (x1, y1, x2, y2). Required if test_realtime is True. model_file_path : Path, optional     Path to save trained models. Required if save_models is True. save_models : bool, optional     Whether to save the trained models to disk. Defaults to True. test_realtime : bool, optional     Whether to run real-time prediction tests. Defaults to True.</p>"},{"location":"api/#patientflow.train.emergency_demand.train_all_models--returns","title":"Returns","text":"<p>None</p>"},{"location":"api/#patientflow.train.emergency_demand.train_all_models--raises","title":"Raises","text":"<p>ValueError     If save_models is True but model_file_path is not provided,     or if test_realtime is True but any of specialties, cdf_cut_points, or curve_params are not provided.</p>"},{"location":"api/#patientflow.train.emergency_demand.train_all_models--notes","title":"Notes","text":"<p>The function generates model names internally: - \"admissions\": \"admissions\" - \"specialty\": \"ed_specialty\" - \"yet_to_arrive\": f\"yet_to_arrive_{int(prediction_window/60)}_hours\"</p> Source code in <code>src/patientflow/train/emergency_demand.py</code> <pre><code>def train_all_models(\n    visits,\n    start_training_set,\n    start_validation_set,\n    start_test_set,\n    end_test_set,\n    yta,\n    prediction_times,\n    prediction_window,\n    yta_time_interval,\n    epsilon,\n    grid_params,\n    exclude_columns,\n    ordinal_mappings,\n    random_seed,\n    visit_col=\"visit_number\",\n    specialties=None,\n    cdf_cut_points=None,\n    curve_params=None,\n    model_file_path=None,\n    save_models=True,\n    test_realtime=True,\n):\n    \"\"\"\n    Train and evaluate patient flow models.\n\n    Parameters\n    ----------\n    visits : pd.DataFrame\n        DataFrame containing visit data.\n    yta : pd.DataFrame\n        DataFrame containing yet-to-arrive data.\n    prediction_times : list\n        List of times for making predictions.\n    prediction_window : int\n        Prediction window size in minutes.\n    yta_time_interval : int\n        Interval size for yet-to-arrive predictions in minutes.\n    epsilon : float\n        Epsilon parameter for model training.\n    grid_params : dict\n        Hyperparameter grid for model training.\n    exclude_columns : list\n        Columns to exclude during training.\n    ordinal_mappings : dict\n        Ordinal variable mappings for categorical features.\n    random_seed : int\n        Random seed for reproducibility.\n    visit_col : str, optional\n        Name of column in dataset that is used to identify a hospital visit (eg visit_number, csn).\n    specialties : list, optional\n        List of specialties to consider. Required if test_realtime is True.\n    cdf_cut_points : list, optional\n        CDF cut points for predictions. Required if test_realtime is True.\n    curve_params : tuple, optional\n        Curve parameters (x1, y1, x2, y2). Required if test_realtime is True.\n    model_file_path : Path, optional\n        Path to save trained models. Required if save_models is True.\n    save_models : bool, optional\n        Whether to save the trained models to disk. Defaults to True.\n    test_realtime : bool, optional\n        Whether to run real-time prediction tests. Defaults to True.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If save_models is True but model_file_path is not provided,\n        or if test_realtime is True but any of specialties, cdf_cut_points, or curve_params are not provided.\n\n    Notes\n    -----\n    The function generates model names internally:\n    - \"admissions\": \"admissions\"\n    - \"specialty\": \"ed_specialty\"\n    - \"yet_to_arrive\": f\"yet_to_arrive_{int(prediction_window/60)}_hours\"\n    \"\"\"\n    # Validate parameters\n    if save_models and model_file_path is None:\n        raise ValueError(\"model_file_path must be provided when save_models is True\")\n\n    if test_realtime:\n        if specialties is None:\n            raise ValueError(\"specialties must be provided when test_realtime is True\")\n        if cdf_cut_points is None:\n            raise ValueError(\n                \"cdf_cut_points must be provided when test_realtime is True\"\n            )\n        if curve_params is None:\n            raise ValueError(\"curve_params must be provided when test_realtime is True\")\n\n    # Set random seed\n    np.random.seed(random_seed)\n\n    # Define model names internally\n    model_names = {\n        \"admissions\": \"admissions\",\n        \"specialty\": \"ed_specialty\",\n        \"yet_to_arrive\": f\"yet_to_arrive_{int(prediction_window/60)}_hours\",\n    }\n\n    if \"arrival_datetime\" in visits.columns:\n        col_name = \"arrival_datetime\"\n    else:\n        col_name = \"snapshot_date\"\n\n    train_visits, valid_visits, test_visits = create_temporal_splits(\n        visits,\n        start_training_set,\n        start_validation_set,\n        start_test_set,\n        end_test_set,\n        col_name=col_name,\n    )\n\n    train_yta, _, _ = create_temporal_splits(\n        yta[(~yta.specialty.isnull())],\n        start_training_set,\n        start_validation_set,\n        start_test_set,\n        end_test_set,\n        col_name=\"arrival_datetime\",\n    )\n\n    # Use predicted_times from visits if not explicitly provided\n    if prediction_times is None:\n        prediction_times = visits.prediction_time.unique()\n\n    # Train admission models\n    admission_models = train_multiple_classifiers(\n        train_visits=train_visits,\n        valid_visits=valid_visits,\n        test_visits=test_visits,\n        grid=grid_params,\n        exclude_from_training_data=exclude_columns,\n        ordinal_mappings=ordinal_mappings,\n        prediction_times=prediction_times,\n        model_name=model_names[\"admissions\"],\n        visit_col=visit_col,\n    )\n\n    # Save admission models if requested\n\n    if save_models:\n        save_model(admission_models, model_names[\"admissions\"], model_file_path)\n\n    # Train specialty model\n    specialty_model = train_sequence_predictor(\n        train_visits=train_visits,\n        model_name=model_names[\"specialty\"],\n        input_var=\"consultation_sequence\",\n        grouping_var=\"final_sequence\",\n        outcome_var=\"specialty\",\n        visit_col=visit_col,\n    )\n\n    # Save specialty model if requested\n    if save_models:\n        save_model(specialty_model, model_names[\"specialty\"], model_file_path)\n\n    # Train yet-to-arrive model\n    yta_model_name = model_names[\"yet_to_arrive\"]\n\n    num_days = (start_validation_set - start_training_set).days\n\n    yta_model = train_weighted_poisson_predictor(\n        train_visits=train_visits,\n        train_yta=train_yta,\n        prediction_window=prediction_window,\n        yta_time_interval=yta_time_interval,\n        prediction_times=prediction_times,\n        epsilon=epsilon,\n        num_days=num_days,\n    )\n\n    # Save yet-to-arrive model if requested\n    if save_models:\n        save_model(yta_model, yta_model_name, model_file_path)\n        print(f\"Models have been saved to {model_file_path}\")\n\n    # Test real-time predictions if requested\n    if test_realtime:\n        test_real_time_predictions(\n            visits=visits,\n            models=(admission_models, specialty_model, yta_model),\n            prediction_window=prediction_window,\n            specialties=specialties,\n            cdf_cut_points=cdf_cut_points,\n            curve_params=curve_params,\n            random_seed=random_seed,\n        )\n\n    return\n</code></pre>"},{"location":"api/#patientflow.train.sequence_predictor","title":"<code>sequence_predictor</code>","text":""},{"location":"api/#patientflow.train.sequence_predictor.get_default_visits","title":"<code>get_default_visits(admitted)</code>","text":"<p>Filters a dataframe of patient visits to include only non-pediatric patients.</p> <p>This function identifies and removes pediatric patients from the dataset based on both age criteria and specialty assignment. It automatically detects the appropriate age column format from the provided dataframe.</p>"},{"location":"api/#patientflow.train.sequence_predictor.get_default_visits--parameters","title":"Parameters:","text":"<p>admitted : DataFrame     A pandas DataFrame containing patient visit information. Must include either     'age_on_arrival' or 'age_group' columns, and a 'specialty' column.</p>"},{"location":"api/#patientflow.train.sequence_predictor.get_default_visits--returns","title":"Returns:","text":"<p>DataFrame     A filtered DataFrame containing only non-pediatric patients (adults).</p>"},{"location":"api/#patientflow.train.sequence_predictor.get_default_visits--notes","title":"Notes:","text":"<p>The function automatically detects which age-related columns are present in the dataframe and configures the appropriate filtering logic. It removes patients who are either: 1. Identified as pediatric based on age criteria, or 2. Assigned to a pediatric specialty</p>"},{"location":"api/#patientflow.train.sequence_predictor.get_default_visits--examples","title":"Examples:","text":"<p>adult_visits = get_default_visits(all_patient_visits) print(f\"Reduced from {len(all_patient_visits)} to {len(adult_visits)} adult visits\")</p> Source code in <code>src/patientflow/train/sequence_predictor.py</code> <pre><code>def get_default_visits(admitted: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Filters a dataframe of patient visits to include only non-pediatric patients.\n\n    This function identifies and removes pediatric patients from the dataset based on\n    both age criteria and specialty assignment. It automatically detects the appropriate\n    age column format from the provided dataframe.\n\n    Parameters:\n    -----------\n    admitted : DataFrame\n        A pandas DataFrame containing patient visit information. Must include either\n        'age_on_arrival' or 'age_group' columns, and a 'specialty' column.\n\n    Returns:\n    --------\n    DataFrame\n        A filtered DataFrame containing only non-pediatric patients (adults).\n\n    Notes:\n    ------\n    The function automatically detects which age-related columns are present in the\n    dataframe and configures the appropriate filtering logic. It removes patients who\n    are either:\n    1. Identified as pediatric based on age criteria, or\n    2. Assigned to a pediatric specialty\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; adult_visits = get_default_visits(all_patient_visits)\n    &gt;&gt;&gt; print(f\"Reduced from {len(all_patient_visits)} to {len(adult_visits)} adult visits\")\n    \"\"\"\n    # Get configuration for categorizing patients based on age columns\n    special_params = create_special_category_objects(admitted.columns)\n\n    # Extract function that identifies non-pediatric patients\n    opposite_special_category_func = special_params[\"special_func_map\"][\"default\"]\n\n    # Determine which category is the special category (should be \"paediatric\")\n    special_category_key = next(\n        key\n        for key, value in special_params[\"special_category_dict\"].items()\n        if value == 1.0\n    )\n\n    # Filter out pediatric patients based on both age criteria and specialty\n    filtered_admitted = admitted[\n        admitted.apply(opposite_special_category_func, axis=1)\n        &amp; (admitted[\"specialty\"] != special_category_key)\n    ]\n\n    return filtered_admitted\n</code></pre>"},{"location":"api/#patientflow.train.sequence_predictor.train_sequence_predictor","title":"<code>train_sequence_predictor(train_visits, model_name, visit_col, input_var, grouping_var, outcome_var)</code>","text":"<p>Train a specialty prediction model.</p> <p>Parameters:</p> Name Type Description Default <code>train_visits</code> <code>DataFrame</code> <p>Training data containing visit information</p> required <code>model_name</code> <code>str</code> <p>Name identifier for the model</p> required <code>visit_col</code> <code>str</code> <p>Column name containing visit identifiers</p> required <code>input_var</code> <code>str</code> <p>Column name for input sequence</p> required <code>grouping_var</code> <code>str</code> <p>Column name for grouping sequence</p> required <code>outcome_var</code> <code>str</code> <p>Column name for target variable</p> required <p>Returns:</p> Type Description <code>SequencePredictor</code> <p>Trained SequencePredictor model</p> Source code in <code>src/patientflow/train/sequence_predictor.py</code> <pre><code>def train_sequence_predictor(\n    train_visits: DataFrame,\n    model_name: str,\n    visit_col: str,\n    input_var: str,\n    grouping_var: str,\n    outcome_var: str,\n) -&gt; SequencePredictor:\n    \"\"\"Train a specialty prediction model.\n\n    Args:\n        train_visits: Training data containing visit information\n        model_name: Name identifier for the model\n        visit_col: Column name containing visit identifiers\n        input_var: Column name for input sequence\n        grouping_var: Column name for grouping sequence\n        outcome_var: Column name for target variable\n\n    Returns:\n        Trained SequencePredictor model\n    \"\"\"\n    visits_single = select_one_snapshot_per_visit(train_visits, visit_col)\n    admitted = visits_single[\n        (visits_single.is_admitted) &amp; ~(visits_single.specialty.isnull())\n    ]\n    filtered_admitted = get_default_visits(admitted)\n\n    filtered_admitted.loc[:, input_var] = filtered_admitted[input_var].apply(\n        lambda x: tuple(x) if x else ()\n    )\n    filtered_admitted.loc[:, grouping_var] = filtered_admitted[grouping_var].apply(\n        lambda x: tuple(x) if x else ()\n    )\n\n    spec_model = SequencePredictor(\n        input_var=input_var,\n        grouping_var=grouping_var,\n        outcome_var=outcome_var,\n    )\n    spec_model.fit(filtered_admitted)\n\n    return spec_model\n</code></pre>"},{"location":"api/#patientflow.train.utils","title":"<code>utils</code>","text":""},{"location":"api/#patientflow.train.utils.save_model","title":"<code>save_model(model, model_name, model_file_path)</code>","text":"<p>Save trained model(s) to disk.</p>"},{"location":"api/#patientflow.train.utils.save_model--parameters","title":"Parameters","text":"<p>model : object or dict     A single model instance or a dictionary of models to save. model_name : str     Base name to use for saving the model(s). model_file_path : Path     Directory path where the model(s) will be saved.</p>"},{"location":"api/#patientflow.train.utils.save_model--returns","title":"Returns","text":"<p>None</p> Source code in <code>src/patientflow/train/utils.py</code> <pre><code>def save_model(model, model_name, model_file_path):\n    \"\"\"\n    Save trained model(s) to disk.\n\n    Parameters\n    ----------\n    model : object or dict\n        A single model instance or a dictionary of models to save.\n    model_name : str\n        Base name to use for saving the model(s).\n    model_file_path : Path\n        Directory path where the model(s) will be saved.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    if isinstance(model, dict):\n        # Handle dictionary of models (e.g., admission models)\n        for name, m in model.items():\n            full_path = model_file_path / name\n            full_path = full_path.with_suffix(\".joblib\")\n            dump(m, full_path)\n    else:\n        # Handle single model (e.g., specialty or yet-to-arrive model)\n        full_path = model_file_path / model_name\n        full_path = full_path.with_suffix(\".joblib\")\n        dump(model, full_path)\n</code></pre>"},{"location":"api/#patientflow.train.weighted_poisson_predictor","title":"<code>weighted_poisson_predictor</code>","text":""},{"location":"api/#patientflow.train.weighted_poisson_predictor.create_yta_filters","title":"<code>create_yta_filters(df)</code>","text":"<p>Create specialty filters for categorizing patients by specialty and age group.</p> <p>This function generates a dictionary of filters based on specialty categories, with special handling for pediatric patients. It uses the SpecialCategoryParams class to determine which specialties correspond to pediatric care.</p>"},{"location":"api/#patientflow.train.weighted_poisson_predictor.create_yta_filters--parameters","title":"Parameters:","text":"<p>df : pandas.DataFrame     DataFrame containing patient data with columns that include either     'age_on_arrival' or 'age_group' for pediatric classification</p>"},{"location":"api/#patientflow.train.weighted_poisson_predictor.create_yta_filters--returns","title":"Returns:","text":"<p>dict     A dictionary mapping specialty names to filter configurations.     Each configuration contains:     - For pediatric specialty: {\"is_child\": True}     - For other specialties: {\"specialty\": specialty_name, \"is_child\": False}</p>"},{"location":"api/#patientflow.train.weighted_poisson_predictor.create_yta_filters--examples","title":"Examples:","text":"<p>df = pd.DataFrame({'patient_id': [1, 2], 'age_on_arrival': [10, 40]}) filters = create_yta_filters(df) print(filters['paediatric']) {'is_child': True} print(filters['medical']) {'specialty': 'medical', 'is_child': False}</p> Source code in <code>src/patientflow/train/weighted_poisson_predictor.py</code> <pre><code>def create_yta_filters(df):\n    \"\"\"\n    Create specialty filters for categorizing patients by specialty and age group.\n\n    This function generates a dictionary of filters based on specialty categories,\n    with special handling for pediatric patients. It uses the SpecialCategoryParams\n    class to determine which specialties correspond to pediatric care.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        DataFrame containing patient data with columns that include either\n        'age_on_arrival' or 'age_group' for pediatric classification\n\n    Returns:\n    --------\n    dict\n        A dictionary mapping specialty names to filter configurations.\n        Each configuration contains:\n        - For pediatric specialty: {\"is_child\": True}\n        - For other specialties: {\"specialty\": specialty_name, \"is_child\": False}\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; df = pd.DataFrame({'patient_id': [1, 2], 'age_on_arrival': [10, 40]})\n    &gt;&gt;&gt; filters = create_yta_filters(df)\n    &gt;&gt;&gt; print(filters['paediatric'])\n    {'is_child': True}\n    &gt;&gt;&gt; print(filters['medical'])\n    {'specialty': 'medical', 'is_child': False}\n    \"\"\"\n    # Get the special category parameters using the picklable implementation\n    special_params = create_special_category_objects(df.columns)\n\n    # Extract necessary data from the special_params\n    special_category_dict = special_params[\"special_category_dict\"]\n\n    # Create the specialty_filters dictionary\n    specialty_filters = {}\n\n    for specialty, is_paediatric_flag in special_category_dict.items():\n        if is_paediatric_flag == 1.0:\n            # For the paediatric specialty, set `is_child` to True\n            specialty_filters[specialty] = {\"is_child\": True}\n        else:\n            # For other specialties, set `is_child` to False\n            specialty_filters[specialty] = {\"specialty\": specialty, \"is_child\": False}\n\n    return specialty_filters\n</code></pre>"},{"location":"api/#patientflow.train.weighted_poisson_predictor.train_weighted_poisson_predictor","title":"<code>train_weighted_poisson_predictor(train_visits, train_yta, prediction_window, yta_time_interval, prediction_times, num_days, epsilon=1e-06)</code>","text":"<p>Train a yet-to-arrive prediction model.</p> <p>Parameters:</p> Name Type Description Default <code>train_visits</code> <code>DataFrame</code> <p>Visits dataset (used for identifying special categories)</p> required <code>train_yta</code> <code>DataFrame</code> <p>Training data for yet-to-arrive predictions</p> required <code>prediction_window</code> <code>int</code> <p>Time window for predictions</p> required <code>yta_time_interval</code> <code>int</code> <p>Time interval for predictions</p> required <code>prediction_times</code> <code>List[float]</code> <p>List of prediction times</p> required <code>epsilon</code> <code>float</code> <p>Epsilon parameter for model</p> <code>1e-06</code> <code>num_days</code> <code>int</code> <p>Number of days to consider</p> required <p>Returns:</p> Type Description <code>WeightedPoissonPredictor</code> <p>Trained WeightedPoissonPredictor model</p> Source code in <code>src/patientflow/train/weighted_poisson_predictor.py</code> <pre><code>def train_weighted_poisson_predictor(\n    train_visits: DataFrame,\n    train_yta: DataFrame,\n    prediction_window: int,\n    yta_time_interval: int,\n    prediction_times: List[float],\n    num_days: int,\n    epsilon: float = 10e-7,\n) -&gt; WeightedPoissonPredictor:\n    \"\"\"Train a yet-to-arrive prediction model.\n\n    Args:\n        train_visits: Visits dataset (used for identifying special categories)\n        train_yta: Training data for yet-to-arrive predictions\n        prediction_window: Time window for predictions\n        yta_time_interval: Time interval for predictions\n        prediction_times: List of prediction times\n        epsilon: Epsilon parameter for model\n        num_days: Number of days to consider\n\n    Returns:\n        Trained WeightedPoissonPredictor model\n    \"\"\"\n    if train_yta.index.name is None:\n        if \"arrival_datetime\" in train_yta.columns:\n            train_yta.loc[:, \"arrival_datetime\"] = pd.to_datetime(\n                train_yta[\"arrival_datetime\"], utc=True\n            )\n            train_yta.set_index(\"arrival_datetime\", inplace=True)\n\n    elif train_yta.index.name != \"arrival_datetime\":\n        print(\"Dataset needs arrival_datetime column\")\n\n    specialty_filters = create_yta_filters(train_visits)\n\n    yta_model = WeightedPoissonPredictor(filters=specialty_filters)\n    yta_model.fit(\n        train_df=train_yta,\n        prediction_window=prediction_window,\n        yta_time_interval=yta_time_interval,\n        prediction_times=prediction_times,\n        epsilon=epsilon,\n        num_days=num_days,\n    )\n\n    return yta_model\n</code></pre>"},{"location":"notebooks/","title":"About the notebooks","text":""},{"location":"notebooks/#background","title":"Background","text":"<p>The notebooks in this folder demonstrate how you can use the PatientFlow repository. Notebooks combine commentary, code and the results produced by that code. Here's how different audiences can use the notebooks:</p> <ul> <li>As a non-programmer seeking to understand the approach: The narrative sections in each notebook introduce my approach to creating predictive models of emergency demand for a hospital.</li> <li>As a data scientist interested in how to model emergency demand: The code snippets, combined with the narrative, show how I trained, tested and applied my models in Python. The output of each notebook cell shows the results of running the code.</li> <li>As a researcher interested in the patientflow package: The repository contains a Python package that can be installed using an import statement in your code, so that you can use the functions I have developed. The notebooks demonstrate use of the functions in the PatientFlow package.</li> </ul>"},{"location":"notebooks/#outline-of-the-notebooks","title":"Outline of the notebooks","text":"<p>I begin with two introductory notebooks to explain who and what the predictive modelling is for.</p> <ul> <li>1_Meet_the_users_of_our_predictions: Talks about the users of emergency demand predictions in acute hospitals.</li> <li>2_Specify_emergency_demand_model: Explains design choices that were made to develop a practical model, and shows an example of the output that is sent five times a day at UCLH.</li> </ul> <p>A set of notebooks show how to get started with this repository:</p> <ul> <li>3a_Set_up_your_environment: Shows how to set things up if you want to run these notebooks in a Jupyter environment</li> <li>3b_Explore_the_datasets: Introduces the two synthetic datasets created to accompany this repository</li> <li>3c_Convert_your_EHR_data_to_snapshots: Shows an example of how you could convert Electronic Health Record data from a relational database into the data structure used here.</li> </ul> <p>A set of notebooks show three models I have developed for predicting number of beds needed for emergency demand.</p> <ul> <li>4a_Predict_probability_of_admission_from_ED: Shows how to train a machine learning model to predict a patient's probability of admission using patient data from the Emergency Department (ED). This includes dividing the data into training, validation, and testing sets, as well as into subsets based on the time of day the predictions are made, applying an XGBoost model for predictions, and saving the models for future use.</li> <li>4b_Predict_demand_from_patients_in_ED Shows how to convert patient-level admission probabilities into a predictions of overall bed demand</li> <li>4c_Predict_probability_of_admission_to_specialty: Shows how to train a model predicting specialty of admission; a sequence of consultation requests is mapped to a probability of being admitted to one of three specialties: medical, surgical, and haematology/oncology, with paediatric patients (under 18) handled differently</li> <li>4d_Predict_demand_from_patients_yet_to_arrive: Shows the use of a time-varying weighted Poisson distribution to predict a number of patients yet to arrive to the ED within a prediction window (say 8 hours). Demonstrates the use of a function that will assume ED performance targets are met when predicting the number admitted by the end of the prediction window</li> </ul> <p>Two notebooks show how I evaluate the performance of the models, and how they are used for real-time prediction.</p> <ul> <li>5_Evaluate_model_performance: Discusses how to evaluate the models' predictions</li> <li>6_Bring_it_all_together: Shows an example of doing live inference using the models trained in the previous steps</li> </ul> <p>Ignore the instructions below if you are just browsing. You can view the notebooks without going through these steps.</p>"},{"location":"notebooks/#preparing-your-notebook-environment","title":"Preparing your notebook environment","text":"<p>The <code>PATH_TO_PATIENTFLOW</code> environment variable needs to be set so notebooks know where the patientflow repository resides on your computer. You have various options:</p> <ul> <li>use a virtual environment and set PATH_TO_PATIENTFLOW up within that</li> <li>set PATH_TO_PATIENTFLOW globally on your computer</li> <li>let each notebook infer PATH_TO_PATIENTFLOW from the location of the notebook file, or specify it within the notebook</li> </ul>"},{"location":"notebooks/#to-set-the-path_to_patientflow-environment-variable-within-your-virtual-environment","title":"To set the PATH_TO_PATIENTFLOW environment variable within your virtual environment","text":"<p>Conda environments</p> <p>Add PATH_TO_PATIENTFLOW to the <code>environment.yml</code> file:</p> <pre><code>variables:\n  PATH_TO_PATIENTFLOW: /path/to/patientflow\n</code></pre> <p>venv environment</p> <p>Add path_to_patientflow to the venv activation script:</p> <pre><code>echo 'export PATH_TO_PATIENTFLOW=/path/to/patientflow' &gt;&gt; venv/bin/activate  # Linux/Mac\necho 'set PATH_TO_PATIENTFLOW=/path/to/patientflow' &gt;&gt; venv/Scripts/activate.bat  # Windows\n</code></pre> <p>The environment variable will be set whenever you activate the virtual environment and unset when you deactivate it. Replace /path/to/patientflow with your repository path.</p>"},{"location":"notebooks/#to-set-the-project_root-environment-variable-from-within-each-notebook","title":"To set the project_root environment variable from within each notebook","text":"<p>A function called <code>set_project_root()</code> can be run in each notebook. If you include the name of a environment variable as shown below, the function will look in your global environment for a variable of this name.</p> <p>Alternatively, if you call the function without any arguments, the function will try to infer the location of the patientflow repo from your currently active path.</p> <pre><code># to specify an environment variable that has been set elsewhere\nproject_root = set_project_root(env_var =\"PATH_TO_PATIENTFLOW\")\n\n# to let the notebook infer the path\nproject_root = set_project_root()\n\n</code></pre> <p>You can also set an environment variable from within a notebook cell:</p> <p>Linux/Mac:</p> <pre><code>%env PATH_TO_PATIENTFLOW=/path/to/patientflow\n</code></pre> <p>Windows:</p> <pre><code>%env PATH_TO_PATIENTFLOW=C:\\path\\to\\patientflow\n</code></pre> <p>Replace /path/to/patientflow with the actual path to your cloned repository.</p>"},{"location":"notebooks/#to-set-project_root-environment-variable-permanently-on-your-system","title":"To set project_root environment variable permanently on your system","text":"<p>Linux/Mac:</p> <pre><code># Add to ~/.bashrc or ~/.zshrc:\nexport PATH_TO_PATIENTFLOW=/path/to/patientflow\n</code></pre> <p>Windows:</p> <pre><code>Open System Properties &gt; Advanced &gt; Environment Variables\nUnder User Variables, click New\nVariable name: PATH_TO_PATIENTFLOW\nVariable value: C:\\path\\to\\patientflow\nClick OK\n</code></pre> <p>Replace /path/to/patientflow with your repository path. Restart your terminal/IDE after setting.</p>"},{"location":"src/patientflow/","title":"PatientFlow: A forthcoming Python package","text":"<p>Our intention is to release this repository as a Python package that can be installed using common methods like <code>pip install</code></p> <p>The package will support predictions of bed demand and discharges by providing functions that</p> <ul> <li>predict patient-level probabilities of admission and discharge, by specialty</li> <li>create probability distributions predicting number of beds needed for or vacated by those patients, at different levels of aggregation</li> <li>return a net bed position by combining predictions of demand and supply of beds</li> <li>evaluate and provide visualisation of the performance of these predictions</li> </ul> <p>The package is intended to serve as a wrapper of the functions typically used for such purposes in the <code>sklearn</code> and <code>scipy</code> python packages, with additional context to support their application and evaluation in bed management in healthcare</p>"},{"location":"src/patientflow/#modules-overview-in-order-of-their-use-in-a-typical-modelling-workflow","title":"Modules Overview (in order of their use in a typical modelling workflow)","text":"<ul> <li><code>load</code>: A module for loading configuration files, saved data and trained models</li> <li><code>prepare</code>: A module for preparing saved data prior to input into model training</li> <li><code>train</code>: A module and submodules for training predictive models</li> <li><code>calculate</code>: A module for calculating time-varying arrival rates, used in one of the custom predictors and in some of the visualisation functions</li> <li><code>predictors</code>: A module and submodules containing custom predictors developed for the <code>patientflow</code> package</li> <li><code>predict</code>: A module using trained models for predicting various aspects of bed demand and discharges</li> <li><code>aggregate</code>: A module that turns patient-level probabilities into aggregate distributions of bed numbers</li> <li><code>viz</code>: A module containing convenient plotting functions to examine the outputs from the above functions</li> </ul> <p>Other modules may follow in future</p>"},{"location":"src/patientflow/#deployment","title":"Deployment","text":"<p>This package is designed for use in hospital data projects analysing patient flow and bed capacity in short time horizons. The modules can be customised to align with specific hospital requirements</p>"}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions using patient-level snapshots\n",
    "\n",
    "## Things to consider when training predictive models using snapshots\n",
    "\n",
    "**Random versus temporal splits**\n",
    "\n",
    "When dividing your data into training, validation and test sets, a random allocation will make your models appear to perform better than they actually would in practice. This is because random splits ignore the temporal nature of healthcare data, where patterns may change over time. A more realistic approach is to use temporal splits, where you train on earlier data and validate/test on later data, mimicking how the model would be deployed in a real-world setting.\n",
    "\n",
    "**Multiple snapshots per visit**\n",
    "\n",
    "To use `patientflow` your data should be in snapshot form. I showed how to create this in the last notebook. I defined a series of prediction times, and then sampled finished visit to get snapshots that represent those visits while still in progress. When you follow this method, you may end up with multiple snapshots. Is this OK, for your analysis? You will need to decide whether you include all snapshots from a single visit into a predictive model. These snapshots from the same visit are inherently correlated, which may violate assumptions of many statistical and machine learning methods. \n",
    "\n",
    "**Multiple visits per patient**\n",
    "\n",
    "The patient identifier is also important, because if the same patient appears in training and test sets, there is the potential for data leakage. We took the decision to probabilistically allocate each patient to training, validation and test sets, where the probability of being allocated to each set is in proportion to the number of visits they made in any of those time periods. \n",
    "\n",
    "`patientflow` includes functions that handle of all these considerations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating fake finished visits\n",
    "\n",
    "I'll start by loading some fake data resembling the structure of EHR data on Emergency Department (ED) visits. In my fake data, each visit has one row, with an arrival time at the ED, a discharge time from the ED, the patient's age and an outcome of whether they were admitted after the ED visit. \n",
    "\n",
    "The `is_admitted` column is our label, indicating the outcome in this imaginary case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload functions every time\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating fake finished visits - a more complicated example \n",
    "\n",
    "In an EHR, information about the patient accumulates as the ED visit progresses. Patients may visit various locations in the ED, such as triage, where their acuity is recorded, and they have various different things done to them, like measurements of vital signs or lab tests. \n",
    "\n",
    "The function below returns three fake dataframes, meant to resemble EHR data. \n",
    "\n",
    "- hospital visit\n",
    "- observations - with a single measurement - a triage score - plus a timestamp for when that was recorded\n",
    "- lab orders - with five types of lab orders plus a timestamp for when that test was requested\n",
    "\n",
    "The function that creates the fake data returns one triage score for each visit, within 10 minutes of arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>prediction_time</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>visit_number</th>\n",
       "      <th>is_admitted</th>\n",
       "      <th>age</th>\n",
       "      <th>latest_triage_score</th>\n",
       "      <th>num_bmp_orders</th>\n",
       "      <th>num_troponin_orders</th>\n",
       "      <th>num_cbc_orders</th>\n",
       "      <th>num_urinalysis_orders</th>\n",
       "      <th>num_d-dimer_orders</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snapshot_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>(6, 0)</td>\n",
       "      <td>2690</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>(9, 30)</td>\n",
       "      <td>2471</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>(9, 30)</td>\n",
       "      <td>2987</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>(9, 30)</td>\n",
       "      <td>3472</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>(9, 30)</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            snapshot_date  ... num_d-dimer_orders\n",
       "snapshot_id                ...                   \n",
       "0              2023-01-01  ...                  0\n",
       "1              2023-01-01  ...                  0\n",
       "2              2023-01-01  ...                  1\n",
       "3              2023-01-01  ...                  0\n",
       "4              2023-01-01  ...                  0\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from patientflow.generate import create_fake_snapshots\n",
    "prediction_times = [(6, 0), (9, 30), (12, 0), (15, 30), (22, 0)] \n",
    "snapshots_df=create_fake_snapshots(prediction_times=prediction_times, start_date='2023-01-01', end_date='2023-04-01')\n",
    "snapshots_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model to predict the outcome of each snapshot\n",
    "\n",
    "Let's train a model to predict admission for the 9:30 prediction time. We will specify that the triage scores are ordinal, to make use of sklearn's OrdinalEncoder to maintain the natural order of categories. \n",
    "\n",
    "We exclude columns that are not relevant to the prediction of probability of admission, including `snapshot_date` and `prediction_time`. Note that here we are trying a different model for each prediction time. That was a design decision, which allows the model to pick up different signals of the outcome at different times of day. You'll see the results of this in later notebooks where I show shap plots for models at different times of day. \n",
    "\n",
    "If the same patient appears in training, validation or test sets, there is the potential for data leakage. The `create_temporal_splits()` function below will randomly allocate each patient_id to training, validation and test sets, where the probability of being allocated to each is in proportion to the number of visits they made in any of those time periods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Patient Set Overlaps (before random assignment):\n",
      "Train-Valid: 0 of 2158\n",
      "Valid-Test: 29 of 1513\n",
      "Train-Test: 100 of 2500\n",
      "All Sets: 0 of 3021 total patients\n",
      "Split sizes: [1811, 629, 1242]\n"
     ]
    }
   ],
   "source": [
    "from datetime import date   \n",
    "from patientflow.prepare import create_temporal_splits\n",
    "\n",
    "# set the temporal split\n",
    "start_training_set = date(2023, 1, 1) \n",
    "start_validation_set = date(2023, 2, 15) # 6 week training set \n",
    "start_test_set = date(2023, 3, 1) # 2 week validation set \n",
    "end_test_set = date(2023, 4, 1) # 1 month test set\n",
    "\n",
    "# create the temporal splits\n",
    "train_visits, valid_visits, test_visits = create_temporal_splits(\n",
    "    snapshots_df,\n",
    "    start_training_set,\n",
    "    start_validation_set,\n",
    "    start_test_set,\n",
    "    end_test_set,\n",
    "    col_name=\"snapshot_date\", # states which column contains the date to use when making the splits \n",
    "    patient_id=\"patient_id\", # states which column contains the patient id to use when making the splits \n",
    "    visit_col=\"visit_number\", # states which column contains the visit number to use when making the splits \n",
    "\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to decide whether you include all snapshots from a single visit into a predictive model. If you do, there will be non-independence in the data. \n",
    "\n",
    "Since we train a different model for each prediction time, then it is only visits spanning more than 24 hours that would have multiple rows. If your snapshots are drawn from visits to ED, this should hopefully not happen too often (though sadly it is becoming more common in the UK). If your snapshots are drawn from inpatient visits, then it is very likely that you will have multiple rows per patient. \n",
    "\n",
    "We took the decision to select one visit at random, even for our ED visits. The function below gives you the option. If you specify `single_snapshot_per_visit` as True, the function will expect a `visit_col` parameter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patientflow.train.classifiers import train_classifier\n",
    "\n",
    "# exclude columns that are not needed for training\n",
    "exclude_from_training_data=['patient_id', 'visit_number', 'snapshot_date', 'prediction_time']\n",
    "\n",
    "# train the patient-level model\n",
    "model = train_classifier(\n",
    "    train_visits,\n",
    "    valid_visits,\n",
    "    test_visits,\n",
    "    grid={\"n_estimators\": [20, 30, 40]},\n",
    "    prediction_time=(9, 30),\n",
    "    exclude_from_training_data=exclude_from_training_data,\n",
    "    ordinal_mappings={'latest_triage_score': [1, 2, 3, 4, 5]},\n",
    "    single_snapshot_per_visit=True,\n",
    "    visit_col='visit_number', # as we are using a single snapshot per visit, we need to specify which column contains the visit number\n",
    "    use_balanced_training=True,\n",
    "    calibrate_probabilities=True,\n",
    "    calibration_method='sigmoid'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the parameters in `train_classifer()`**\n",
    "\n",
    "There are a few parameters in this function to explain. \n",
    "\n",
    "- `grid`: specifies the grid to use in hyperparameter tuning.\n",
    "- `prediction_time`: is used to identify which patient snapshots to use for training.\n",
    "- `single_snapshot_per_visit`: if this is True, the function will randomly pick one snapshot for any visit, using `visit_col` as the column name that identifies the visit identifier. \n",
    "- `exclude_from_training_data`: certain columns in the data should not be used for training, including visit numbers and dates.\n",
    "- `ordinal_mappings`: the function makes use of SKLearn's Ordinal Mapping encoder.\n",
    "- `use_balanced_training`: in healthcare contexts, there are often fewer observations in the positive class. Set this to True for imbalanced samples (common for ED visits, when most patients are discharged, and for predicting inpatient discharge from hospital when most patients remain). It will downsample the negative class. \n",
    "- `calibrate_probabilities`: when you downsample the negative class, it is a good idea to calibrate the probabilities to account for this class imbalance. Setting this to True will apply isotonic regression to calibrate the predicted probabilities, ensuring they better reflect the probabilities in the original data distribution.\n",
    "- `calibration_method`: options are sigmoid or isotonic; I have found that sigmoid (the default) works better.\n",
    "\n",
    "**About machine learning choices**\n",
    "\n",
    "By default, the function will use an XGBoost classifier, initialised with the hyperparamter grid provided with log loss as the evaluation metric. Chronological ross-validation is used, with the best hyperparameters selected based on minimising log loss in the validation set. We chose XGBoost because it is quick to train, generally performs well, and handles missing values. \n",
    "\n",
    "If you wish to use a different classifer, you can use another parameter,  not shown here:\n",
    "\n",
    "\n",
    "- `model_class` (not shown here):  You can pass your own model in an optional model_class argument, which expects classifier class (like XGBClassifier or other scikit-learn compatible classifiers) that can be instantiated and initialised with the parameters provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the object returned by `train_classifier()`\n",
    "\n",
    "The function returns an object of type TrainedClassifer(). Meta data and metrics from the training process are returned with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object returned is of type: <class 'patientflow.model_artifacts.TrainedClassifier'>\n",
      "\n",
      "The metadata from the training process are returned in the `training_results` attribute:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResults(prediction_time=(9, 30), training_info={'cv_trials': [HyperParameterTrial(parameters={'n_estimators': 20}, cv_results={'train_auc': np.float64(0.9891000722886609), 'train_logloss': np.float64(0.2401767879184195), 'train_auprc': np.float64(0.9915814710451907), 'valid_auc': np.float64(0.6902652943461767), 'valid_logloss': np.float64(0.7612948055228466), 'valid_auprc': np.float64(0.6423223640376016)}), HyperParameterTrial(parameters={'n_estimators': 30}, cv_results={'train_auc': np.float64(0.9964572270616507), 'train_logloss': np.float64(0.2037677992999191), 'train_auprc': np.float64(0.9968371157941393), 'valid_auc': np.float64(0.6828805304172951), 'valid_logloss': np.float64(0.7965992894598218), 'valid_auprc': np.float64(0.6440393638797277)}), HyperParameterTrial(parameters={'n_estimators': 40}, cv_results={'train_auc': np.float64(0.9986821301541798), 'train_logloss': np.float64(0.17958817128501964), 'train_auprc': np.float64(0.9986379247873801), 'valid_auc': np.float64(0.6922204225513049), 'valid_logloss': np.float64(0.8288787785614758), 'valid_auprc': np.float64(0.6543331451114808)})], 'features': {'names': ['age', 'latest_triage_score', 'num_bmp_orders_0', 'num_bmp_orders_1', 'num_troponin_orders_0', 'num_troponin_orders_1', 'num_cbc_orders_0', 'num_cbc_orders_1', 'num_urinalysis_orders_0', 'num_urinalysis_orders_1', 'num_d-dimer_orders_0', 'num_d-dimer_orders_1'], 'importances': [0.11312869191169739, 0.3690102994441986, 0.11062111705541611, 0.0, 0.10884614288806915, 0.0, 0.11563073843717575, 0.0, 0.0881805568933487, 0.0, 0.09458249062299728, 0.0], 'has_importance_values': True}, 'dataset_info': {'train_valid_test_set_no': {'train_set_no': 299, 'valid_set_no': 106, 'test_set_no': 213}, 'train_valid_test_class_balance': {'y_train_class_balance': {0: 0.7123745819397993, 1: 0.28762541806020064}, 'y_valid_class_balance': {0: 0.6698113207547169, 1: 0.330188679245283}, 'y_test_class_balance': {0: 0.6901408450704225, 1: 0.30985915492957744}}}}, calibration_info={'method': 'sigmoid'}, test_results={'test_auc': 0.7148010719439291, 'test_logloss': 0.5654510176351998, 'test_auprc': 0.5559247479672749}, balance_info={'is_balanced': True, 'original_size': 299, 'balanced_size': 172, 'original_positive_rate': np.float64(0.28762541806020064), 'balanced_positive_rate': np.float64(0.5), 'majority_to_minority_ratio': 1.0})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Object returned is of type: {type(model)}')\n",
    "\n",
    "print(f'\\nThe metadata from the training process are returned in the `training_results` attribute:')\n",
    "model.training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better view of what is included within the results, here is a list of the fields returned: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataclass fields in TrainingResults:\n",
      "prediction_time\n",
      "training_info\n",
      "calibration_info\n",
      "test_results\n",
      "balance_info\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import fields\n",
    "print(\"\\nDataclass fields in TrainingResults:\")\n",
    "for field in fields(model.training_results):\n",
    "    print(field.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction time has been saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction time is: (9, 30)\n"
     ]
    }
   ],
   "source": [
    "print(f'The prediction time is: {model.training_results.prediction_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An object called training_info contains information related to model training. To simplify the code, I'll assign it to a variable called results. It will tell us the size and class balance of each set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training_info object contains the following keys: dict_keys(['cv_trials', 'features', 'dataset_info'])\n",
      "\n",
      "Number in each set{'train_set_no': 299, 'valid_set_no': 106, 'test_set_no': 213}\n",
      "train: 71.2% neg, 28.8% pos\n",
      "valid: 67.0% neg, 33.0% pos\n",
      "test: 69.0% neg, 31.0% pos\n"
     ]
    }
   ],
   "source": [
    "results = model.training_results.training_info\n",
    "\n",
    "print(f\"The training_info object contains the following keys: {results.keys()}\")\n",
    "\n",
    "print(f\"\\nNumber in each set{results['dataset_info']['train_valid_test_set_no']}\")\n",
    "\n",
    "def print_class_balance(d):\n",
    "    for k in d:\n",
    "        print(f\"{k.split('_')[1]}: {d[k][0]:.1%} neg, {d[k][1]:.1%} pos\")\n",
    "\n",
    "\n",
    "print_class_balance(results['dataset_info']['train_valid_test_class_balance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class balance information is also saved in the training_results, which will store information about the differences between the class balance when forcing the training set to be balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_balanced': True,\n",
       " 'original_size': 299,\n",
       " 'balanced_size': 172,\n",
       " 'original_positive_rate': np.float64(0.28762541806020064),\n",
       " 'balanced_positive_rate': np.float64(0.5),\n",
       " 'majority_to_minority_ratio': 1.0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_results.balance_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the type of calibration done on balanced samples is saved in training_results also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'sigmoid'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_results.calibration_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of hyperparameter tuning are saved in a HyperParameterTrial object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HyperParameterTrial(parameters={'n_estimators': 20}, cv_results={'train_auc': np.float64(0.9891000722886609), 'train_logloss': np.float64(0.2401767879184195), 'train_auprc': np.float64(0.9915814710451907), 'valid_auc': np.float64(0.6902652943461767), 'valid_logloss': np.float64(0.7612948055228466), 'valid_auprc': np.float64(0.6423223640376016)}),\n",
       " HyperParameterTrial(parameters={'n_estimators': 30}, cv_results={'train_auc': np.float64(0.9964572270616507), 'train_logloss': np.float64(0.2037677992999191), 'train_auprc': np.float64(0.9968371157941393), 'valid_auc': np.float64(0.6828805304172951), 'valid_logloss': np.float64(0.7965992894598218), 'valid_auprc': np.float64(0.6440393638797277)}),\n",
       " HyperParameterTrial(parameters={'n_estimators': 40}, cv_results={'train_auc': np.float64(0.9986821301541798), 'train_logloss': np.float64(0.17958817128501964), 'train_auprc': np.float64(0.9986379247873801), 'valid_auc': np.float64(0.6922204225513049), 'valid_logloss': np.float64(0.8288787785614758), 'valid_auprc': np.float64(0.6543331451114808)})]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results are stored in a HyperParameterTrial object\n",
    "results['cv_trials']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: {'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the trial with the lowest validation logloss\n",
    "best_trial = min(results[\"cv_trials\"], key=lambda trial: trial.cv_results['valid_logloss'])\n",
    "\n",
    "# print the best parameters\n",
    "print(f'The best parameters are: {best_trial.parameters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results on the test set were:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_auc': 0.7148010719439291,\n",
       " 'test_logloss': 0.5654510176351998,\n",
       " 'test_auprc': 0.5559247479672749}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'The results on the test set were:')\n",
    "model.training_results.test_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each record in the snapshots dataframe is indexed by a unique snapshot_id. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here I have shown how `patientflow` can help you\n",
    "\n",
    "* handle multiple snapshots per visit and multiple visits per patient\n",
    "* impose a temporal split on your training and test sets, allowing for the point above \n",
    "* train a model to predict some later outcome using functions that handle class imbalance and calibration\n",
    "\n",
    "In the next notice, I show how to evaluate models applied to patient snapshots. \n",
    "\n",
    "A process like this creates a predicted probability of admission for each patient, based on what is known about them at the time of the snapshot. However, bed managers really want predictions for the whole cohort of patients in the ED at a point in time. This is where `patientflow` comes into its own. In the next notebook, I show how to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patientflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
